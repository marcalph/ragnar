{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81df2fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marcalph/Projects/ragnar/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: weave version 0.51.59 is available!  To upgrade, please run:\n",
      "\u001b[36m\u001b[1mweave\u001b[0m:  $ pip install weave --upgrade\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: Logged in as Weights & Biases user: marcalph.\n",
      "\u001b[36m\u001b[1mweave\u001b[0m: View Weave data at https://wandb.ai/marcalph/rag-course/weave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of evaluation samples:  20\n"
     ]
    }
   ],
   "source": [
    "import weave\n",
    "WANDB_PROJECT = \"rag-course\"\n",
    "\n",
    "weave_client = weave.init(WANDB_PROJECT)\n",
    "eval_dataset = weave.ref(\n",
    "    \"weave:///rag-course/dev/object/Dataset:Qj4IFICc2EbdXu5A5UuhkPiWgxM1GvJMIvXEyv1DYnM\"\n",
    ").get()\n",
    "\n",
    "print(\"Number of evaluation samples: \", len(eval_dataset.rows))\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11cf85f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "weave.flow.dataset.Dataset"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b4f8d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "contexts",
         "rawType": "object",
         "type": "unknown"
        }
       ],
       "ref": "7622894b-6621-42f7-abc4-a12cf6e2c883",
       "rows": [
        [
         "0",
         "How can I access the run object from the Lightning WandBLogger function?",
         "In PyTorch Lightning, the `WandbLogger` is used to log metrics, model weights, and other data to Weights & Biases during training. To access the `wandb.Run` object from within a `LightningModule` when using `WandbLogger`, you can use the `Trainer.logger.experiment` attribute. This attribute provides direct access to the underlying `wandb.Run` object, allowing you to interact with the Weights & Biases API directly.\n\nHere's how you can access the `wandb.Run` object using `WandbLogger` in PyTorch Lightning:\n\n```python\nfrom pytorch_lightning import Trainer, LightningModule\nfrom pytorch_lightning.loggers import WandbLogger\n\nclass MyModel(LightningModule):\n    def training_step(self, batch, batch_idx):\n        # Your training logic here\n        loss = ...\n\n        # Log metrics\n        self.log('train_loss', loss)\n\n        # Access the wandb.Run object\n        run = self.trainer.logger.experiment\n        # Now you can use `run` to interact with wandb API, e.g., to log additional data\n        run.log({'additional_metric': value})\n\n        return loss\n\n# Setup the logger\nwandb_logger = WandbLogger(project='my_project', entity='my_entity')\n\n# Setup the trainer\ntrainer = Trainer(logger=wandb_logger)\n\n# Initialize your model\nmodel = MyModel()\n\n# Train the model\ntrainer.fit(model)\n```\n\nIn this example:\n- A `WandbLogger` is instantiated and passed to the `Trainer`.\n- Inside the `LightningModule`, you can access the `wandb.Run` object via `self.trainer.logger.experiment`.\n- You can then use this `run` object to log additional data or interact with the Weights & Biases API directly.\n\nThis approach allows you to leverage the full capabilities of Weights & Biases directly from your training loop in PyTorch Lightning.",
         "WeaveList([{'content': 'Using PyTorch Lightning\\'s WandbLogger\\nPyTorch Lightning has multiple `WandbLogger` (Pytorch) (Fabric) classes that can be used to seamlessly log metrics, model weights, media and more. Just instantiate the WandbLogger and pass it to Lightning\\'s `Trainer` or `Fabric`.\\nwandb_logger = WandbLogger()\\ntrainer = Trainer(logger=wandb_logger)\\nfabric = L.Fabric(loggers=[wandb_logger])\\nfabric.launch()\\nfabric.log_dict({\\n    \"important_metric\": important_metric\\n})\\nLogger arguments\\nBelow are some of the most used parameters in WandbLogger, see the PyTorch Lightning for a full list and description\\n(Pytorch)\\n(Fabric)\\nParameter\\nDescription\\n`project`\\nDefine what wandb Project to log to\\n`name`\\nGive a name to your wandb run\\n`log_model`\\nLog all models if `log_model=\"all\"` or at end of training if `log_model=True`\\n`save_dir`\\nPath where data is saved\\nLog your hyperparameters\\nclass LitModule(LightningModule):\\n    def __init__(self, *args, **kwarg):\\n        self.save_hyperparameters()\\nwandb_logger.log_hyperparams(\\n    {\\n        \"hyperparameter_1\": hyperparameter_1,\\n        \"hyperparameter_2\": hyperparameter_2,\\n    }\\n)\\nLog additional config parameters\\n# add one parameter\\nwandb_logger.experiment.config[\"key\"] = value\\n# add multiple parameters\\nwandb_logger.experiment.config.update({key1: val1, key2: val2})\\n# use directly wandb module\\nwandb.config[\"key\"] = value\\nwandb.config.update()\\nLog gradients, parameter histogram and model topology\\nYou can pass your model object to `wandblogger.watch()` to monitor your models\\'s gradients and parameters as you train. See the PyTorch Lightning `WandbLogger` documentation', 'source': 'guides/integrations/lightning.md', 'score': 0.9996229999999999, 'relevance': 2}, {'content': 'PyTorch Lightning\\nTry in a Colab Notebook here →\\nPyTorch Lightning provides a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training and 16-bit precision. W&B provides a lightweight wrapper for logging your ML experiments. But you don\\'t need to combine the two yourself: Weights & Biases is incorporated directly into the PyTorch Lightning library via the WandbLogger.\\n⚡ Get going lightning-fast with just a few lines.\\nfrom lightning.pytorch.loggers import WandbLogger\\nfrom lightning.pytorch import Trainer\\nwandb_logger = WandbLogger(log_model=\"all\")\\ntrainer = Trainer(logger=wandb_logger)\\n:::info\\nUsing wandb.log(): Please note that the `WandbLogger` logs to W&B using the Trainer\\'s `global_step`. If you are making additional calls to `wandb.log` directly in your code, do not use the `step` argument in `wandb.log()`. \\nInstead, log the Trainer\\'s `global_step` like your other metrics, like so:\\n`wandb.log({\"accuracy\":0.99, \"trainer/global_step\": step})`\\n:::\\nimport lightning as L\\nfrom wandb.integration.lightning.fabric import WandbLogger\\nwandb_logger = WandbLogger(log_model=\"all\")\\nfabric = L.Fabric(loggers=[wandb_logger])\\nfabric.launch()\\nfabric.log_dict({\"important_metric\": important_metric})\\nSign up and Log in to wandb\\na) Sign up for a free account\\nb) Pip install the `wandb` library\\nc) To log in in your training script, you\\'ll need to be signed in to you account at www.wandb.ai, then you will find your API key on the Authorize page.\\nIf you are using Weights and Biases for the first time you might want to check out our quickstart\\npip install wandb\\nwandb login\\n!pip install wandb\\nimport wandb\\nwandb.login()', 'source': 'guides/integrations/lightning.md', 'score': 0.9393594, 'relevance': 2}, {'content': 'Check out interactive examples!\\nYou can follow along in our video tutorial with our tutorial colab here\\nFrequently Asked Questions\\nHow does W&B integrate with Lightning?\\nThe core integration is based on the Lightning loggers API, which lets you write much of your logging code in a framework-agnostic way. `Logger`s are passed to the Lightning Trainer and are triggered based on that API\\'s rich hook-and-callback system. This keeps your research code well-separated from engineering and logging code.\\nWhat does the integration log without any additional code?\\nWe\\'ll save your model checkpoints to W&B, where you can view them or download them for use in future runs. We\\'ll also capture system metrics, like GPU usage and network I/O, environment information, like hardware and OS information, code state (including git commit and diff patch, notebook contents and session history), and anything printed to the standard out.\\nWhat if I really need to use wandb.run in my training setup?\\nYou will have to essentially expand the scope of the variable you need to access yourself. In other words, making sure that the initial conditions are the same on all processes.\\nif os.environ.get(\"LOCAL_RANK\", None) is None:\\n    os.environ[\"WANDB_DIR\"] = wandb.run.dir\\nThen, you can use `os.environ[\"WANDB_DIR\"]` to set up the model checkpoints directory. This way, `wandb.run.dir` can be used by any non-zero rank processes as well.', 'source': 'guides/integrations/lightning.md', 'score': 0.9999046300000001, 'relevance': 1}, {'content': 'With implicit wandb integrations\\nIf you\\'re using a framework integration we support, you can also pass in the callback directly:\\n@wandb_log\\ndef train_model(\\n    train_dataloader_path: components.InputPath(\"dataloader\"),\\n    test_dataloader_path: components.InputPath(\"dataloader\"),\\n    model_path: components.OutputPath(\"pytorch_model\")\\n):\\n    from pytorch_lightning.loggers import WandbLogger\\n    from pytorch_lightning import Trainer\\n    trainer = Trainer(logger=WandbLogger())\\n    ...  # do training', 'source': 'guides/integrations/other/kubeflow-pipelines-kfp.md', 'score': 0.9520419, 'relevance': 1}, {'content': 'Log images, text and more\\nThe `WandbLogger` has `log_image`, `log_text` and `log_table` methods for logging media.\\nYou can also directly call `wandb.log` or `trainer.logger.experiment.log` to log other media types such as Audio, Molecules, Point Clouds, 3D Objects and more.\\n# using tensors, numpy arrays or PIL images\\nwandb_logger.log_image(key=\"samples\", images=[img1, img2])\\n# adding captions\\nwandb_logger.log_image(key=\"samples\", images=[img1, img2], caption=[\"tree\", \"person\"])\\n# using file path\\nwandb_logger.log_image(key=\"samples\", images=[\"img_1.jpg\", \"img_2.jpg\"])\\n# using .log in the trainer\\ntrainer.logger.experiment.log(\\n    {\"samples\": [wandb.Image(img, caption=caption) for (img, caption) in my_images]},\\n    step=current_trainer_global_step,\\n)\\n# data should be a list of lists\\ncolumns = [\"input\", \"label\", \"prediction\"]\\nmy_data = [[\"cheese\", \"english\", \"english\"], [\"fromage\", \"french\", \"spanish\"]]\\n# using columns and data\\nwandb_logger.log_text(key=\"my_samples\", columns=columns, data=my_data)\\n# using a pandas DataFrame\\nwandb_logger.log_text(key=\"my_samples\", dataframe=my_dataframe)\\n# log a W&B Table that has a text caption, an image and audio\\ncolumns = [\"caption\", \"image\", \"sound\"]\\n# data should be a list of lists\\nmy_data = [\\n    [\"cheese\", wandb.Image(img_1), wandb.Audio(snd_1)],\\n    [\"wine\", wandb.Image(img_2), wandb.Audio(snd_2)],\\n]\\n# log the Table\\nwandb_logger.log_table(key=\"my_samples\", columns=columns, data=data)\\nYou can use Lightning\\'s Callbacks system to control when you log to Weights & Biases via the WandbLogger, in this example we log a sample of our validation images and predictions:', 'source': 'guides/integrations/lightning.md', 'score': 0.8296300999999999, 'relevance': 1}, {'content': \"Run\\nView source on GitHub\\nA unit of computation logged by wandb. Typically, this is an ML experiment.\\nRun(\\n    settings: Settings,\\n    config: Optional[Dict[str, Any]] = None,\\n    sweep_config: Optional[Dict[str, Any]] = None,\\n    launch_config: Optional[Dict[str, Any]] = None\\n) -> None\\nCreate a run with `wandb.init()`:\\nimport wandb\\nrun = wandb.init()\\nThere is only ever at most one active `wandb.Run` in any process,\\nand it is accessible as `wandb.run`:\\nimport wandb\\nassert wandb.run is None\\nwandb.init()\\nassert wandb.run is not None\\nanything you log with `wandb.log` will be sent to that run.\\nIf you want to start more runs in the same script or notebook, you'll need to\\nfinish the run that is in-flight. Runs can be finished with `wandb.finish` or\\nby using them in a `with` block:\\nimport wandb\\nwandb.init()\\nwandb.finish()\\nassert wandb.run is None\\nwith wandb.init() as run:\\n    pass  # log data here\\nassert wandb.run is None\", 'source': 'ref/python/run.md', 'score': 0.99928474, 'relevance': 0}, {'content': 'Examples:\\nSet where the run is logged\\nYou can change where the run is logged, just like changing\\nthe organization, repository, and branch in git:\\nimport wandb\\nuser = \"geoff\"\\nproject = \"capsules\"\\ndisplay_name = \"experiment-2021-10-31\"\\nwandb.init(entity=user, project=project, name=display_name)\\nAdd metadata about the run to the config\\nPass a dictionary-style object as the `config` keyword argument to add\\nmetadata, like hyperparameters, to your run.\\nimport wandb\\nconfig = {\"lr\": 3e-4, \"batch_size\": 32}\\nconfig.update({\"architecture\": \"resnet\", \"depth\": 34})\\nwandb.init(config=config)\\nRaises\\n`Error`\\nif some unknown or internal error happened during the run initialization.\\n`AuthenticationError`\\nif the user failed to provide valid credentials.\\n`CommError`\\nif there was a problem communicating with the WandB server.\\n`UsageError`\\nif the user provided invalid arguments.\\n`KeyboardInterrupt`\\nif user interrupts the run.\\nReturns\\nA `Run` object.', 'source': 'ref/python/init.md', 'score': 0.99253935, 'relevance': 0}, {'content': 'Runs\\nA single unit of computation logged by W&B is called a run. You can think of a W&B run as an atomic element of your whole project. You should initiate a new run when you:\\nTrain a model\\nChange a hyperparameter\\nUse a different model\\nLog data or a model as a W&B Artifact\\nDownload a W&B Artifact\\nFor example, during a sweep, W&B explores a hyperparameter search space that you specify. Each new hyperparameter combination created by the sweep is implemented and recorded as a unique run. \\n:::tip\\nSome key things to consider when you create and manage runs:\\n* Anything you log with `wandb.log` is recorded in that run.  For more information on how log objects in W&B, see Log Media and Objects. \\n* Each run is associated to a specific W&B project.\\n* View runs and their properties within the run\\'s project workspace on the W&B App UI.\\n* There is only at most one active wandb.Run in any process,\\nand it is accessible as `wandb.run`.\\n:::\\nCreate a run\\nCreate a W&B run with wandb.init():\\nimport wandb\\nrun = wandb.init()\\nWe recommend you specify a project name and a W&B entity when you create a new run. W&B creates a new project (if the project does not already exist) within the W&B entity you provide. If the project already exists, W&B stores the run in that project.\\nFor example, the following code snippet initializes a run that is stored in a project called `model_registry_example` that is scoped within a `wandbee` entity:\\nimport wandb\\nrun = wandb.init(entity=\"wandbee\", \\\\\\n        project=\"model_registry_example\")\\nW&B prints the name of the run that is created along with a URL path to find out more information about that specific run. \\nFor example, the code snippet above produces this output:', 'source': 'guides/runs/intro.md', 'score': 0.9912548, 'relevance': 0}, {'content': 'Get the command that ran the run\\nEach run captures the command that launched it on the run overview page. To pull this command down from the API, you can run:\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nmeta = json.load(run.file(\"wandb-metadata.json\").download())\\nprogram = [\"python\"] + [meta[\"program\"]] + meta[\"args\"]', 'source': 'guides/track/public-api-guide.md', 'score': 0.9897908999999999, 'relevance': 0}, {'content': 'You can then log the parent_span to W&B like as below. \\nrun = wandb.init(name=\"manual_span_demo\", project=\"wandb_prompts_demo\")\\nrun.log({\"trace\": trace_tree.WBTraceTree(parent_span)})\\nrun.finish()\\nClicking on the W&B Run link generated will take you to a workspace where you can inspect the Trace created.', 'source': 'tutorials/prompts.md', 'score': 0.98942953, 'relevance': 0}])"
        ],
        [
         "1",
         "is there a method to auto delete log files from my disk once a log completes?",
         "Currently, Weights & Biases does not provide a built-in method to automatically delete local log files once a log completes directly through its API or settings. However, you can manage log files manually or implement a custom solution to handle this task.\n\nHere are a couple of approaches you might consider:\n\n### 1. Manual Cleanup\nAfter your experiments are complete and you've ensured that all data is synced with the W&B servers, you can manually delete the log files from your disk.\n\n### 2. Custom Script for Automatic Cleanup\nYou can write a custom script that runs at the end of your experiments to delete the log files. Here’s a simple example using Python:\n\n```python\nimport os\nimport shutil\n\ndef clean_log_dir(directory):\n    if os.path.exists(directory):\n        shutil.rmtree(directory)\n        print(f\"Log directory {directory} has been removed\")\n    else:\n        print(\"Directory does not exist\")\n\n# Example usage\nlog_directory = './wandb'\nclean_log_dir(log_directory)\n```\n\nThis script checks if the directory exists and then removes it along with all its contents. You would need to call this function at the end of your experiment or training session.\n\n### 3. Using Python atexit\nIf you want the cleanup to happen automatically when your Python script exits, you can use the `atexit` module which allows you to register cleanup functions. Here's how you might integrate it:\n\n```python\nimport atexit\nimport shutil\n\ndef cleanup_logs():\n    log_dir = './wandb'\n    if os.path.exists(log_dir):\n        shutil.rmtree(log_dir)\n        print(f\"Cleaned up {log_dir}\")\n\natexit.register(cleanup_logs)\n```\n\nThis will ensure that the log directory is cleaned up whenever the Python interpreter terminates normally.\n\n### Best Practices\n- **Ensure Data Integrity**: Always make sure that all your data is properly synced to the W&B servers before deleting any local files to avoid data loss.\n- **Regular Monitoring**: Keep an eye on the disk space used by W&B logs, especially when running multiple or long-running experiments.\n- **Use W&B Artifacts**: For managing outputs and datasets more systematically, consider using [W&B Artifacts](https://docs.wandb.ai/guides/artifacts) which also supports versioning and is integrated with the W&B dashboard.\n\nThese methods provide flexibility depending on your specific needs and environment.",
         "WeaveList([{'content': 'wandb artifact cache cleanup\\nUsage\\n`wandb artifact cache cleanup [OPTIONS] TARGET_SIZE`\\nSummary\\nClean up less frequently used files from the artifacts cache\\nOptions\\nOption\\nDescription\\n--remove-temp / --no-remove-temp\\nRemove temp files', 'source': 'ref/cli/wandb-artifact/wandb-artifact-cache/wandb-artifact-cache-cleanup.md', 'score': 0.0001323819, 'relevance': 2}, {'content': 'Log Media and Objects in Experiments\\nLog Media and Objects in Experiments\\nLog a dictionary of metrics, media, or custom objects to a step with the W&B Python SDK. W&B collects the key-value pairs during each step and stores them in one unified dictionary each time you log data with `wandb.log()`. Data logged from your script is saved locally to your machine in a directory called `wandb`, then synced to the W&B cloud or your private server. \\n:::info\\nKey-value pairs are stored in one unified dictionary only if you pass the same value for each step. W&B writes all of the collected keys and values to memory if you log a different value for `step`.\\n:::\\nEach call to `wandb.log` is a new `step` by default. W&B uses steps as the default x-axis when it creates charts and panels. You can optionally create and use a custom x-axis or capture a custom summary metric. For more information, see Customize log axes.\\n:::caution\\nUse `wandb.log()` to log consecutive values for each `step`: 0, 1, 2, and so on. It is not possible to write to a specific history step. W&B only writes to the \"current\" and \"next\" step.\\n:::\\nAutomatically logged data\\nW&B automatically logs the following information during a W&B Experiment:\\nSystem metrics: CPU and GPU utilization, network, etc. These are shown in the System tab on the run page. For the GPU, these are fetched with nvidia-smi.\\nCommand line: The stdout and stderr are picked up and show in the logs tab on the run page.\\nTurn on Code Saving in your account\\'s Settings page to log:\\nGit commit: Pick up the latest git commit and see it on the overview tab of the run page, as well as a `diff.patch` file if there are any uncommitted changes.\\nDependencies: The `requirements.txt` file will be uploaded and shown on the files tab of the run page, along with any files you save to the `wandb` directory for the run.', 'source': 'guides/track/log/intro.md', 'score': 1.35253e-05, 'relevance': 2}, {'content': \"Storage\\nIf you are approaching or exceeding your storage limit, there are multiple paths forward to manage your data. The path that's best for you will depend on your account type and your current project setup.\\nManage storage consumption\\nW&B offers different methods of optimizing your storage consumption:\\nUse\\xa0reference artifacts\\xa0to track files saved outside the W&B system, instead of uploading them to W&B storage.\\nUse an external cloud storage bucket for storage. (Enterprise only)\\nDelete data\\nYou can also choose to delete data to remain under your storage limit. There are several ways to do this:\\nDelete data interactively with the app UI.\\nSet a TTL policy on Artifacts so they are automatically deleted.\", 'source': 'guides/app/features/storage.md', 'score': 0.00024156630000000002, 'relevance': 1}, {'content': 'File count\\nKeep the total number of files uploaded for a single run under 1,000. You can use W&B Artifacts when you need to log a large number of files. Exceeding 1,000 files in a single run can slow down your run pages.\\nPython script performance\\nThere are a few ways that your performance of your python script is reduced:\\nThe size of your data is too large. Large data sizes could introduce a >1 ms overhead to the training loop.\\nThe speed of your network and the how the W&B backend is configured\\nCalling `wandb.log` more than a few times per second. This is due to a small latency added to the training loop every time `wandb.log` is called.\\n:::info\\nIs frequent logging slowing your training runs down? Check out this Colab for methods to get better performance by changing your logging strategy.\\n:::\\nW&B does not assert any limits beyond rate limiting. The W&B Python SDK automatically completes an exponential \"backoff\" and \"retry\" requests that exceed limits. W&B Python SDK responds with a “Network failure” on the command line. For unpaid accounts, W&B may reach out in extreme cases where usage exceeds reasonable thresholds.\\nRate limits\\nW&B SaaS Cloud API implements a rate limit to maintain system integrity and ensure availability. This measure prevents any single user from monopolizing available resources in the shared infrastructure, ensuring that the service remains accessible to all users. You may encounter a lower rate limit for a variety of reasons. \\n:::note\\nRate limits are subject to change.\\n:::\\nThe `wandb.log` calls in your script utilize a metrics logging API to log your training data to W&B. This API is engaged through either online or offline syncing. In either case, it imposes a rate limit quota limit in a rolling time window. This includes limits on total request size and request rate, where latter refers to the number of requests in a time duration. \\nRate limits are applied to each W&B project. So if you have 3 projects in a team, each project has its own rate limit quota. Users on Teams and Enterprise plans have higher rate limits than those on the Free plan.', 'source': 'guides/track/limits.md', 'score': 1.59366e-05, 'relevance': 1}, {'content': 'End a run\\nW&B automatically ends runs and logs data from that run to your W&B project. You can end a run manually with the run.finish command. For example:\\nimport wandb\\nrun = wandb.init()\\nrun.finish()\\n:::info\\nW&B suggests that you use the wandb.finish method at the end of the child process if you call wandb.init from a child process.\\n:::', 'source': 'guides/runs/intro.md', 'score': 0.0003500686, 'relevance': 0}, {'content': '3. View your OpenAI API inputs and responses\\nClick on the Weights & Biases run link generated by `autolog` in step 1. This will redirect you to your project workspace in the W&B App.\\nSelect a run you created to view the trace table, trace timeline and the model architecture of the OpenAI LLM used.\\n4. Disable autolog\\nWe recommend that you call `disable()` to close all W&B processes when you are finished using the OpenAI API.\\nautolog.disable()\\nNow your inputs and completions will be logged to Weights & Biases, ready for analysis or to be shared with colleagues.', 'source': 'guides/integrations/other/openai-api.md', 'score': 0.0002492325, 'relevance': 0}, {'content': 'log\\nView source\\nlog(\\n    data: Dict[str, Any],\\n    step: Optional[int] = None,\\n    commit: Optional[bool] = None,\\n    sync: Optional[bool] = None\\n) -> None', 'source': 'ref/python/run.md', 'score': 0.000179528, 'relevance': 0}, {'content': \"Audit logs\\nUse W&B Server audit logs to track user activity within your teams, and to conform to your enterprise governance requirements. The audit logs are JSON-formatted, and their access mechanism(s) depend on your W&B Server deployment type:\\nW&B Server Deployment type\\nAudit logs access mechanism(s)\\nSelf-managed\\nSynced to instance-level bucket every 10 minutes. Also available using the API.\\nDedicated Cloud with secure storage connector (BYOB)\\nSynced to instance-level bucket (BYOB) every 10 minutes. Also available using the API.\\nDedicated Cloud with W&B managed storage (without BYOB)\\nOnly available using the API.\\nOnce you've access to your audit logs, analyze those using your preferred tools, such as Pandas, Amazon Redshift, Google BigQuery, Microsoft Fabric, and more. You may need to transform the JSON-formatted audit logs into a format relevant to the tool before analysis. Information on how to transform your audit logs for specific tools is outside the scope of W&B documentation.\\n:::tip\\nAudit Log Retention: If a compliance, security or risk team in your organization requires audit logs to be retained for a specific period of time, W&B recommends to periodically transfer the logs from your instance-level bucket to a long-term retention storage. If you're instead using the API to access the audit logs, you can implement a simple script that runs periodically (like daily or every few days) to fetch any logs that may have been generated since the time of the last script run, and store those in a short-term storage for analysis or directly transfer to a long-term retention storage.\\n:::\\n:::note\\nAudit logs are not available for W&B Multi-tenant Cloud yet.\\n:::\", 'source': 'guides/hosting/monitoring-usage/audit-logging.md', 'score': 3.42685e-05, 'relevance': 0}, {'content': 'Examples of logging behavior\\nKind of Variable\\nbehavior\\nExample\\nData Type\\nInstance\\nAuto-logged\\n`self.accuracy`\\n`float`\\nInstance\\nLogged if `datasets=True`\\n`self.df`\\n`pd.DataFrame`\\nInstance\\nNot logged if `datasets=False`\\n`self.df`\\n`pd.DataFrame`\\nLocal\\nNever logged\\n`accuracy`\\n`float`\\nLocal\\nNever logged\\n`df`\\n`pd.DataFrame`\\nDoes this track artifact lineage?\\nYes! If you have an artifact that is an output of step A and an input to step B, we automatically construct the lineage DAG for you.\\nFor an example of this behavior, please see this notebook and its corresponding W&B Artifacts page', 'source': 'guides/integrations/other/metaflow.md', 'score': 2.90834e-05, 'relevance': 0}, {'content': 'Log Images, Tables, Text, Audio and More\\nIn addition to metrics, you can log plots, histograms, tables, text and media such as images, videos, audios, 3D and more.\\nSome considerations when logging data include:\\nHow often should the metric be logged? Should it be optional?\\nWhat type of data could be helpful in visualizing?\\nFor images, you can log sample predictions, segmentation masks etc to see the evolution over time.\\nFor text, you can log tables of sample predictions for later exploration.\\nRefer to Log Data with wandb.log for a full guide on logging media, objects, plots and more.\\nDistributed Training\\nFor frameworks supporting distributed environments, you can adapt any of the following workflows:\\nDetect which is the “main” process and only use `wandb` there. Any required data coming from other processes must be routed to the main process first. (This workflow is encouraged).\\nCall `wandb` in every process and auto-group them by giving them all the same unique `group` name\\nSee Log Distributed Training Experiments for more details\\nLogging Model Checkpoints And More\\nIf your framework uses or produces models or datasets, you can log them for full traceability and have wandb automatically monitor your entire pipeline through W&B Artifacts.\\nWhen using Artifacts, it might be useful but not necessary to let your users define:\\nThe ability to log model checkpoints or datasets (in case you want to make it optional)\\nThe path/reference of the artifact being used as input if any. For example “user/project/artifact”\\nThe frequency for logging Artifacts', 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 1.92232e-05, 'relevance': 0}])"
        ],
        [
         "2",
         "Is it possible to group by tags within the UI?",
         "In Weights & Biases, while tags are very useful for labeling and organizing runs with specific features, the platform does not support grouping runs by tags directly in the UI. This limitation is due to the fact that a run can have multiple tags, which complicates the grouping logic.\n\nHowever, you can use tags in conjunction with other features to organize and filter your runs effectively. For example, you can use tags to mark runs and then filter the runs table in the UI to display only those with specific tags. This allows you to manually group and compare runs based on the tags they share.\n\nIf you need to group runs, it's recommended to use the `group` or `job_type` parameters in `wandb.init()` to organize runs into larger experiments or categories, which can then be grouped in the UI.\n\nFor more detailed information on how to use tags and other grouping methods in Weights & Biases, you can refer to the [guide on grouping runs](https://docs.wandb.ai/guides/runs/grouping).",
         "WeaveList([{'content': \"Turn off grouping\\nClick the grouping button and clear group fields at any time, which returns the table and graphs to their ungrouped state.\\nGrouping graph settings\\nClick the edit button in the upper right corner of a graph and select the Advanced tab to change the line and shading. You can select the mean, minimum, or maximum value for the line in each group. For the shading, you can turn off shading, and show the min and max, the standard deviation, and the standard error.\\nCommon Questions\\nCan we group runs by tags?\\nBecause a run can have multiple tags we don't support grouping by this field. Our recommendation would be to add a value to the config object of these runs and then group by this config value. You can do this with our API.\", 'source': 'guides/runs/grouping.md', 'score': 0.9911183, 'relevance': 2}, {'content': 'Group Runs\\nGroup W&B Runs\\nGroup individual jobs into experiments by passing a unique group name to wandb.init().\\nUse Cases\\nDistributed training: Use grouping if your experiments are split up into different pieces with separate training and evaluation scripts that should be viewed as parts of a larger whole.\\nMultiple processes: Group multiple smaller processes together into an experiment.\\nK-fold cross-validation: Group together runs with different random seeds to see a larger experiment. Here\\'s an example of k-fold cross-validation with sweeps and grouping.\\nThere are three ways to set grouping:\\n1. Set group in your script\\nPass an optional group and job_type to wandb.init(). This gives you a dedicated group page for each experiment, which contains the individual runs. For example:`wandb.init(group=\"experiment_1\", job_type=\"eval\")`\\n2. Set a group environment variable\\nUse `WANDB_RUN_GROUP` to specify a group for your runs as an environment variable. For more on this, check our docs for Environment Variables. Group should be unique within your project and shared by all runs in the group. You can use `wandb.util.generate_id()` to generate a unique 8 character string to use in all your processes— for example, `os.environ[\"WANDB_RUN_GROUP\"] = \"experiment-\" + wandb.util.generate_id()`\\n3. Toggle grouping in the UI\\nYou can dynamically group by any config column. For example, if you use `wandb.config` to log batch size or learning rate, you can then group by those hyperparameters dynamically in the web app.\\nDistributed training with grouping\\nSuppose you set grouping in `wandb.init()`, we will group runs by default in the UI. You can toggle this on and off by clicking the Group button at the top of the table. Here\\'s an example project generated from sample code where we set grouping. You can click on each \"Group\" row in the sidebar to get to a dedicated group page for that experiment.\\nFrom the project page above, you can click a Group in the left sidebar to get to a dedicated page like this one:\\nGrouping dynamically in the UI\\nYou can group runs by any column, for example by hyperparameter. Here\\'s an example of what that looks like:\\nSidebar: Runs are grouped by the number of epochs.\\nGraphs: Each line represents the group\\'s mean, and the shading indicates the variance. This behavior can be changed in the graph settings.', 'source': 'guides/runs/grouping.md', 'score': 0.05877655700000001, 'relevance': 2}, {'content': '`job_type`\\n(str, optional) Specify the type of run, which is useful when you\\'re grouping runs together into larger experiments using group. For example, you might have multiple jobs in a group, with job types like train and eval. Setting this makes it easy to filter and group similar runs together in the UI so you can compare apples to apples.\\n`tags`\\n(list, optional) A list of strings, which will populate the list of tags on this run in the UI. Tags are useful for organizing runs together, or applying temporary labels like \"baseline\" or \"production\". It\\'s easy to add and remove tags in the UI, or filter down to just runs with a specific tag. If you are resuming a run, its tags will be overwritten by the tags you pass to `wandb.init()`. If you want to add tags to a resumed run without overwriting its existing tags, use `run.tags += [\"new_tag\"]` after `wandb.init()`.\\n`name`\\n(str, optional) A short display name for this run, which is how you\\'ll identify this run in the UI. By default, we generate a random two-word name that lets you easily cross-reference runs from the table to charts. Keeping these run names short makes the chart legends and tables easier to read. If you\\'re looking for a place to save your hyperparameters, we recommend saving those in config.\\n`notes`\\n(str, optional) A longer description of the run, like a `-m` commit message in git. This helps you remember what you were doing when you ran this run.\\n`dir`\\n(str or pathlib.Path, optional) An absolute path to a directory where metadata will be stored. When you call `download()` on an artifact, this is the directory where downloaded files will be saved. By default, this is the `./wandb` directory.', 'source': 'ref/python/init.md', 'score': 0.9863105400000001, 'relevance': 1}, {'content': 'Tags\\nTags can be used to label runs with particular features that might not be obvious from the logged metrics or Artifact data -- this run\\'s model is `in_production`, that run is `preemptible`, this run represents the `baseline`.\\nHow to add tags\\nYou can add tags to a run when it is created: `wandb.init(tags=[\"tag1\", \"tag2\"])` .\\nYou can also update the tags of a run during training (e.g. if a particular metrics crosses a pre-defined threshold):\\nrun = wandb.init(entity=\"entity\", project=\"capsules\", tags=[\"debug\"])\\n...\\nif current_loss < threshold:\\n    run.tags = run.tags + (\"release_candidate\",)\\nThere are also several ways to add tags after runs have been logged to W&B.\\nAfter a run is created, you can update tags using our public API like so:\\nrun = wandb.Api().run(\"{entity}/{project}/{run-id}\")\\nrun.tags.append(\"tag1\")  # you can choose tags based on run data here\\nrun.update()\\nYou can read more about how to use the Public API in the reference documentation or guide.\\nThis method is best suited to tagging large numbers of runs with the same tag or tags.\\nIn the runs sidebar of the Project Page,  click the table icon in the upper-right.  This will expand the sidebar into the full runs table.\\nHover over a run in the table to see a checkbox on the left or look in the header row for a checkbox that will allow you to select all runs.\\nClick the checkbox to enable bulk actions. Select the runs to which you\\'d like to apply your tag(s).\\nClick the Tag button above the rows of runs.\\nType a tag you\\'d like to add and click \"Add\" below the text box to add a new tag.\\nThis method is best suited to applying a tag or tags to a single run by hand.\\nIn the left sidebar of the Run Page, click the top Overview tab.\\nNext to \"Tags\" is a gray ➕ button. Click on that plus to add a tag.\\nType a tag you\\'d like to add and click \"Add\" below the text box to add a new tag.', 'source': 'guides/app/features/tags.md', 'score': 0.0049822700000000004, 'relevance': 1}, {'content': 'How to remove tags\\nTags can also be removed from runs via the UI.\\nThis method is best suited to removing tags from a large numbers of runs.\\nIn the runs sidebar of the Project Page,  click the table icon in the upper-right.  This will expand the sidebar into the full runs table.\\nHover over a run in the table to see a checkbox on the left or look in the header row for a checkbox that will allow you to select all runs.\\nClick either checkbox to enable bulk actions. Select the runs to from which you\\'d like to remove your tag(s).\\nClick the Tag button above the rows of runs.\\nClick the checkbox next to a tag to remove it from the run.\\nIn the left sidebar of the Run Page, click the top Overview tab. The tags on the run are visible here.\\nHover over a tag and click the \"x\" to remove it from the run.', 'source': 'guides/app/features/tags.md', 'score': 0.045948207000000005, 'relevance': 0}, {'content': 'Model tags\\nModel tags are keywords or labels that belong to one or more registered models.\\nUse model tags to organize registered models into categories and to search over those categories in the Model Registry\\'s search bar. Model tags appear at the top of the Registered Model Card. You might choose to use them to group your registered models by ML task, owning team, or priority. The same model tag can be added to multiple registered models to allow for grouping. \\n:::info\\nModel tags, which are labels applied to registered models for grouping and discoverability, are different from model aliases. Model aliases are unique identifiers or nicknames that you use to fetch a model version programatically. To learn more about using tags to organize the tasks in your Model Registry, see Organize models.\\n:::\\nModel artifact\\nA model artifact is a collection of logged model versions. Model versions are stored in a model artifact in the order they are logged to the model artifact. \\nA model artifact can contain one or more model versions. A model artifact can be empty if no model versions are logged to it. \\nFor example, suppose you create a model artifact. During model training, you periodically save your model during checkpoints. Each checkpoint corresponds to its own model version. All of the model versions created during your model training and checkpoint saving are stored in the same model artifact you created at the beginning of your training script.\\nThe proceeding image shows a model artifact that contains three model versions: v0, v1, and v2.\\nView an example model artifact here.\\nRegistered model\\nA registered model is a collection of pointers (links) to model versions. You can think of a registered model as a folder of \"bookmarks\" of candidate models for the same ML task. Each \"bookmark\" of a registered model is a pointer to a model version that belongs to a model artifact. You can use model tags to group your registered models. \\nRegistered models often represent candidate models for a single modeling use case or task. For example, you might create registered model for different image classification task based on the model you use: \"ImageClassifier-ResNet50\", \"ImageClassifier-VGG16\", \"DogBreedClassifier-MobileNetV2\" and so on. Model versions are assigned version numbers in the order in which they were linked to the registered model.\\nView an example Registered Model here.', 'source': 'guides/model_registry/model-management-concepts.md', 'score': 0.0282746, 'relevance': 0}, {'content': \"How can I log additional metrics after a run completes?\\nThere are several ways to do this.\\nFor complicated workflows, we recommend using multiple runs and setting group parameters in wandb.init to a unique value in all the processes that are run as part of a single experiment. The runs table will automatically group the table by the group ID and the visualizations will behave as expected. This will allow you to run multiple experiments and training runs as separate processes log all the results into a single place.\\nFor simpler workflows, you can call `wandb.init` with `resume=True` and `id=UNIQUE_ID` and then later call `wandb.init` with the same `id=UNIQUE_ID`. Then you can log normally with wandb.log or `wandb.summary` and the runs values will update.\\nPerformance\\nWill wandb slow down my training?\\nW&B should have a negligible effect on your training performance if you use it normally. Normal use of wandb means logging less than once a second and logging less than a few megabytes of data at each step. W&B runs in a separate process and the function calls don't block, so if the network goes down briefly or there are intermittent read write issues on disk it should not affect your performance. It is possible to log a huge amount of data quickly, and if you do that you might create disk I/O issues. If you have any questions, please don't hesitate to contact us.\\nHow many runs to create per project?\\nWe recommend you have roughly 10k runs per project max for performance reasons.\\nBest practices to organize hyperparameter searches\\nIf 10k runs per project (approx.) is a reasonable limit then our recommendation would be to set tags in `wandb.init()` and have a unique tag for each search. This means that you'll easily be able to filter the project down to a given search by clicking that tag in the Project Page in the Runs Table. For example `wandb.init(tags='your_tag')`  docs for this can be found here.\", 'source': 'guides/technical-faq/metrics-and-performance.md', 'score': 0.0036642202000000002, 'relevance': 0}, {'content': 'Organize models\\nUse model tags to organize registered models into categories and to search over those categories. \\nNavigate to the W&B Model Registry app at https://wandb.ai/registry/model.\\nSelect View details next to the name of the registered model you want to add a model tag to. \\n    \\nScroll to the Model card section.\\nClick the plus button (+) next to the Tags field.\\nType in the name for your tag or search for a pre-existing model tag.\\n    For example. the following image shows multiple model tags added to a registered model called FineTuned-Review-Autocompletion:', 'source': 'guides/model_registry/organize-models.md', 'score': 0.0032476764000000003, 'relevance': 0}, {'content': 'WANDB_IGNORE_GLOBS\\nSet this to a comma separated list of file globs to ignore. These files will not be synced to the cloud.\\nWANDB_JOB_NAME\\nSpecify a name for any jobs created by `wandb`. For more information, see create a job\\nWANDB_JOB_TYPE\\nSpecify the job type, like \"training\" or \"evaluation\" to indicate different types of runs. See grouping for more info.\\nWANDB_MODE\\nIf you set this to \"offline\" wandb will save your run metadata locally and not sync to the server. If you set this to \"disabled\" wandb will turn off completely.\\nWANDB_NAME\\nThe human-readable name of your run. If not set it will be randomly generated for you\\nWANDB_NOTEBOOK_NAME\\nIf you\\'re running in jupyter you can set the name of the notebook with this variable. We attempt to auto detect this.\\nWANDB_NOTES\\nLonger notes about your run. Markdown is allowed and you can edit this later in the UI.\\nWANDB_PROJECT\\nThe project associated with your run. This can also be set with `wandb init`, but the environmental variable will override the value.\\nWANDB_RESUME\\nBy default this is set to never. If set to auto wandb will automatically resume failed runs. If set to must forces the run to exist on startup. If you want to always generate your own unique ids, set this to allow and always set WANDB_RUN_ID.\\nWANDB_RUN_GROUP\\nSpecify the experiment name to automatically group runs together. See grouping for more info.\\nWANDB_RUN_ID\\nSet this to a globally unique string (per project) corresponding to a single run of your script. It must be no longer than 64 characters. All non-word characters will be converted to dashes. This can be used to resume an existing run in cases of failure.\\nWANDB_SILENT\\nSet this to true to silence wandb log statements. If this is set all logs will be written to WANDB_DIR/debug.log\\nWANDB_SHOW_RUN\\nSet this to true to automatically open a browser with the run url if your operating system supports it.\\nWANDB_TAGS\\nA comma separated list of tags to be applied to the run.\\nWANDB_USERNAME\\nThe username of a member of your team associated with the run. This can be used along with a service account API key to enable attribution of automated runs to members of your team.', 'source': 'guides/track/environment-variables.md', 'score': 0.0030396497, 'relevance': 0}, {'content': \"Runs Table\\nOn the project page, we show runs in a sidebar. Expand the sidebar to view a table of hyperparameters and summary metrics across runs.\\nSearch run names\\nWe support full regex search on run names in the table. When you type a query in the search box, that will filter down the visible runs in the graphs on the workspace as well as filtering the rows of the table.\\nFilter and sort runs by minimum and maximum values\\nSort the runs table by the minimum or maximum value of a logged metric. This is particularly useful if you want to view the best (or worst) recorded value.\\nThe following steps describe how to sort the run table by a specific metric based on the minimum or maximum recorded value:\\nHover your mouse over the column with the metric you want to sort with.\\nSelect the kebob menu (three vertical lines).\\nFrom the dropdown, select either Show min or Show max.\\nFrom the same dropdown, select Sort by asc or Sort by desc to sort in ascending or descending order, respectively. \\nWhat to do in case regex fails?\\nIf regex doesn't provide you the desired results, you can make use of tags to filter out the runs in Runs Table. Tags can be added either on run creation or after they're finished. Once the tags are added to a run, you can add a tag filter as shown in the gif below.\\nSearch End Time for runs\\nWe provide a column named `End Time` that logs that last heartbeat from the client process. The field is hidden by default.\\nResize the sidebar\\nWould you like to make more space for the graphs on the project page? Click and drag the edge of the column header to resize the sidebar. You'll still be able to click on the eye icon to turn on and off runs on the graphs.\\nAdd sidebar columns\\nOn the project page, we show runs in a sidebar. To show more columns:\\nClick the button in the upper right corner of the sidebar to expand the table.\\nOn a column header, click the dropdown menu to pin a column.\\nPinned columns will be available in the sidebar when you collapse the table.\\nHere's a screen capture. I expand the table, pin two columns, collapse the table, then resize the sidebar.\\nBulk select runs\\nDelete multiple runs at once, or tag a group of runs— bulk selection makes it easier to keep the runs table organized.\", 'source': 'guides/app/features/runs-table.md', 'score': 0.001133569, 'relevance': 0}])"
        ],
        [
         "3",
         "why did wandb stop logging model parameters with a huggingface model.  I have used in the past and it worked.  Maybe I changed an environement or config, how do I reset?",
         "It sounds like you're experiencing issues with Weights & Biases not logging model parameters when using a Hugging Face model. This could be due to several reasons, such as changes in environment variables or configuration settings. Here are some steps to troubleshoot and reset your configuration:\n\n### 1. Check `WANDB_WATCH` Environment Variable\nEnsure that the `WANDB_WATCH` environment variable is set correctly. This variable controls whether model gradients and parameters are logged. You should set it to `all` to log both gradients and parameters.\n\n```python\nimport os\nos.environ['WANDB_WATCH'] = 'all'  # Log histograms of gradients and parameters\n```\n\nOr, you can set this in your terminal before running your script:\n```bash\nexport WANDB_WATCH=all\n```\n\n### 2. Verify Integration in Your Script\nMake sure that the `report_to` argument in your `TrainingArguments` or script is set to `wandb`. This ensures that the Hugging Face integration with Weights & Biases is active.\n\n```python\nfrom transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    report_to=\"wandb\",\n    ...\n)\n```\n\n### 3. Check for Any Overrides in Your Code\nSometimes, specific settings in your code might override the environment variables. Look for any lines in your script that might be setting `WANDB_WATCH` to `false` or modifying other related settings.\n\n### 4. Reset W&B Environment Variables\nIf you suspect that your environment variables are not set correctly, you can reset them. Here's how you can set them to typical values for logging with Hugging Face:\n\n```python\n# Set environment variables\nos.environ['WANDB_WATCH'] = 'all'  # Ensure all parameters and gradients are logged\nos.environ['WANDB_LOG_MODEL'] = 'true'  # Log model as an artifact\nos.environ['WANDB_PROJECT'] = 'your_project_name'  # Specify your project name\n```\n\n### 5. Re-login to W&B\nSometimes, simply re-authenticating can resolve issues. Make sure you are logged in to W&B:\n\n```python\nimport wandb\nwandb.login()\n```\n\n### 6. Update W&B and Hugging Face Libraries\nEnsure that you are using the latest versions of both `wandb` and `transformers` libraries, as updates often fix bugs and improve integration features.\n\n```bash\npip install --upgrade wandb transformers\n```\n\n### 7. Review W&B Dashboard\nCheck your W&B dashboard to see if there are any error messages or warnings that might give you more context about what's going wrong.\n\nBy following these steps, you should be able to resolve the issue with logging model parameters in Weights & Biases when using a Hugging Face model. If the problem persists, consider reaching out to W&B support or checking the [community forums](https://wandb.me/community) for more help.",
         "WeaveList([{'content': 'Additional W&B settings\\nFurther configuration of what is logged with `Trainer` is possible by setting environment variables. A full list of W&B environment variables can be found here.\\nEnvironment Variable\\nUsage\\n`WANDB_PROJECT`\\nGive your project a name (`huggingface` by default)\\n`WANDB_LOG_MODEL`\\nLog the model checkpoint as a W&B Artifact (`false` by default) \\n`false` (default): No model checkpointing `checkpoint`: A checkpoint will be uploaded every args.save_steps (set in the Trainer\\'s TrainingArguments). `end`: The final model checkpoint will be uploaded at the end of training.\\n`WANDB_WATCH`\\nSet whether you\\'d like to log your models gradients, parameters or neither\\n`false` (default): No gradient or parameter logging `gradients`: Log histograms of the gradients `all`: Log histograms of gradients and parameters\\n`WANDB_DISABLED`\\nSet to `true` to disable logging entirely (`false` by default)\\n`WANDB_SILENT`\\nSet to `true` to silence the output printed by wandb (`false` by default)\\nWANDB_WATCH=all\\nWANDB_SILENT=true\\n%env WANDB_WATCH=all\\n%env WANDB_SILENT=true\\nCustomize wandb.init\\nThe `WandbCallback` that `Trainer` uses will call `wandb.init` under the hood when `Trainer` is initialized. You can alternatively set up your runs manually by calling `wandb.init` before the`Trainer` is initialized. This gives you full control over your W&B run configuration.\\nAn example of what you might want to pass to `init` is below. For more details on how to use `wandb.init`, check out the reference documentation.\\nwandb.init(\\n    project=\"amazon_sentiment_analysis\",\\n    name=\"bert-base-high-lr\",\\n    tags=[\"baseline\", \"high-lr\"],\\n    group=\"bert\",\\n)', 'source': 'guides/integrations/huggingface.md', 'score': 0.96260285, 'relevance': 2}, {'content': '4) Turn on model checkpointing\\nUsing Weights & Biases\\' Artifacts, you can store up to 100GB of models and datasets for free and then use the Weights & Biases Model Registry to register models to prepare them for staging or deployment in your production environment.\\nLogging your Hugging Face model checkpoints to Artifacts can be done by setting the `WANDB_LOG_MODEL` environment variable to one of `end` or `checkpoint` or `false`: \\n`checkpoint`: a checkpoint will be uploaded every `args.save_steps` from the TrainingArguments. \\n`end`:  the model will be uploaded at the end of training. \\nUse `WANDB_LOG_MODEL` along with `load_best_model_at_end` to upload the best model at the end of training.\\nimport os\\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\\nWANDB_LOG_MODEL=\"checkpoint\"\\n%env WANDB_LOG_MODEL=\"checkpoint\"\\nAny Transformers `Trainer` you initialize from now on will upload models to your W&B project. The model checkpoints you log will be viewable through the Artifacts UI, and include the full model lineage (see an example model checkpoint in the UI here. \\n:::info\\nBy default, your model will be saved to W&B Artifacts as `model-{run_id}` when `WANDB_LOG_MODEL` is set to `end` or `checkpoint-{run_id}` when `WANDB_LOG_MODEL` is set to `checkpoint`.\\nHowever, If you pass a run_name in your `TrainingArguments`, the model will be saved as `model-{run_name}` or `checkpoint-{run_name}`.\\n:::\\nW&B Model Registry\\nOnce you have logged your checkpoints to Artifacts, you can then register your best model checkpoints and centralize them across your team using the Weights & Biases Model Registry. Here you can organize your best models by task, manage model lifecycle, facilitate easy tracking and auditing throughout the ML lifecyle, and automate downstream actions with webhooks or jobs. \\nSee the Model Registry documentation for how to link a model Artifact to the Model Registry.', 'source': 'guides/integrations/huggingface.md', 'score': 0.6389479, 'relevance': 2}, {'content': 'Loading a saved model\\nIf you saved your model to W&B Artifacts with `WANDB_LOG_MODEL`, you can download your model weights for additional training or to run inference. You just load them back into the same Hugging Face architecture that you used before.\\n# Create a new run\\nwith wandb.init(project=\"amazon_sentiment_analysis\") as run:\\n    # Pass the name and version of Artifact\\n    my_model_name = \"model-bert-base-high-lr:latest\"\\n    my_model_artifact = run.use_artifact(my_model_name)\\n    # Download model weights to a folder and return the path\\n    model_dir = my_model_artifact.download()\\n    # Load your Hugging Face model from that folder\\n    #  using the same model class\\n    model = AutoModelForSequenceClassification.from_pretrained(\\n        model_dir, num_labels=num_labels\\n    )\\n    # Do additional training, or run inference\\nResume training from a checkpoint\\nIf you had set `WANDB_LOG_MODEL=\\'checkpoint\\'` you can also resume training by you can using the `model_dir` as the `model_name_or_path` argument in your `TrainingArguments` and pass `resume_from_checkpoint=True` to `Trainer`.\\nlast_run_id = \"xxxxxxxx\"  # fetch the run_id from your wandb workspace\\n# resume the wandb run from the run_id\\nwith wandb.init(\\n    project=os.environ[\"WANDB_PROJECT\"],\\n    id=last_run_id,\\n    resume=\"must\",\\n) as run:\\n    # Connect an Artifact to the run\\n    my_checkpoint_name = f\"checkpoint-{last_run_id}:latest\"\\n    my_checkpoint_artifact = run.use_artifact(my_model_name)\\n    # Download checkpoint to a folder and return the path\\n    checkpoint_dir = my_checkpoint_artifact.download()\\n    # reinitialize your model and trainer\\n    model = AutoModelForSequenceClassification.from_pretrained(\\n        \"<model_name>\", num_labels=num_labels\\n    )\\n    # your awesome training arguments here.\\n    training_args = TrainingArguments()\\n    trainer = Trainer(model=model, args=training_args)\\n    # make sure use the checkpoint dir to resume training from the checkpoint\\n    trainer.train(resume_from_checkpoint=checkpoint_dir)', 'source': 'guides/integrations/huggingface.md', 'score': 0.684053, 'relevance': 1}, {'content': \"Hugging Face\\nTry in a Colab Notebook here →\\nVisualize your Hugging Face model's performance quickly with a seamless W&B integration.\\nCompare hyperparameters, output metrics, and system stats like GPU utilization across your models. \\n🤔 Why should I use W&B?\\nUnified dashboard: Central repository for all your model metrics and predictions\\nLightweight: No code changes required to integrate with Hugging Face\\nAccessible: Free for individuals and academic teams\\nSecure: All projects are private by default\\nTrusted: Used by machine learning teams at OpenAI, Toyota, Lyft and more\\nThink of W&B like GitHub for machine learning models— save machine learning experiments to your private, hosted dashboard. Experiment quickly with the confidence that all the versions of your models are saved for you, no matter where you're running your scripts.\\nW&B lightweight integrations works with any Python script, and all you need to do is sign up for a free W&B account to start tracking and visualizing your models.\\nIn the Hugging Face Transformers repo, we've instrumented the Trainer to automatically log training and evaluation metrics to W&B at each logging step.\\nHere's an in depth look at how the integration works: Hugging Face + W&B Report.\\n🚀 Install, Import, and Log in\\nInstall the Hugging Face and Weights & Biases libraries, and the GLUE dataset and training script for this tutorial.\\n- Hugging Face Transformers: Natural language models and datasets\\n- Weights & Biases: Experiment tracking and visualization\\n- GLUE dataset: A language understanding benchmark dataset\\n- GLUE script: Model training script for sequence classification\\n!pip install datasets wandb evaluate accelerate -qU\\n!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py\\n# the run_glue.py script requires transformers dev\\n!pip install -q git+https://github.com/huggingface/transformers\\n🖊️ Sign up for a free account →\\n🔑 Put in your API key\\nOnce you've signed up, run the next cell and click on the link to get your API key and authenticate this notebook.\\nimport wandb\\nwandb.login()\\nOptionally, we can set environment variables to customize W&B logging. See documentation.\\n# Optional: log both gradients and parameters\\n%env WANDB_WATCH=all\", 'source': 'tutorials/huggingface.md', 'score': 0.6166473, 'relevance': 1}, {'content': 'Hugging Face Transformers\\nThe Hugging Face Transformers library makes state-of-the-art NLP models like BERT and training techniques like mixed precision and gradient checkpointing easy to use. The W&B integration adds rich, flexible experiment tracking and model versioning to interactive centralized dashboards without compromising that ease of use.\\n🤗 Next-level logging in few lines\\nos.environ[\"WANDB_PROJECT\"] = \"<my-amazing-project>\"  # name your W&B project\\nos.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # log all model checkpoints\\nfrom transformers import TrainingArguments, Trainer\\nargs = TrainingArguments(..., report_to=\"wandb\")  # turn on W&B logging\\ntrainer = Trainer(..., args=args)\\n:::info\\nIf you\\'d rather dive straight into working code, check out this Google Colab.\\n:::\\nGetting started: track experiments\\n1) Sign Up, install the wandb library and log in\\na) Sign up for a free account\\nb) Pip install the `wandb` library\\nc) To log in in your training script, you\\'ll need to be signed in to you account at www.wandb.ai, then you will find your API key on the Authorize page.\\nIf you are using Weights and Biases for the first time you might want to check out our quickstart\\npip install wandb\\nwandb login\\n!pip install wandb\\nimport wandb\\nwandb.login()\\n2) Name the project\\nA Project is where all of the charts, data, and models logged from related runs are stored. Naming your project helps you organize your work and keep all the information about a single project in one place.\\nTo add a run to a project simply set the `WANDB_PROJECT` environment variable to the name of your project. The `WandbCallback` will pick up this project name environment variable and use it when setting up your run.\\nWANDB_PROJECT=amazon_sentiment_analysis\\n%env WANDB_PROJECT=amazon_sentiment_analysis\\nimport os\\nos.environ[\"WANDB_PROJECT\"]=\"amazon_sentiment_analysis\"\\n:::info\\nMake sure you set the project name before you initialize the `Trainer`.\\n:::\\nIf a project name is not specified the project name defaults to \"huggingface\".', 'source': 'guides/integrations/huggingface.md', 'score': 0.2724115, 'relevance': 1}, {'content': \"Run The Library With wandb As Optional\\nIf you want to make `wandb` optional when your users use your library, you can either:\\nDefine a `wandb` flag such as:\\ntrainer = my_trainer(..., use_wandb=True)\\npython train.py ... --use-wandb\\nOr, set `wandb` to be disabled in `wandb.init`\\nwandb.init(mode=“disabled”)\\nexport WANDB_MODE=disabled\\nor\\nwandb disabled\\nOr, set `wandb` to be offline - note this will still run `wandb`, it just won't try and communicate back to W&B over the internet\\nexport WANDB_MODE=offline\\nor\\nos.environ['WANDB_MODE'] = 'offline'\\nwandb offline\\nDefining A wandb Run Config\\nWith a `wandb` run config you can provide metadata about your model, dataset, and so on when you create a W&B Run. You can use this information to compare different experiments and quickly understand what are the main differences.\\nTypical config parameters you can log include:\\nModel name, version, architecture parameters etc\\nDataset name, version, number of train/val examples etc\\nTraining parameters such as learning rate, batch size, optimizer etc\\nThe following code snippet shows how to log a config:\\nconfig = {“batch_size”:32, …}\\nwandb.init(…, config=config)\\nUpdating The wandb config\\nUse `wandb.config.update` to update the config. Updating your configuration dictionary is useful when parameters are obtained after the dictionary was defined, for example you might want to add a model’s parameters after the model is instantiated.\\nwandb.config.update({“model_parameters” = 3500})\\nFor more information on how to define a config file, see Configure Experiments with wandb.config\\nLogging To W&B\", 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.16344544, 'relevance': 0}, {'content': 'import os\\nimport wandb\\nfrom tensorflow import keras\\nfrom tensorflow.keras import layers\\nconfig = {\"optimizer\": \"adam\", \"loss\": \"categorical_crossentropy\"}\\n# Initialize a W&B run\\nrun = wandb.init(entity=\"charlie\", project=\"mnist-experiments\", config=config)\\n# Hyperparameters\\nloss = run.config[\"loss\"]\\noptimizer = run.config[\"optimizer\"]\\nmetrics = [\"accuracy\"]\\nnum_classes = 10\\ninput_shape = (28, 28, 1)\\n# Training algorithm\\nmodel = keras.Sequential(\\n    [\\n        layers.Input(shape=input_shape),\\n        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\\n        layers.MaxPooling2D(pool_size=(2, 2)),\\n        layers.Flatten(),\\n        layers.Dropout(0.5),\\n        layers.Dense(num_classes, activation=\"softmax\"),\\n    ]\\n)\\n# Configure the model for training\\nmodel.compile(loss=loss, optimizer=optimizer, metrics=metrics)\\n# Save model\\nmodel_filename = \"model.h5\"\\nlocal_filepath = \"./\"\\nfull_path = os.path.join(local_filepath, model_filename)\\nmodel.save(filepath=full_path)\\n# Log the model to the W&B run\\nrun.log_model(path=full_path, name=\"MNIST\")\\nrun.finish()\\nWhen the user called `log_model`, a model artifact named `MNIST` was created and the file `model.h5` was added to the model artifact. Your terminal or notebook will print information of where to find information about the run the model was logged to.\\nView run different-surf-5 at: https://wandb.ai/charlie/mnist-experiments/runs/wlby6fuw\\nSynced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 0 other file(s)\\nFind logs at: ./wandb/run-20231206_103511-wlby6fuw/logs', 'source': 'guides/track/log/log-models.md', 'score': 0.07172112, 'relevance': 0}, {'content': '👟 Train the model\\nNext, call the downloaded training script run_glue.py and see training automatically get tracked to the Weights & Biases dashboard. This script fine-tunes BERT on the Microsoft Research Paraphrase Corpus— pairs of sentences with human annotations indicating whether they are semantically equivalent.\\n%env WANDB_PROJECT=huggingface-demo\\n%env TASK_NAME=MRPC\\n!python run_glue.py \\\\\\n  --model_name_or_path bert-base-uncased \\\\\\n  --task_name $TASK_NAME \\\\\\n  --do_train \\\\\\n  --do_eval \\\\\\n  --max_seq_length 256 \\\\\\n  --per_device_train_batch_size 32 \\\\\\n  --learning_rate 2e-4 \\\\\\n  --num_train_epochs 3 \\\\\\n  --output_dir /tmp/$TASK_NAME/ \\\\\\n  --overwrite_output_dir \\\\\\n  --logging_steps 50\\n👀 Visualize results in dashboard\\nClick the link printed out above, or go to wandb.ai to see your results stream in live. The link to see your run in the browser will appear after all the dependencies are loaded — look for the following output: \"wandb: 🚀 View run at [URL to your unique run]\"\\nVisualize Model Performance\\nIt\\'s easy to look across dozens of experiments, zoom in on interesting findings, and visualize highly dimensional data.\\nCompare Architectures\\nHere\\'s an example comparing BERT vs DistilBERT — it\\'s easy to see how different architectures effect the evaluation accuracy throughout training with automatic line plot visualizations.\\n📈 Track key information effortlessly by default\\nWeights & Biases saves a new run for each experiment. Here\\'s the information that gets saved by default:\\n- Hyperparameters: Settings for your model are saved in Config\\n- Model Metrics: Time series data of metrics streaming in are saved in Log\\n- Terminal Logs: Command line outputs are saved and available in a tab\\n- System Metrics: GPU and CPU utilization, memory, temperature etc.\\n🤓 Learn more!\\nDocumentation: docs on the Weights & Biases and Hugging Face integration\\nVideos: tutorials, interviews with practitioners, and more on our YouTube channel\\nContact: Message us at contact@wandb.com with questions', 'source': 'tutorials/huggingface.md', 'score': 0.06359858, 'relevance': 0}, {'content': 'Keras Models\\nTry in a Colab Notebook here →\\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\\nThis colab notebook introduces the `WandbModelCheckpoint` callback. Use this callback to log your model checkpoints to Weight and Biases Artifacts.\\n🌴 Setup and Installation\\nFirst, let us install the latest version of Weights and Biases. We will then authenticate this colab instance to use W&B.\\n!pip install -qq -U wandb\\nimport os\\nimport tensorflow as tf\\nfrom tensorflow.keras import layers\\nfrom tensorflow.keras import models\\nimport tensorflow_datasets as tfds\\n# Weights and Biases related imports\\nimport wandb\\nfrom wandb.integration.keras import WandbMetricsLogger\\nfrom wandb.integration.keras import WandbModelCheckpoint\\nIf this is your first time using W&B or you are not logged in, the link that appears after running `wandb.login()` will take you to sign-up/login page. Signing up for a free account is as easy as a few clicks.\\nwandb.login()\\n🌳 Hyperparameters\\nUse of proper config system is a recommended best practice for reproducible machine learning. We can track the hyperparameters for every experiment using W&B. In this colab we will be using simple Python `dict` as our config system.\\nconfigs = dict(\\n    num_classes = 10,\\n    shuffle_buffer = 1024,\\n    batch_size = 64,\\n    image_size = 28,\\n    image_channels = 1,\\n    earlystopping_patience = 3,\\n    learning_rate = 1e-3,\\n    epochs = 10\\n)', 'source': 'tutorials/keras_models.md', 'score': 0.044265877, 'relevance': 0}, {'content': \"MMF\\nThe `WandbLogger` class in Meta AI's MMF library will enable Weights & Biases to log the training/validation metrics, system (GPU and CPU) metrics, model checkpoints and configuration parameters.\\nCurrent features\\nThe following features are currently supported by the `WandbLogger` in MMF:\\nTraining & Validation metrics\\nLearning Rate over time\\nModel Checkpoint saving to W&B Artifacts\\nGPU and CPU system metrics\\nTraining configuration parameters\\nConfig parameters\\nThe following options are available in MMF config to enable and customize the wandb logging:\\ntraining:\\n    wandb:\\n        enabled: true\\n        # An entity is a username or team name where you're sending runs.\\n        # By default it will log the run to your user account.\\n        entity: null\\n        # Project name to be used while logging the experiment with wandb\\n        project: mmf\\n        # Experiment/ run name to be used while logging the experiment\\n        # under the project with wandb. The default experiment name\\n        # is: ${training.experiment_name}\\n        name: ${training.experiment_name}\\n        # Turn on model checkpointing, saving checkpoints to W&B Artifacts\\n        log_model_checkpoint: true\\n        # Additional argument values that you want to pass to wandb.init(). \\n        # Check out the documentation at https://docs.wandb.ai/ref/python/init\\n        # to see what arguments are available, such as:\\n        # job_type: 'train'\\n        # tags: ['tag1', 'tag2']\\nenv:\\n    # To change the path to the directory where wandb metadata would be \\n    # stored (Default: env.log_dir):\\n    wandb_logdir: ${env:MMF_WANDB_LOGDIR,}\", 'source': 'guides/integrations/other/mmf.md', 'score': 0.027117657000000003, 'relevance': 0}])"
        ],
        [
         "4",
         "can we stopped to automatically add new config params when a new run is added to an experiment?",
         "In Weights & Biases, when you initialize a new run using `wandb.init()`, any configuration parameters you specify are automatically added to the run's configuration. However, if you want to prevent automatic updates or additions to the configuration after the run has started, you can control this behavior using the `allow_val_change` attribute of the `wandb.config` object.\n\nTo prevent new configuration parameters from being automatically added after the run has started, you can set `allow_val_change` to `False`. This setting will ensure that once the configuration is set during the initialization of the run, no new keys can be added, and existing keys cannot be modified. Here's how you can implement this:\n\n```python\nimport wandb\n\n# Initialize a new run\nrun = wandb.init(project=\"your_project_name\", config={\n    \"parameter1\": value1,\n    \"parameter2\": value2\n})\n\n# Set allow_val_change to False to prevent changes\nwandb.config.allow_val_change = False\n\n# Attempting to add a new parameter or change an existing one will now raise an error\n# wandb.config[\"new_parameter\"] = new_value  # This will raise an error\n```\n\nThis setup is particularly useful when you want to ensure the integrity of your experiment configurations and prevent accidental changes or additions after the run has started.",
         "WeaveList([{'content': 'Set the configuration throughout your script\\nYou can add more parameters to your config object throughout your script. The proceeding code snippet demonstrates how to add new key-value pairs to your config object:\\nimport wandb\\n# Define a config dictionary object\\nconfig = {\\n    \"hidden_layer_sizes\": [32, 64],\\n    \"kernel_sizes\": [3],\\n    \"activation\": \"ReLU\",\\n    \"pool_sizes\": [2],\\n    \"dropout\": 0.5,\\n    \"num_classes\": 10,\\n}\\n# Pass the config dictionary when you initialize W&B\\nrun = wandb.init(project=\"config_example\", config=config)\\n# Update config after you initialize W&B\\nwandb.config[\"dropout\"] = 0.2\\nwandb.config.epochs = 4\\nwandb.config[\"batch_size\"] = 32\\nYou can update multiple values at a time:\\nwandb.init(config={\"epochs\": 4, \"batch_size\": 32})\\n# later\\nwandb.config.update({\"lr\": 0.1, \"channels\": 16})\\nSet the configuration after your Run has finished\\nUse the W&B Public API to update your config (or anything else about from a complete Run) after your Run. This is particularly useful if you forgot to log a value during a Run. \\nProvide your `entity`, `project name`, and the `Run ID` to update your configuration after a Run has finished. Find these values directly from the Run object itself `wandb.run` or from the W&B App UI:\\napi = wandb.Api()\\n# Access attributes directly from the run object\\n# or from the W&B App\\nusername = wandb.run.entity\\nproject = wandb.run.project\\nrun_id = wandb.run.id\\nrun = api.run(f\"{username}/{project}/{run_id}\")\\nrun.config[\"bar\"] = 32\\nrun.update()\\nabsl.FLAGS\\nYou can also pass in absl flags.\\nflags.DEFINE_string(\"model\", None, \"model to run\")  # name, default, help\\nwandb.config.update(flags.FLAGS)  # adds absl flags to config', 'source': 'guides/track/config.md', 'score': 0.7862948, 'relevance': 2}, {'content': 'Configure Experiments\\nConfigure a Machine Learning Experiment\\nTry in a Colab Notebook here\\nUse the `wandb.config` object to save your training configuration such as: \\n- hyperparameters\\n- input settings such as the dataset name or model type\\n- any other independent variables for your experiments. \\nThe `wandb.config` attribute makes it easy to analyze your experiments and reproduce your work in the future. You can group by configuration values in the W&B App, compare the settings of different W&B Runs and view how different training configurations affect the output. A Run\\'s `config` attribute is a dictionary-like object, and it can be built from lots of dictionary-like objects.\\n:::info\\nDependent variables (like loss and accuracy) or output metrics should be saved with `wandb.log`instead.\\n:::\\nSet up an experiment configuration\\nConfigurations are typically defined in the beginning of a training script. Machine learning workflows may vary, however, so you are not required to define a configuration at the beginning of your training script.\\n:::caution\\nWe recommend that you avoid using dots in your config variable names. Instead, use a dash or underscore instead. Use the dictionary access syntax `[\"key\"][\"foo\"]` instead of the attribute access syntax `config.key.foo` if your script accesses `wandb.config` keys below the root.\\n:::\\nThe following sections outline different common scenarios of how to define your experiments configuration.', 'source': 'guides/track/config.md', 'score': 0.08464396, 'relevance': 2}, {'content': 'Access run config inputs\\nJobs launched with run config inputs can access the input values through the `Run.config`. The `Run` returned by `wandb.init` in the job code will have the input values automatically set. Use \\nfrom wandb.sdk import launch\\nrun_config_overrides = launch.load_wandb_config()\\nto load the run config input values anywhere in the job code.\\nReconfigure a file\\nThe Launch SDK also provides a way to manage input values stored in config files in the job code. This is a common pattern in many deep learning and large language model use cases, like this torchtune example or this Axolotl config). \\n:::info\\nSweeps on Launch does not support the use of config file inputs as sweep parameters. Sweep parameters must be controlled through the `Run.config` object.\\n:::\\nThe `launch.manage_config_file` function can be used to add a config file as an input to the Launch job, giving you access to edit values within the config file when launching the job.\\nBy default, no run config inputs will be captured if `launch.manage_config_file` is used. Calling `launch.manage_wandb_config` overrides this behavior.\\nConsider the following example:\\nimport yaml\\nimport wandb\\nfrom wandb.sdk import launch\\n# Required for launch sdk use.\\nwandb.require(\"core\")\\nlaunch.manage_config_file(\"config.yaml\")\\nwith open(\"config.yaml\", \"r\") as f:\\n    config = yaml.safe_load(f)\\nwith wandb.init(config=config):\\n    # Etc.\\n    pass\\nImagine the code is run with an adjacent file `config.yaml`:\\nlearning_rate: 0.01\\nbatch_size: 32\\nmodel: resnet\\ndataset: cifar10\\nThe call to `launch.manage_config_file` will add the `config.yaml` file as an input to the job, making it reconfigurable when launching from the W&B CLI or UI. \\nThe `include` and `exclude` keyword arugments may be used to filter the acceptable input keys for the config file in the same way as `launch.manage_wandb_config`.', 'source': 'guides/launch/job-inputs.md', 'score': 0.16572809, 'relevance': 1}, {'content': 'Reconfigure the Run object\\nThe `Run` object returned by `wandb.init` in a job can be reconfigured, by default. The Launch SDK provides a way to customize what parts of the `Run.config` object can be reconfigured when launching the job.\\nimport wandb\\nfrom wandb.sdk import launch\\n# Required for launch sdk use.\\nwandb.require(\"core\")\\nconfig = {\\n    \"trainer\": {\\n        \"learning_rate\": 0.01,\\n        \"batch_size\": 32,\\n        \"model\": \"resnet\",\\n        \"dataset\": \"cifar10\",\\n        \"private\": {\\n            \"key\": \"value\",\\n        },\\n    },\\n    \"seed\": 42,\\n}\\nwith wandb.init(config=config):\\n    launch.manage_wandb_config(\\n        include=[\"trainer\"], \\n        exclude=[\"trainer.private\"],\\n    )\\n    # Etc.\\nThe function `launch.manage_wandb_config` configures the job to accept input values for the `Run.config` object.  The optional `include` and `exclude` options take path prefixes within the nested config object.  This can be useful if, for example, a job uses a library whose options you don\\'t want to expose to end users.  \\nIf `include` prefixes are provided, only paths within the config that match an `include` prefix will accept input values. If `exclude` prefixes are provided, no paths that match the `exclude` list will be filtered out of the input values. If a path matches both an `include` and an `exclude` prefix, the `exclude` prefix will take precedence.\\nIn the preceding example, the path `[\"trainer.private\"]` will filter out the `private` key from the `trainer` object, and the path `[\"trainer\"]` will filter out all keys not under the `trainer` object.\\n:::tip\\nUse a ``-escaped `.` to filter out keys with a `.` in their name. \\nFor example, `r\"trainer\\\\.private\"` filters out the `trainer.private` key rather than the `private` key under the `trainer` object.\\nNote that the `r` prefix above denotes a raw string.\\n:::\\nIf the code above is packaged and run as a job, the input types of the job will be:', 'source': 'guides/launch/job-inputs.md', 'score': 0.0005152657, 'relevance': 1}, {'content': \"Run The Library With wandb As Optional\\nIf you want to make `wandb` optional when your users use your library, you can either:\\nDefine a `wandb` flag such as:\\ntrainer = my_trainer(..., use_wandb=True)\\npython train.py ... --use-wandb\\nOr, set `wandb` to be disabled in `wandb.init`\\nwandb.init(mode=“disabled”)\\nexport WANDB_MODE=disabled\\nor\\nwandb disabled\\nOr, set `wandb` to be offline - note this will still run `wandb`, it just won't try and communicate back to W&B over the internet\\nexport WANDB_MODE=offline\\nor\\nos.environ['WANDB_MODE'] = 'offline'\\nwandb offline\\nDefining A wandb Run Config\\nWith a `wandb` run config you can provide metadata about your model, dataset, and so on when you create a W&B Run. You can use this information to compare different experiments and quickly understand what are the main differences.\\nTypical config parameters you can log include:\\nModel name, version, architecture parameters etc\\nDataset name, version, number of train/val examples etc\\nTraining parameters such as learning rate, batch size, optimizer etc\\nThe following code snippet shows how to log a config:\\nconfig = {“batch_size”:32, …}\\nwandb.init(…, config=config)\\nUpdating The wandb config\\nUse `wandb.config.update` to update the config. Updating your configuration dictionary is useful when parameters are obtained after the dictionary was defined, for example you might want to add a model’s parameters after the model is instantiated.\\nwandb.config.update({“model_parameters” = 3500})\\nFor more information on how to define a config file, see Configure Experiments with wandb.config\\nLogging To W&B\", 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.49894142, 'relevance': 0}, {'content': 'launch_add\\nView source on GitHub\\nEnqueue a W&B launch experiment. With either a source uri, job or docker_image.\\nlaunch_add(\\n    uri: Optional[str] = None,\\n    job: Optional[str] = None,\\n    config: Optional[Dict[str, Any]] = None,\\n    template_variables: Optional[Dict[str, Union[float, int, str]]] = None,\\n    project: Optional[str] = None,\\n    entity: Optional[str] = None,\\n    queue_name: Optional[str] = None,\\n    resource: Optional[str] = None,\\n    entry_point: Optional[List[str]] = None,\\n    name: Optional[str] = None,\\n    version: Optional[str] = None,\\n    docker_image: Optional[str] = None,\\n    project_queue: Optional[str] = None,\\n    resource_args: Optional[Dict[str, Any]] = None,\\n    run_id: Optional[str] = None,\\n    build: Optional[bool] = (False),\\n    repository: Optional[str] = None,\\n    sweep_id: Optional[str] = None,\\n    author: Optional[str] = None,\\n    priority: Optional[int] = None\\n) -> \"public.QueuedRun\"', 'source': 'ref/python/launch-library/launch_add.md', 'score': 0.005934067, 'relevance': 0}, {'content': 'Next steps: Visualize your experiment\\nUse the W&B Dashboard as a central place to organize and visualize results from your machine learning models. With just a few clicks, construct rich, interactive charts like parallel coordinates plots, parameter importance analyzes, and more.\\nFor more information on how to view experiments and specific runs, see Visualize results from experiments.\\nBest Practices\\nThe following are some suggested guidelines to consider when you create experiments:\\nConfig: Track hyperparameters, architecture, dataset, and anything else you\\'d like to use to reproduce your model. These will show up in columns— use config columns to group, sort, and filter runs dynamically in the app.\\nProject: A project is a set of experiments you can compare together. Each project gets a dedicated dashboard page, and you can easily turn on and off different groups of runs to compare different model versions.\\nNotes: A quick commit message to yourself. The note can be set from your script. You can edit notes at a later time on the Overview section of your project\\'s dashboard on the W&B App.\\nTags: Identify baseline runs and favorite runs. You can filter runs using tags. You can edit tags at a later time on the Overview section of your project\\'s dashboard on the W&B App.\\nThe following code snippet demonstrates how to define a W&B Experiment using the best practices listed above:\\nimport wandb\\nconfig = dict(\\n    learning_rate=0.01, momentum=0.2, architecture=\"CNN\", dataset_id=\"cats-0192\"\\n)\\nwandb.init(\\n    project=\"detect-cats\",\\n    notes=\"tweak baseline\",\\n    tags=[\"baseline\", \"paper1\"],\\n    config=config,\\n)\\nFor more more information about available parameters when defining a W&B Experiment, see the wandb.init API docs in the API Reference Guide.', 'source': 'guides/track/launch.md', 'score': 0.004556114, 'relevance': 0}, {'content': \"parameters_dict.update({\\n    'learning_rate': {\\n        # a flat distribution between 0 and 0.1\\n        'distribution': 'uniform',\\n        'min': 0,\\n        'max': 0.1\\n      },\\n    'batch_size': {\\n        # integers between 32 and 256\\n        # with evenly-distributed logarithms \\n        'distribution': 'q_log_uniform_values',\\n        'q': 8,\\n        'min': 32,\\n        'max': 256,\\n      }\\n    })\\nWhen we're finished, `sweep_config` is a nested dictionary\\nthat specifies exactly which `parameters` we're interested in trying\\nand what `method` we're going to use to try them.\\nimport pprint\\npprint.pprint(sweep_config)\\nBut that's not all of the configuration options!\\nFor example, we also offer the option to `early_terminate` your runs with the HyperBand scheduling algorithm. See more here.\\nYou can find a list of all configuration options here\\nand a big collection of examples in YAML format here.\", 'source': 'tutorials/sweeps.md', 'score': 0.0041019293, 'relevance': 0}, {'content': 'Enable runs to automatically resume\\nThe following code snippet shows how to enable runs to automatically resume with the Python SDK or with environment variables. \\nThe following code snippet shows how to specify a W&B run ID with the Python SDK. \\nReplace values enclosed within `<>` with your own:\\nrun = wandb.init(entity=\"<entity>\", \\\\ \\n        project=\"<project>\", id=\"<run ID>\", resume=\"<resume>\")\\nThe following example shows how to specify the W&B `WANDB_RUN_ID` variable in a bash script: \\nrun_experiment.sh\\nRUN_ID=\"$1\"\\nWANDB_RESUME=allow WANDB_RUN_ID=\"$RUN_ID\" python eval.py\\nWithin your terminal, you could run the shell script along with the W&B run ID. The following code snippet passes the run ID `akj172`: \\nsh run_experiment.sh akj172 \\n:::important\\nAutomatic resuming only works if the process is restarted on top of the same filesystem as the failed process. \\n:::\\nFor example, suppose you execute a python script called `train.py` in a directory called called `Users/AwesomeEmployee/Desktop/ImageClassify/training/`. Within `train.py`, the script creates a run that enables automatic resuming. Suppose next that the training script is stopped.  To resume this run, you would need to restart your `train.py` script within `Users/AwesomeEmployee/Desktop/ImageClassify/training/` .\\n:::tip\\nIf you can not share a filesystem, specify the `WANDB_RUN_ID` environment variable or pass the run ID with the W&B Python SDK. See the Create a run section in the \"What are runs?\" page for more information on run IDs.\\n:::', 'source': 'guides/runs/resuming.md', 'score': 0.003991728, 'relevance': 0}, {'content': 'More Resources\\nAutoTrain Advanced now supports Experiment Tracking by Rishiraj Acharya.\\n🤗 Autotrain Docs', 'source': 'guides/integrations/autotrain.md', 'score': 3.42685e-05, 'relevance': 0}])"
        ],
        [
         "5",
         "I feel wandb cannot connect the server, how can I test it on the terminal",
         "To test the connection to the `wandb` server from the terminal, you can follow these steps:\n\n### Step 1: Check Server Health\nYou can use a simple `curl` command to check if the server is responding. Replace `your_server_url` with the actual URL of your W&B server.\n\n```bash\ncurl your_server_url\n```\n\nIf the server is up and running, you should receive a response from this command. If there's no response, it indicates that the server might not be reachable.\n\n### Step 2: Verify Local Configuration\nEnsure that your local `wandb` configuration is set up correctly. You can check the current settings by running:\n\n```bash\nwandb settings\n```\n\nLook for the `base_url` setting to confirm that it's pointing to the correct server URL.\n\n### Step 3: Use `wandb verify` Command\nYou can use the `wandb verify` command to test the connectivity and configuration of your local instance. This command performs several checks, including testing the connection to the specified W&B server.\n\n```bash\nwandb verify\n```\n\nThis command will output logs that can help you identify any connectivity or configuration issues.\n\n### Step 4: Check Network Issues\nIf there are still issues, check your network settings. Firewalls or network policies might be blocking the connection to the server. You can use `ping` or `traceroute` to diagnose network connectivity issues.\n\n```bash\nping your_server_url\ntraceroute your_server_url\n```\n\nThese commands help you determine if there are any network hops that are failing or if the server is unreachable.\n\n### Step 5: Review Logs\nIf the above steps do not resolve the issue, check the logs for any error messages that might give more insight into what is going wrong. You can find the logs in the directory where you are running your `wandb` commands, typically under `wandb/debug.log`.\n\nBy following these steps, you should be able to diagnose and potentially resolve connectivity issues with the `wandb` server using terminal commands. If problems persist, consider reaching out to W&B support for further assistance.",
         "WeaveList([{'content': 'Verify your installation\\nVery your W&B Server is configured properly. Run the following commands in your terminal:\\npip install wandb\\nwandb login --host=https://YOUR_DNS_DOMAIN\\nwandb verify\\nCheck log files to view any errors the W&B Server hits at startup. Run the following commands based on whether if you use Docker or Kubernetes: \\ndocker logs wandb-local\\nkubectl get pods\\nkubectl logs wandb-XXXXX-XXXXX\\nContact W&B Support if you encounter errors.', 'source': 'guides/hosting/self-managed/bare-metal.md', 'score': 0.99857306, 'relevance': 2}, {'content': 'wandb verify\\nUsage\\n`wandb verify [OPTIONS]`\\nSummary\\nVerify your local instance\\nOptions\\nOption\\nDescription\\n--host\\nTest a specific instance of W&B', 'source': 'ref/cli/wandb-verify.md', 'score': 0.9720456, 'relevance': 2}, {'content': 'Troubleshooting\\nIf wandb crashes, will it possibly crash my training run?\\nIt is extremely important to us that we never interfere with your training runs. We run wandb in a separate process to make sure that if wandb somehow crashes, your training will continue to run. If the internet goes out, wandb will continue to retry sending data to wandb.ai.\\nWhy is a run marked crashed in W&B when it’s training fine locally?\\nThis is likely a connection problem — if your server loses internet access and data stops syncing to W&B, we mark the run as crashed after a short period of retrying.\\nDoes logging block my training?\\n\"Is the logging function lazy? I don\\'t want to be dependent on the network to send the results to your servers and then carry on with my local operations.\"\\nCalling `wandb.log` writes a line to a local file; it does not block any network calls. When you call `wandb.init` we launch a new process on the same machine that listens for filesystem changes and talks to our web service asynchronously from your training process.\\nHow do I stop wandb from writing to my terminal or my jupyter notebook output?\\nSet the environment variable WANDB_SILENT to `true`.\\nos.environ[\"WANDB_SILENT\"] = \"true\"\\n%env WANDB_SILENT=true\\nWANDB_SILENT=true\\nHow do I kill a job with wandb?\\nPress `Ctrl+D` on your keyboard to stop a script that is instrumented with wandb.\\nHow do I deal with network issues?\\nIf you\\'re seeing SSL or network errors:`wandb: Network error (ConnectionError), entering retry loop`. You can try a couple of different approaches to solving this issue:\\nUpgrade your SSL certificate. If you\\'re running the script on an Ubuntu server, run `update-ca-certificates` We can\\'t sync training logs without a valid SSL certificate because it\\'s a security vulnerability.\\nIf your network is flaky, run training in offline mode and sync the files to us from a machine that has Internet access.\\nTry running W&B Private Hosting, which operates on your machine and doesn\\'t sync files to our cloud servers.\\n`SSL CERTIFICATE_VERIFY_FAILED`: this error could be due to your company\\'s firewall. You can set up local CAs and then use:\\n`export REQUESTS_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt`', 'source': 'guides/technical-faq/troubleshooting.md', 'score': 0.42572459999999995, 'relevance': 1}, {'content': 'Getting started\\nFollow this \"Hello, world!\" example to learn the general workflow to install W&B Server for Dedicated Cloud and Self Managed hosting options. By the end of this demo, you will know how to host W&B Server on your local machine using a Trial Mode W&B license. \\nFor demonstration purposes, this demo uses a local development server on port `8080` (`localhost:8080`).\\n:::tip\\nTrial Mode vs. Production Setup\\nIn Trial Mode, you run the Docker container on a single machine. This setup is ideal for testing the product, but it is not scalable.\\nFor production work, set up a scalable file system to avoid data loss. W&B strongly recommends that you:\\n* Allocate extra space in advance, \\n* Resize the file system proactively as you log more data\\n* Configure external metadata and object stores for backup.\\n:::\\nPrerequisites\\nBefore you get started, ensure your local machine satisfies the following requirements: \\nInstall Python\\nInstall Docker and ensure it is running\\nInstall or upgrade the latest version of W&B:\\n   \\npip install --upgrade wandb\\n1. Pull the W&B Docker image\\nRun the following in your terminal:\\nwandb server start\\nThis command pulls the latest W&B Docker image wandb/local.\\n2. Create a W&B account\\nNavigate to `http://localhost:8080/signup` and create an initial user account. Provide a name, email address, a username, and a password: \\nClick the Sign Up button to create a W&B account. \\n:::note\\nFor this demo, create a new W&B account even if you already have a W&B account. \\n:::\\nCopy your API key\\nAfter you create an account, navigate to `http://localhost:8080/authorize`.  \\nCopy the W&B API key that appears on the screen. At a later step, you will need this key at a later step to verify your login credentials.', 'source': 'guides/hosting/self-managed/basic-setup.md', 'score': 0.9837849999999999, 'relevance': 1}, {'content': 'Command Line Interface\\nUsage\\n`wandb [OPTIONS] COMMAND [ARGS]...`\\nOptions\\nOption\\nDescription\\n--version\\nShow the version and exit.\\nCommands\\nCommand\\nDescription\\nagent\\nRun the W&B agent\\nartifact\\nCommands for interacting with artifacts\\nbeta\\nBeta versions of wandb CLI commands.\\ncontroller\\nRun the W&B local sweep controller\\ndisabled\\nDisable W&B.\\ndocker\\nRun your code in a docker container.\\ndocker-run\\nWrap `docker run` and adds WANDB_API_KEY and WANDB_DOCKER...\\nenabled\\nEnable W&B.\\ninit\\nConfigure a directory with Weights & Biases\\njob\\nCommands for managing and viewing W&B jobs\\nlaunch\\nLaunch or queue a W&B Job.\\nlaunch-agent\\nRun a W&B launch agent.\\nlaunch-sweep\\nRun a W&B launch sweep (Experimental).\\nlogin\\nLogin to Weights & Biases\\noffline\\nDisable W&B sync\\nonline\\nEnable W&B sync\\npull\\nPull files from Weights & Biases\\nrestore\\nRestore code, config and docker state for a run\\nscheduler\\nRun a W&B launch sweep scheduler (Experimental)\\nserver\\nCommands for operating a local W&B server\\nstatus\\nShow configuration settings\\nsweep\\nInitialize a hyperparameter sweep.\\nsync\\nUpload an offline training directory to W&B\\nverify\\nVerify your local instance', 'source': 'ref/cli/README.md', 'score': 0.88798404, 'relevance': 1}, {'content': 'Launch a job\\nLet\\'s send a job to our agent. You can launch a simple \"hello world\" from a terminal logged into your W&B account with:\\nwandb launch -d wandb/job_hello_world:main -p <target-wandb-project> -q <your-queue-name> -e <your-queue-entity>\\nYou can test with any job or image you like, but make sure your cluster can pull your image.  See Minikube’s documentation for additional guidance. You can also test using one of our public jobs.', 'source': 'tutorials/minikube_gpu.md', 'score': 0.9906961, 'relevance': 0}, {'content': 'User Login\\nThere are a few ways for your users to log in to W&B:\\nLog into W&B with a bash command in a terminal\\nwandb login $MY_WANDB_KEY\\nIf they\\'re in a Jupyter or Colab notebook, log into W&B like so\\nimport wandb\\nwandb.login\\nSet a W&B environment variable for the API key\\nexport WANDB_API_KEY=$YOUR_API_KEY\\nor\\nos.environ[\\'WANDB_API_KEY\\'] = \"abc123...\"\\nIf a user is using wandb for the first time without following any of the steps mentioned above, they will automatically be prompted to login when your script calls `wandb.init`\\nStarting A wandb Run\\nA W&B Run is a unit of computation logged by W&B. Typically you associate a single W&B Run per training experiment.\\nInitialize W&B and start a Run within your code with:\\nwandb.init()\\nOptionally you can provide a name for their project, or let the user set it themselves with parameter such as `wandb_project` in your code along with the username or team name, such as `wandb_entity` , for the entity parameter:\\nwandb.init(project=wandb_project, entity=wandb_entity)\\nWhere To Place wandb.init?\\nYour library should create W&B Run as early as possible because any output in your console, including error messages, are logged as part of the W&B Run. This makes debugging easier.', 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.6127153, 'relevance': 0}, {'content': 'wandb server\\nUsage\\n`wandb server [OPTIONS] COMMAND [ARGS]...`\\nSummary\\nCommands for operating a local W&B server\\nOptions\\nOption\\nDescription\\nCommands\\nCommand\\nDescription\\nstart\\nStart a local W&B server\\nstop\\nStop a local W&B server', 'source': 'ref/cli/wandb-server/README.md', 'score': 0.5115259999999999, 'relevance': 0}, {'content': \"wandb server start\\nUsage\\n`wandb server start [OPTIONS]`\\nSummary\\nStart a local W&B server\\nOptions\\nOption\\nDescription\\n-p, --port\\nThe host port to bind W&B server on\\n-e, --env\\nEnv vars to pass to wandb/local\\n--daemon / --no-daemon\\nRun or don't run in daemon mode\", 'source': 'ref/cli/wandb-server/wandb-server-start.md', 'score': 0.24310295, 'relevance': 0}, {'content': \"Run The Library With wandb As Optional\\nIf you want to make `wandb` optional when your users use your library, you can either:\\nDefine a `wandb` flag such as:\\ntrainer = my_trainer(..., use_wandb=True)\\npython train.py ... --use-wandb\\nOr, set `wandb` to be disabled in `wandb.init`\\nwandb.init(mode=“disabled”)\\nexport WANDB_MODE=disabled\\nor\\nwandb disabled\\nOr, set `wandb` to be offline - note this will still run `wandb`, it just won't try and communicate back to W&B over the internet\\nexport WANDB_MODE=offline\\nor\\nos.environ['WANDB_MODE'] = 'offline'\\nwandb offline\\nDefining A wandb Run Config\\nWith a `wandb` run config you can provide metadata about your model, dataset, and so on when you create a W&B Run. You can use this information to compare different experiments and quickly understand what are the main differences.\\nTypical config parameters you can log include:\\nModel name, version, architecture parameters etc\\nDataset name, version, number of train/val examples etc\\nTraining parameters such as learning rate, batch size, optimizer etc\\nThe following code snippet shows how to log a config:\\nconfig = {“batch_size”:32, …}\\nwandb.init(…, config=config)\\nUpdating The wandb config\\nUse `wandb.config.update` to update the config. Updating your configuration dictionary is useful when parameters are obtained after the dictionary was defined, for example you might want to add a model’s parameters after the model is instantiated.\\nwandb.config.update({“model_parameters” = 3500})\\nFor more information on how to define a config file, see Configure Experiments with wandb.config\\nLogging To W&B\", 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.06766981, 'relevance': 0}])"
        ],
        [
         "6",
         "give me a script that goes through every project of yours and then deletes all images and media files from it using the api",
         "To create a script that goes through every project in your Weights & Biases account and deletes all images and media files, you'll need to use the W&B API. Below is a Python script that demonstrates how to authenticate with the API, list all projects, and delete media files from each project.\n\n### Step-by-Step Script\n\n1. **Authenticate with the W&B API**: You need to authenticate using your API key, which you can find in your W&B account settings.\n\n2. **List all projects**: Use the API to fetch all projects under your entity.\n\n3. **Identify and delete media files**: For each project, list all artifacts, identify those that are images or media files, and delete them.\n\nHere's the Python script:\n\n```python\nimport wandb\n\n# Authenticate with the API\napi = wandb.Api()\n\n# Your entity name\nentity = \"your_entity_name\"\n\n# List all projects in your entity\nprojects = api.projects(entity=entity)\n\n# Loop through each project\nfor project in projects:\n    print(f\"Processing project: {project.name}\")\n    # List all artifacts in the project\n    artifacts = api.artifacts(f\"{entity}/{project.name}\", per_page=50)\n    for artifact in artifacts:\n        # Check if the artifact is an image or media file\n        if artifact.type in ['image', 'media']:\n            print(f\"Deleting artifact: {artifact.name}\")\n            # Delete the artifact\n            artifact.delete()\n\nprint(\"Completed deleting images and media files from all projects.\")\n```\n\n### Explanation\n\n- **Authentication**: The script starts by authenticating with the W&B API using your API key.\n- **Listing Projects**: It retrieves all projects under your specified entity.\n- **Deleting Media Files**: For each project, it fetches all artifacts and checks if they are images or media files based on their type. If they are, it deletes them.\n\n### Important Notes\n\n- **Permissions**: Ensure that your API key has the necessary permissions to delete artifacts.\n- **Backup**: Consider backing up your data before running this script, as deletions are irreversible.\n\nThis script provides a basic framework. You might need to adjust the artifact type checking depending on how you categorize or store images and media files in your projects.",
         "WeaveList([{'content': 'Delete multiple artifact versions with a specific alias\\nThe proceeding code demonstrates how to delete multiple artifact versions that have a specific alias. Provide the entity, project name, and run ID that created the artifacts. Replace the deletion logic with your own:\\nimport wandb\\nruns = api.run(\"entity/project_name/run_id\")\\n# Delete artifact ith alias \\'v3\\' and \\'v4\\nfor artifact_version in runs.logged_artifacts():\\n    # Replace with your own deletion logic.\\n    if artifact_version.name[-2:] == \"v3\" or artifact_version.name[-2:] == \"v4\":\\n        artifact.delete(delete_aliases=True)\\nDelete all versions of an artifact that do not have an alias\\nThe following code snippet demonstrates how to delete all versions of an artifact that do not have an alias. Provide the name of the project and entity for the `project` and `entity` keys in `wandb.Api`, respectively. Replace the `<>` with the name of your artifact:\\nimport wandb\\n# Provide your entity and a project name when you\\n# use wandb.Api methods.\\napi = wandb.Api(overrides={\"project\": \"project\", \"entity\": \"entity\"})\\nartifact_type, artifact_name = \"<>\"  # provide type and name\\nfor v in api.artifact_versions(artifact_type, artifact_name):\\n    # Clean up versions that don\\'t have an alias such as \\'latest\\'.\\n    # NOTE: You can put whatever deletion logic you want here.\\n    if len(v.aliases) == 0:\\n        v.delete()\\nDelete an artifact collection\\nTo delete an artifact collection:\\nNavigate to the artifact collection you want to delete and hover over it.\\nSelect the kebab dropdown next to the artifact collection name.\\nChoose Delete.\\nYou can also delete artifact collection programmatically with the delete() method. Provide the name of the project and entity for the `project` and `entity` keys in `wandb.Api`, respectively:\\nimport wandb\\n# Provide your entity and a project name when you\\n# use wandb.Api methods.\\napi = wandb.Api(overrides={\"project\": \"project\", \"entity\": \"entity\"})\\ncollection = api.artifact_collection(\\n    \"<artifact_type>\", \"entity/project/artifact_collection_name\"\\n)\\ncollection.delete()', 'source': 'guides/artifacts/delete-artifacts.md', 'score': 0.17398883, 'relevance': 2}, {'content': 'Delete artifacts\\nDelete W&B Artifacts\\nDelete artifacts interactively with the App UI or programmatically with the W&B SDK. When you delete an artifact, W&B marks that artifact as a soft-delete. In other words, the artifact is marked for deletion but files are not immediately deleted from storage. \\nThe contents of the artifact remain as a soft-delete, or pending deletion state, until a regularly run garbage collection process reviews all artifacts marked for deletion. The garbage collection process deletes associated files from storage if the artifact and its associated files are not used by a previous or subsequent artifact versions. \\nThe sections in this page describe how to delete specific artifact versions, how to delete an artifact collection, how to delete artifacts with and without aliases, and more. You can schedule when artifacts are deleted from W&B with TTL policies. For more information, see Manage data retention with Artifact TTL policy.\\n:::note\\nArtifacts that are scheduled for deletion with a TTL policy, deleted with the W&B SDK, or deleted with the W&B App UI are first soft-deleted. Artifacts that are soft deleted undergo garbage collection before they are hard-deleted.\\n:::\\nDelete an artifact version\\nTo delete an artifact version:\\nSelect the name of the artifact. This will expand the artifact view and list all the artifact versions associated with that artifact.\\nFrom the list of artifacts, select the artifact version you want to delete.\\nOn the right hand side of the workspace, select the kebab dropdown.\\nChoose Delete.\\nAn artifact version can also be deleted programatically via the delete() method. See the examples below. \\nDelete multiple artifact versions with aliases\\nThe following code example demonstrates how to delete artifacts that have aliases associated with them. Provide the entity, project name, and run ID that created the artifacts.\\nimport wandb\\nrun = api.run(\"entity/project/run_id\")\\nfor artifact in run.logged_artifacts():\\n    artifact.delete()\\nSet the `delete_aliases` parameter to the boolean value, `True` to delete aliases if the artifact has one or more aliases.\\nimport wandb\\nrun = api.run(\"entity/project/run_id\")\\nfor artifact in run.logged_artifacts():\\n    # Set delete_aliases=True in order to delete\\n    # artifacts with one more aliases\\n    artifact.delete(delete_aliases=True)', 'source': 'guides/artifacts/delete-artifacts.md', 'score': 0.0068239546, 'relevance': 2}, {'content': 'flush\\nView source\\nflush()\\nFlush the local cache.\\nThe api object keeps a local cache of runs, so if the state of the run may\\nchange while executing your script you must clear the local cache with\\n`api.flush()` to get the latest values associated with the run.\\nfrom_path\\nView source\\nfrom_path(\\n    path\\n)\\nReturn a run, sweep, project or report from a path.\\nExamples:\\nproject = api.from_path(\"my_project\")\\nteam_project = api.from_path(\"my_team/my_project\")\\nrun = api.from_path(\"my_team/my_project/runs/id\")\\nsweep = api.from_path(\"my_team/my_project/sweeps/id\")\\nreport = api.from_path(\"my_team/my_project/reports/My-Report-Vm11dsdf\")\\nArguments\\n`path`\\n(str) The path to the project, run, sweep or report\\nReturns\\nA `Project`, `Run`, `Sweep`, or `BetaReport` instance.\\nRaises\\nwandb.Error if path is invalid or the object doesn\\'t exist\\njob\\nView source\\njob(\\n    name: Optional[str],\\n    path: Optional[str] = None\\n) -> \"public.Job\"\\nReturn a `Job` from the given parameters.\\nArguments\\n`name`\\n(str) The job name.\\n`path`\\n(str, optional) If given, the root path in which to download the job artifact.\\nReturns\\nA `Job` object.\\nlist_jobs\\nView source\\nlist_jobs(\\n    entity: str,\\n    project: str\\n) -> List[Dict[str, Any]]\\nReturn a list of jobs, if any, for the given entity and project.\\nArguments\\n`entity`\\n(str) The entity for the listed job(s).\\n`project`\\n(str) The project for the listed job(s).\\nReturns\\nA list of matching jobs.\\nproject\\nView source\\nproject(\\n    name: str,\\n    entity: Optional[str] = None\\n) -> \"public.Project\"\\nReturn the `Project` with the given name (and entity, if given).\\nArguments\\n`name`\\n(str) The project name.\\n`entity`\\n(str) Name of the entity requested. If None, will fall back to the default entity passed to `Api`. If no default entity, will raise a `ValueError`.\\nReturns\\nA `Project` object.', 'source': 'ref/python/public-api/api.md', 'score': 0.015967654, 'relevance': 1}, {'content': \"Storage\\nIf you are approaching or exceeding your storage limit, there are multiple paths forward to manage your data. The path that's best for you will depend on your account type and your current project setup.\\nManage storage consumption\\nW&B offers different methods of optimizing your storage consumption:\\nUse\\xa0reference artifacts\\xa0to track files saved outside the W&B system, instead of uploading them to W&B storage.\\nUse an external cloud storage bucket for storage. (Enterprise only)\\nDelete data\\nYou can also choose to delete data to remain under your storage limit. There are several ways to do this:\\nDelete data interactively with the app UI.\\nSet a TTL policy on Artifacts so they are automatically deleted.\", 'source': 'guides/app/features/storage.md', 'score': 0.0009966077, 'relevance': 1}, {'content': 'Data Privacy and Compliance\\nArtifact Data Privacy and Compliance\\nFiles are uploaded to Google Cloud bucket managed by W&B when you log artifacts. The contents of the bucket are encrypted both at rest and in transit. Artifact files are only visible to users who have access to the corresponding project.\\nWhen you delete a version of an artifact, it is marked for soft deletion in our database and removed from your storage cost. When you delete an entire artifact, it is queued for permanently deletion and all of its contents are removed from the W&B bucket. If you have specific needs around file deletion please reach out to Customer Support.\\nFor sensitive datasets that cannot reside in a multi-tenant environment, you can use either a private W&B server connected to your cloud bucket or reference artifacts. Reference artifacts track references to private buckets without sending file contents to W&B. Reference artifacts maintain links to files on your buckets or servers. In other words, W&B only keeps track of the metadata associated with the files and not the files themselves.\\nCreate a reference artifact similar to how you create a non reference artifact:\\nimport wandb\\nrun = wandb.init()\\nartifact = wandb.Artifact(\"animals\", type=\"dataset\")\\nartifact.add_reference(\"s3://my-bucket/animals\")\\nFor alternatives, contact us at contact@wandb.com to talk about private cloud and on-premises installations.', 'source': 'guides/artifacts/data-privacy-and-compliance.md', 'score': 0.0016166499, 'relevance': 0}, {'content': 'Export metrics from all runs in a project to a CSV file\\nThis script pulls down the runs in a project and produces a dataframe and a CSV of runs including their names, configs, and summary stats. Replace `<entity>` and `<project>` with your W&B entity and the name of your project, respectively.\\nimport pandas as pd\\nimport wandb\\napi = wandb.Api()\\nentity, project = \"<entity>\", \"<project>\"\\nruns = api.runs(entity + \"/\" + project)\\nsummary_list, config_list, name_list = [], [], []\\nfor run in runs:\\n    # .summary contains the output keys/values\\n    #  for metrics such as accuracy.\\n    #  We call ._json_dict to omit large files\\n    summary_list.append(run.summary._json_dict)\\n    # .config contains the hyperparameters.\\n    #  We remove special values that start with _.\\n    config_list.append({k: v for k, v in run.config.items() if not k.startswith(\"_\")})\\n    # .name is the human-readable name of the run.\\n    name_list.append(run.name)\\nruns_df = pd.DataFrame(\\n    {\"summary\": summary_list, \"config\": config_list, \"name\": name_list}\\n)\\nruns_df.to_csv(\"project.csv\")\\nGet the starting time for a run\\nThis code snippet retrieves the time at which the run was created.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"entity/project/run_id\")\\nstart_time = run.created_at\\nUpload files to a finished run\\nThe code snippet below uploads a selected file to a finished run.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"entity/project/run_id\")\\nrun.upload_file(\"file_name.extension\")\\nDownload a file from a run\\nThis finds the file \"model-best.h5\" associated with with run ID uxte44z7 in the cifar project and saves it locally.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nrun.file(\"model-best.h5\").download()\\nDownload all files from a run\\nThis finds all files associated with a run and saves them locally.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nfor file in run.files():\\n    file.download()', 'source': 'guides/track/public-api-guide.md', 'score': 0.0004339325, 'relevance': 0}, {'content': 'wandb artifact cache cleanup\\nUsage\\n`wandb artifact cache cleanup [OPTIONS] TARGET_SIZE`\\nSummary\\nClean up less frequently used files from the artifacts cache\\nOptions\\nOption\\nDescription\\n--remove-temp / --no-remove-temp\\nRemove temp files', 'source': 'ref/cli/wandb-artifact/wandb-artifact-cache/wandb-artifact-cache-cleanup.md', 'score': 0.0003874762, 'relevance': 0}, {'content': '%%capture\\n!curl -SL $src_url > $src_zip\\n!unzip $src_zip\\nimport wandb\\nimport pandas as pd\\nimport os\\nwith wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=\"log_datasets\") as run:\\n    img_paths = []\\n    for root, dirs, files in os.walk(\"nature_100\", topdown=False):\\n        for name in files:\\n            img_path = os.path.join(root, name)\\n            label = img_path.split(\"/\")[1]\\n            img_paths.append([img_path, label])\\n    index_df = pd.DataFrame(columns=[\"image_path\", \"label\"], data=img_paths)\\n    index_df.to_csv(\"index.csv\", index=False)\\n    train_art = wandb.Artifact(\\n        name=\"Nature_100\",\\n        type=\"raw_images\",\\n        description=\"nature image dataset with 10 classes, 10 images per class\",\\n    )\\n    train_art.add_dir(\"nature_100\")\\n    # Also adding a csv indicating the labels of each image\\n    train_art.add_file(\"index.csv\")\\n    wandb.log_artifact(train_art)', 'source': 'tutorials/models.md', 'score': 0.0001647466, 'relevance': 0}, {'content': 'Known issues and limitations\\nIf the destination namespace does not exist, W&B creates one automatically.\\nIf a run or artifact has the same ID in the destination namespace, W&B treats it as an incremental import. The destination run/artifact is validated and retried if it failed in a previous import.\\nNo data is ever deleted from the source system.\\nSometimes when bulk importing (especially large artifacts), you can run into S3 rate limits. If you see `botocore.exceptions.ClientError: An error occurred (SlowDown) when calling the PutObject operation`, you can try spacing out imports by moving just a few namespaces at a time.\\nImported run tables appear to be blank in the workspace, but if you nav to the Artifacts tab and click the equivalent run table artifact you should see the table as expected.\\nSystem metrics and custom charts (not explicitly logged with `wandb.log`) are not imported\\nExport Data\\nUse the Public API to export or update data that you have saved to W&B. Before using this API, you\\'ll want to log data from your script — check the Quickstart for more details.\\nUse Cases for the Public API\\nExport Data: Pull down a dataframe for custom analysis in a Jupyter Notebook. Once you have explored the data, you can sync your findings by creating a new analysis run and logging results, for example: `wandb.init(job_type=\"analysis\")`\\nUpdate Existing Runs: You can update the data logged in association with a W&B run. For example, you might want to update the config of a set of runs to include additional information, like the architecture or a hyperparameter that wasn\\'t originally logged.\\nSee the Generated Reference Docs for details on available functions.\\nAuthentication\\nAuthenticate your machine with your API key in one of two ways:\\nRun `wandb login` on the command line and paste in your API key.\\nSet the `WANDB_API_KEY` environment variable to your API key.\\nFind the run path\\nTo use the Public API, you\\'ll often need the run path which is `<entity>/<project>/<run_id>`. In the app UI, open a run page and click the Overview tab to get the run path.', 'source': 'guides/track/public-api-guide.md', 'score': 0.0001263204, 'relevance': 0}, {'content': 'File count\\nKeep the total number of files uploaded for a single run under 1,000. You can use W&B Artifacts when you need to log a large number of files. Exceeding 1,000 files in a single run can slow down your run pages.\\nPython script performance\\nThere are a few ways that your performance of your python script is reduced:\\nThe size of your data is too large. Large data sizes could introduce a >1 ms overhead to the training loop.\\nThe speed of your network and the how the W&B backend is configured\\nCalling `wandb.log` more than a few times per second. This is due to a small latency added to the training loop every time `wandb.log` is called.\\n:::info\\nIs frequent logging slowing your training runs down? Check out this Colab for methods to get better performance by changing your logging strategy.\\n:::\\nW&B does not assert any limits beyond rate limiting. The W&B Python SDK automatically completes an exponential \"backoff\" and \"retry\" requests that exceed limits. W&B Python SDK responds with a “Network failure” on the command line. For unpaid accounts, W&B may reach out in extreme cases where usage exceeds reasonable thresholds.\\nRate limits\\nW&B SaaS Cloud API implements a rate limit to maintain system integrity and ensure availability. This measure prevents any single user from monopolizing available resources in the shared infrastructure, ensuring that the service remains accessible to all users. You may encounter a lower rate limit for a variety of reasons. \\n:::note\\nRate limits are subject to change.\\n:::\\nThe `wandb.log` calls in your script utilize a metrics logging API to log your training data to W&B. This API is engaged through either online or offline syncing. In either case, it imposes a rate limit quota limit in a rolling time window. This includes limits on total request size and request rate, where latter refers to the number of requests in a time duration. \\nRate limits are applied to each W&B project. So if you have 3 projects in a team, each project has its own rate limit quota. Users on Teams and Enterprise plans have higher rate limits than those on the Free plan.', 'source': 'guides/track/limits.md', 'score': 3.5913e-05, 'relevance': 0}])"
        ],
        [
         "7",
         "I'm trying to get data from a run.history() and I get only 500 points (the real run is 3.2k points). It happens only with some runs but not all of them. Which can be the reason?",
         "When you use `run.history()` in Weights & Biases, by default, it samples the data to 500 points. This is a common reason why you might see only 500 points even if the actual run has more data points like 3.2k in your case.\n\nTo retrieve more than the default 500 data points, you can adjust the `samples` parameter in the `run.history()` method. Here's how you can modify your code to fetch more or all data points:\n\n```python\n# Assuming 'run' is your run object\n# To fetch more than 500 points, specify the 'samples' parameter\nhistory = run.history(samples=1000)  # Adjust the number as needed\n\n# To fetch all data points, you might consider using run.scan_history()\n# This method does not sample the data\nfull_history = [row for row in run.scan_history()]\n```\n\nThe `run.scan_history()` method is particularly useful if you need to access all data points without sampling, as it iterates over every data point logged in the run.\n\nKeep in mind that fetching a large number of data points can be slow, especially if the data size is large. If you experience performance issues or timeouts, consider fetching data in smaller chunks or increasing the timeout settings if possible.\n\n**Note**: The sampling behavior of `run.history()` is designed to provide a quick overview of the run data without overwhelming the system or the network with large amounts of data. If you consistently need more detailed data, using `run.scan_history()` as shown above is recommended.",
         "WeaveList([{'content': 'run.history()\\n   _step  val   loss  _runtime  _timestamp\\n0      0  500  0.244         4  1644345412\\n1      1   45  0.521         4  1644345412\\n2      2  240  0.785         4  1644345412\\n3      3   31  0.305         4  1644345412\\n4      4  525  0.041         4  1644345412\\nrun.summary\\n{\\n    \"_runtime\": 4,\\n    \"_step\": 4,\\n    \"_timestamp\": 1644345412,\\n    \"_wandb\": {\"runtime\": 3},\\n    \"loss\": 0.041,\\n    \"val\": 525,\\n}\\nSampling\\nThe default history method samples the metrics to a fixed number of samples (the default is 500, you can change this with the `samples` __ argument). If you want to export all of the data on a large run, you can use the `run.scan_history()` method. For more details see the API Reference.', 'source': 'guides/track/public-api-guide.md', 'score': 0.9977128, 'relevance': 2}, {'content': 'Rename a metric in a run, after the run has finished\\nThis example renames a summary column in your tables.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nrun.summary[\"new_name\"] = run.summary[\"old_name\"]\\ndel run.summary[\"old_name\"]\\nrun.summary.update()\\n:::caution\\nRenaming a column only applies to tables. Charts will still refer to metrics by their original names.\\n:::\\nUpdate config for an existing run\\nThis examples updates one of your configuration settings.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nrun.config[\"key\"] = updated_value\\nrun.update()\\nExport system resource consumptions to a CSV file\\nThe snippet below would find the system resource consumptions and then, save them to a CSV.\\nimport wandb\\nrun = wandb.Api().run(\"<entity>/<project>/<run_id>\")\\nsystem_metrics = run.history(stream=\"events\")\\nsystem_metrics.to_csv(\"sys_metrics.csv\")\\nGet unsampled metric data\\nWhen you pull data from history, by default it\\'s sampled to 500 points. Get all the logged data points using `run.scan_history()`. Here\\'s an example downloading all the `loss` data points logged in history.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nhistory = run.scan_history()\\nlosses = [row[\"loss\"] for row in history]\\nGet paginated data from history\\nIf metrics are being fetched slowly on our backend or API requests are timing out, you can try lowering the page size in `scan_history` so that individual requests don\\'t time out. The default page size is 500, so you can experiment with different sizes to see what works best:\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nrun.scan_history(keys=sorted(cols), page_size=100)', 'source': 'guides/track/public-api-guide.md', 'score': 0.9965029999999999, 'relevance': 2}, {'content': 'Why is nothing showing up in my graphs?\\nIf you\\'re seeing \"No visualization data logged yet\" that means that we haven\\'t gotten the first `wandb.log` call from your script yet. This could be because your run takes a long time to finish a step. If you\\'re logging at the end of each epoch, you could log a few times per epoch to see data stream in more quickly.\\nWhy is the same metric appearing more than once?\\nIf you\\'re logging different types of data under the same key, we have to split them out in our database. This means you\\'ll see multiple entries of the same metric name in a dropdown in the UI. The types we group by are `number`, `string`, `bool`, `other` (mostly arrays), and any `wandb` data type (`Histogram`, `Image`, etc). Send only one type to each key to avoid this behavior.\\nWe store metrics in a case-insensitive fashion, so make sure you don\\'t have two metrics with the same name like `\"My-Metric\"` and `\"my-metric\"`.\\nHow can I access the data logged to my runs directly and programmatically?\\nThe history object is used to track metrics logged by `wandb.log`. Using our API, you can access the history object via `run.history()`.\\napi = wandb.Api()\\nrun = api.run(\"username/project/run_id\")\\nprint(run.history())\\nWhat happens when I log millions of steps to W&B? How is that rendered in the browser?\\nThe more points you send us, the longer it will take to load your graphs in the UI. If you have more than 1000 points on a line, we sample down to 1000 points on the backend before we send your browser the data. This sampling is nondeterministic, so if you refresh the page you\\'ll see a different set of sampled points.\\nGuidelines\\nWe recommend that you try to log less than 10,000 points per metric. If you log more than 1 million points in a line, it will take us while to load the page. For more on strategies for reducing logging footprint without sacrificing accuracy, check out this Colab. If you have more than 500 columns of config and summary metrics, we\\'ll only show 500 in the table.', 'source': 'guides/track/log/logging-faqs.md', 'score': 0.8824278999999999, 'relevance': 1}, {'content': 'Date\\nruns = api.runs(\\n    \"<entity>/<project>\",\\n    {\"$and\": [{\"created_at\": {\"$lt\": \"YYYY-MM-DDT##\", \"$gt\": \"YYYY-MM-DDT##\"}}]},\\n)\\nRead specific metrics from a run\\nTo pull specific metrics from a run, use the `keys` argument. The default number of samples when using `run.history()` is 500. Logged steps that do not include a specific metric will appear in the output dataframe as `NaN`. The `keys` argument will cause the API to sample steps that include the listed metric keys more frequently.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nif run.state == \"finished\":\\n    for i, row in run.history(keys=[\"accuracy\"]).iterrows():\\n        print(row[\"_timestamp\"], row[\"accuracy\"])\\nCompare two runs\\nThis will output the config parameters that are different between `run1` and `run2`.\\nimport pandas as pd\\nimport wandb\\napi = wandb.Api()\\n# replace with your <entity>, <project>, and <run_id>\\nrun1 = api.run(\"<entity>/<project>/<run_id>\")\\nrun2 = api.run(\"<entity>/<project>/<run_id>\")\\ndf = pd.DataFrame([run1.config, run2.config]).transpose()\\ndf.columns = [run1.name, run2.name]\\nprint(df[df[run1.name] != df[run2.name]])\\nOutputs:\\n              c_10_sgd_0.025_0.01_long_switch base_adam_4_conv_2fc\\nbatch_size                                 32                   16\\nn_conv_layers                               5                    4\\noptimizer                             rmsprop                 adam\\nUpdate metrics for a run, after the run has finished\\nThis example sets the accuracy of a previous run to `0.9`. It also modifies the accuracy histogram of a previous run to be the histogram of `numpy_array`.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nrun.summary[\"accuracy\"] = 0.9\\nrun.summary[\"accuracy_histogram\"] = wandb.Histogram(numpy_array)\\nrun.summary.update()', 'source': 'guides/track/public-api-guide.md', 'score': 0.8224745, 'relevance': 1}, {'content': 'W&B Line Plots Point aggregation\\nUse point aggregation methods within W&B Line Plots to improve data visualization accuracy and performance. There are two types of point aggregation modes: Random sampling and Full fidelity.\\n:::note\\nWorkspaces use Random sampling mode by default. Switching to Full fidelity applies the chart setting per user.\\n:::\\nRandom sampling\\nFor performance reasons, when over 1500 points were chosen for a line plot metric, the point aggregation method returns 1500 randomly sampled points. Each metric is sampled separately. Only steps where the metric are actually logged are considered. Because random sampling samples non-deterministically, this method sometimes excluded important outliers or spikes.\\nExample: Accessing run history\\nTo access the complete history of metrics logged during a run, you can use the W&B Run API. The following example demonstrates how to retrieve and process the loss values from a specific run:\\n# Initialize the W&B API\\nrun = api.run(\"l2k2/examples-numpy-boston/i0wt6xua\")\\n# Retrieve the history of the \\'Loss\\' metric\\nhistory = run.scan_history(keys=[\"Loss\"])\\n# Extract the loss values from the history\\nlosses = [row[\"Loss\"] for row in history]\\nFull fidelity\\nThe full fidelity point aggregation method replaces random sampling with an averaging approach that maintains the integrity of critical visual insights, such as outliers and spikes. Full fidelity mode guarantees the inclusion of minimum and maximum values within each bucket on your chart, allowing for high-detail zoom capabilities.\\nSome key benfits of full fidelity mode include:\\n* Accurate Data Representation: Ensures all critical outlier spikes are displayed.\\n* High-Density Visualization: Maintains full data resolution beyond the 1,500 point limit.\\n* Enhanced Zoom: Users can zoom into data points without losing detail due to downsampling.', 'source': 'guides/app/features/panels/line-plot/sampling.md', 'score': 0.7108291, 'relevance': 1}, {'content': 'Export Run Data\\nDownload data from a finished or active run. Common usage includes downloading a dataframe for custom analysis in a Jupyter notebook, or using custom logic in an automated environment.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nThe most commonly used attributes of a run object are:\\nAttribute\\nMeaning\\n`run.config`\\nA dictionary of the run\\'s configuration information, such as the hyperparameters for a training run or the preprocessing methods for a run that creates a dataset Artifact. Think of these as the run\\'s \"inputs\".\\n`run.history()`\\nA list of dictionaries meant to store values that change while the model is training such as loss. The command `wandb.log()` appends to this object.\\n`run.summary`\\nA dictionary of information that summarizes the run\\'s results. This can be scalars like accuracy and loss, or large files. By default, `wandb.log()` sets the summary to the final value of a logged time series. The contents of the summary can also be set directly. Think of the summary as the run\\'s \"outputs\".\\nYou can also modify or update the data of past runs. By default a single instance of an api object will cache all network requests. If your use case requires real time information in a running script, call `api.flush()` to get updated values.\\nUnderstanding the Different Attributes\\nFor the below run\\nn_epochs = 5\\nconfig = {\"n_epochs\": n_epochs}\\nrun = wandb.init(project=project, config=config)\\nfor n in range(run.config.get(\"n_epochs\")):\\n    run.log(\\n        {\"val\": random.randint(0, 1000), \"loss\": (random.randint(0, 1000) / 1000.00)}\\n    )\\nrun.finish()\\nthese are the different outputs for the above run object attributes\\nrun.config\\n{\"n_epochs\": 5}', 'source': 'guides/track/public-api-guide.md', 'score': 0.601418, 'relevance': 0}, {'content': 'Runs\\nView source on GitHub\\nAn iterable collection of runs associated with a project and optional filter.\\nRuns(\\n    client: \"RetryingClient\",\\n    entity: str,\\n    project: str,\\n    filters: Optional[Dict[str, Any]] = None,\\n    order: Optional[str] = None,\\n    per_page: int = 50,\\n    include_sweeps: bool = (True)\\n)\\nThis is generally used indirectly via the `Api`.runs method.\\nAttributes\\nMethods\\nconvert_objects\\nView source\\nconvert_objects()\\nhistories\\nView source\\nhistories(\\n    samples: int = 500,\\n    keys: Optional[List[str]] = None,\\n    x_axis: str = \"_step\",\\n    format: Literal[\\'default\\', \\'pandas\\', \\'polars\\'] = \"default\",\\n    stream: Literal[\\'default\\', \\'system\\'] = \"default\"\\n)\\nReturn sampled history metrics for all runs that fit the filters conditions.\\nArguments\\n`samples`\\n(int, optional) The number of samples to return per run\\n`keys`\\n(list[str], optional) Only return metrics for specific keys\\n`x_axis`\\n(str, optional) Use this metric as the xAxis defaults to _step\\n`format`\\n(Literal, optional) Format to return data in, options are \"default\", \"pandas\", \"polars\"\\n`stream`\\n(Literal, optional) \"default\" for metrics, \"system\" for machine metrics\\nReturns\\n`pandas.DataFrame`\\nIf format=\"pandas\", returns a `pandas.DataFrame` of history metrics.\\n`polars.DataFrame`\\nIf format=\"polars\", returns a `polars.DataFrame` of history metrics. list of dicts: If format=\"default\", returns a list of dicts containing history metrics with a run_id key.\\nnext\\nView source\\nnext()\\nupdate_variables\\nView source\\nupdate_variables()\\n__getitem__\\nView source\\n__getitem__(\\n    index\\n)\\n__iter__\\nView source\\n__iter__()\\n__len__\\nView source\\n__len__()\\nClass Variables\\n`QUERY`', 'source': 'ref/python/public-api/runs.md', 'score': 0.58148134, 'relevance': 0}, {'content': \"log\\nView source on GitHub\\nLog a dictionary of data to the current run's history.\\nlog(\\n    data: Dict[str, Any],\\n    step: Optional[int] = None,\\n    commit: Optional[bool] = None,\\n    sync: Optional[bool] = None\\n) -> None\", 'source': 'ref/python/log.md', 'score': 0.56643826, 'relevance': 0}, {'content': 'Get runs from a specific sweep\\nThis snippet downloads all the runs associated with a particular sweep.\\nimport wandb\\napi = wandb.Api()\\nsweep = api.sweep(\"<entity>/<project>/<sweep_id>\")\\nsweep_runs = sweep.runs\\nGet the best run from a sweep\\nThe following snippet gets the best run from a given sweep.\\nimport wandb\\napi = wandb.Api()\\nsweep = api.sweep(\"<entity>/<project>/<sweep_id>\")\\nbest_run = sweep.best_run()\\nThe `best_run` is the run with the best metric as defined by the `metric` parameter in the sweep config.\\nDownload the best model file from a sweep\\nThis snippet downloads the model file with the highest validation accuracy from a sweep with runs that saved model files to `model.h5`.\\nimport wandb\\napi = wandb.Api()\\nsweep = api.sweep(\"<entity>/<project>/<sweep_id>\")\\nruns = sorted(sweep.runs, key=lambda run: run.summary.get(\"val_acc\", 0), reverse=True)\\nval_acc = runs[0].summary.get(\"val_acc\", 0)\\nprint(f\"Best run {runs[0].name} with {val_acc}% val accuracy\")\\nruns[0].file(\"model.h5\").download(replace=True)\\nprint(\"Best model saved to model-best.h5\")\\nDelete all files with a given extension from a run\\nThis snippet deletes files with a given extension from a run.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nextension = \".png\"\\nfiles = run.files()\\nfor file in files:\\n    if file.name.endswith(extension):\\n        file.delete()\\nDownload system metrics data\\nThis snippet produces a dataframe with all the system resource consumption metrics for a run and then saves it to a CSV.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nsystem_metrics = run.history(stream=\"events\")\\nsystem_metrics.to_csv(\"sys_metrics.csv\")\\nUpdate summary metrics\\nYou can pass a dictionary to update summary metrics.\\nsummary.update({\"key\": val})', 'source': 'guides/track/public-api-guide.md', 'score': 0.027066175, 'relevance': 0}, {'content': 'Get the latest git commit through the API\\nIn the UI, click on a run and then click the Overview tab on the run page to see the latest git commit. It\\'s also in the file `wandb-metadata.json` . Using the public API, you can get the git hash with `run.commit`.\\nFrequently Asked Questions\\nHow do I export data to visualize in matplotlib or seaborn?\\nCheck out our API examples for some common export patterns. You can also click the download button on a custom plot or on the expanded runs table to download a CSV from your browser.\\nHow do I get a run\\'s name and ID during a run?\\nAfter calling `wandb.init()` you can access the random run ID or the human readable run name from your script like this:\\nUnique run ID (8 character hash): `wandb.run.id`\\nRandom run name (human readable): `wandb.run.name`\\nIf you\\'re thinking about ways to set useful identifiers for your runs, here\\'s what we recommend:\\nRun ID: leave it as the generated hash. This needs to be unique across runs in your project.\\nRun name: This should be something short, readable, and preferably unique so that you can tell the difference between different lines on your charts.\\nRun notes: This is a great place to put a quick description of what you\\'re doing in your run. You can set this with `wandb.init(notes=\"your notes here\")`\\nRun tags: Track things dynamically in run tags, and use filters in the UI to filter your table down to just the runs you care about. You can set tags from your script and then edit them in the UI, both in the runs table and the overview tab of the run page. See the detailed instructions here.\\nPublic API Examples\\nRead metrics from a run\\nThis example outputs timestamp and accuracy saved with `wandb.log({\"accuracy\": acc})` for a run saved to `\"<entity>/<project>/<run_id>\"`.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nif run.state == \"finished\":\\n    for i, row in run.history().iterrows():\\n        print(row[\"_timestamp\"], row[\"accuracy\"])\\nFilter runs\\nYou can filters by using the MongoDB Query Language.', 'source': 'guides/track/public-api-guide.md', 'score': 0.013171949, 'relevance': 0}])"
        ],
        [
         "8",
         "if I execute a run in offline mode, is there a method of interacting with the run data using the API before it's upload to wandb?",
         "When you execute a run in offline mode using Weights & Biases, the data is stored locally on your machine and not sent to the W&B servers until you decide to sync it. While the run is in offline mode, you can interact with the run data using the local API, but with some limitations compared to the full capabilities when connected online.\n\nHere’s how you can interact with run data in offline mode:\n\n1. **Accessing Run Data Locally**: You can access the run data stored in the local files. W&B stores the data in a directory (usually `wandb/` in your project directory). This includes configuration files, model checkpoints (if saved), and logs.\n\n2. **Using Python API**: You can use the W&B Python API to read the run data from these files. For example, you can load the configuration or the metrics logged during the run by reading the JSON files in the `wandb/` directory.\n\n3. **Syncing Data**: Once you are ready to upload the data to the W&B servers for more comprehensive analysis and sharing, you can use the `wandb sync` command. This command will upload the local run data to the W&B servers, and then you can use the W&B web interface or the public API to interact with your data as usual.\n\nHere is an example of how you might sync your data after running in offline mode:\n\n```bash\nwandb sync /path/to/run/directory\n```\n\nThis command will find all the runs in the specified directory that haven't been synced and will upload them to the W&B servers.\n\nRemember, while the run is in offline mode, some features like real-time charts and collaboration may not be available until the data is synced with the W&B servers. For more detailed information on working with offline runs and syncing them, you can refer to the [Weights & Biases documentation on offline runs](https://docs.wandb.ai/guides/track/tracking-faq).",
         "WeaveList([{'content': 'Is it possible to save metrics offline and sync them to W&B later?\\nBy default, `wandb.init` starts a process that syncs metrics in real time to our cloud hosted app. If your machine is offline, you don\\'t have internet access, or you just want to hold off on the upload, here\\'s how to run `wandb` in offline mode and sync later.\\nYou will need to set two environment variables.\\n`WANDB_API_KEY=$KEY`, where `$KEY` is the API Key from your settings page\\n`WANDB_MODE=\"offline\"`\\nAnd here\\'s a sample of what this would look like in your script:\\nimport wandb\\nimport os\\nos.environ[\"WANDB_API_KEY\"] = YOUR_KEY_HERE\\nos.environ[\"WANDB_MODE\"] = \"offline\"\\nconfig = {\\n    \"dataset\": \"CIFAR10\",\\n    \"machine\": \"offline cluster\",\\n    \"model\": \"CNN\",\\n    \"learning_rate\": 0.01,\\n    \"batch_size\": 128,\\n}\\nwandb.init(project=\"offline-demo\")\\nfor i in range(100):\\n    wandb.log({\"accuracy\": i})\\nHere\\'s a sample terminal output:\\nAnd once you\\'re ready, just run a sync command to send that folder to the cloud.\\nwandb sync wandb/dryrun-folder-name\\nWhat is the difference between wandb.init modes?\\nModes can be \"online\", \"offline\" or \"disabled\", and default to online.\\n`online`(default): In this mode, the client sends data to the wandb server.\\n`offline`: In this mode, instead of sending data to the wandb server, the client will store data on your local machine which can be later synced with the wandb sync command.\\n`disabled`: In this mode, the client returns mocked objects and prevents all network communication. The client will essentially act like a no-op. In other words, all logging is entirely disabled. However, stubs out of all the API methods are still callable. This is usually used in tests.\\nMy run\\'s state is \"crashed\" on the UI but is still running on my machine. What do I do to get my data back?\\nYou most likely lost connection to your machine while training. You can recover your data by running wandb sync [PATH_TO_RUN]. The path to your run will be a folder in your `wandb` directory corresponding to the Run ID of the run in progress.', 'source': 'guides/track/tracking-faq.md', 'score': 0.9896718, 'relevance': 2}, {'content': 'Setup\\nHow can I configure the name of the run in my training code?\\nAt the top of your training script when you call `wandb.init`, pass in an experiment name, like this: `wandb.init(name=\"my_awesome_run\")`.\\nCan I run wandb offline?\\nIf you\\'re training on an offline machine and want to upload your results to our servers afterwards, we have a feature for you!\\nSet the environment variable `WANDB_MODE=offline` to save the metrics locally, no internet required.\\nWhen you\\'re ready, run `wandb init` in your directory to set the project name.\\nRun `wandb sync YOUR_RUN_DIRECTORY` to push the metrics to our cloud service and see your results in our hosted web app.\\nYou can check via API whether your run is offline by using `run.settings._offline` or `run.settings.mode` after your wandb.init().\\nSome use-cases where you can use wandb sync\\nIf you don’t have internet.\\nIf you need to fully disable things.\\nTo sync your run later due to any reason. For instance: if you want to avoid using resources on a training machine.\\nDoes this only work for Python?\\nCurrently, the library only works with Python 2.7+ & 3.6+ projects. The architecture mentioned above should enable us to integrate with other languages easily. If you have a need for monitoring other languages, send us a note at contact@wandb.com.\\nIs there an anaconda package?\\nYes! You can either install with `pip` or with `conda`. For the latter, you\\'ll need to get the package from the conda-forge channel.\\n# Create a conda env\\nconda create -n wandb-env python=3.8 anaconda\\n# Activate created env\\nconda activate wandb-env\\n# install wandb with pip in this conda env\\npip install wandb\\nconda activate myenv\\nconda install wandb --channel conda-forge\\nIf you run into issues with this install, please let us know. This Anaconda doc on managing packages has some helpful guidance.', 'source': 'guides/technical-faq/setup.md', 'score': 0.9644556, 'relevance': 2}, {'content': 'wandb sync\\nUsage\\n`wandb sync [OPTIONS] [PATH]...`\\nSummary\\nUpload an offline training directory to W&B\\nOptions\\nOption\\nDescription\\n--id\\nThe run you want to upload to.\\n-p, --project\\nThe project you want to upload to.\\n-e, --entity\\nThe entity to scope to.\\n--job_type\\nSpecifies the type of run for grouping   related runs together.\\n--sync-tensorboard / --no-sync-tensorboard\\nStream tfevent files to wandb.\\n--include-globs\\nComma separated list of globs to include.\\n--exclude-globs\\nComma separated list of globs to exclude.\\n--include-online / --no-include-online\\nInclude online runs\\n--include-offline / --no-include-offline\\nInclude offline runs\\n--include-synced / --no-include-synced\\nInclude synced runs\\n--mark-synced / --no-mark-synced\\nMark runs as synced\\n--sync-all\\nSync all runs\\n--clean\\nDelete synced runs\\n--clean-old-hours\\nDelete runs created before this many hours.   To be used alongside --clean flag.\\n--clean-force\\nClean without confirmation prompt.\\n--show\\nNumber of runs to show\\n--append\\nAppend run\\n--skip-console\\nSkip console logs', 'source': 'ref/cli/wandb-sync.md', 'score': 0.8743993600000001, 'relevance': 2}, {'content': 'Export Run Data\\nDownload data from a finished or active run. Common usage includes downloading a dataframe for custom analysis in a Jupyter notebook, or using custom logic in an automated environment.\\nimport wandb\\napi = wandb.Api()\\nrun = api.run(\"<entity>/<project>/<run_id>\")\\nThe most commonly used attributes of a run object are:\\nAttribute\\nMeaning\\n`run.config`\\nA dictionary of the run\\'s configuration information, such as the hyperparameters for a training run or the preprocessing methods for a run that creates a dataset Artifact. Think of these as the run\\'s \"inputs\".\\n`run.history()`\\nA list of dictionaries meant to store values that change while the model is training such as loss. The command `wandb.log()` appends to this object.\\n`run.summary`\\nA dictionary of information that summarizes the run\\'s results. This can be scalars like accuracy and loss, or large files. By default, `wandb.log()` sets the summary to the final value of a logged time series. The contents of the summary can also be set directly. Think of the summary as the run\\'s \"outputs\".\\nYou can also modify or update the data of past runs. By default a single instance of an api object will cache all network requests. If your use case requires real time information in a running script, call `api.flush()` to get updated values.\\nUnderstanding the Different Attributes\\nFor the below run\\nn_epochs = 5\\nconfig = {\"n_epochs\": n_epochs}\\nrun = wandb.init(project=project, config=config)\\nfor n in range(run.config.get(\"n_epochs\")):\\n    run.log(\\n        {\"val\": random.randint(0, 1000), \"loss\": (random.randint(0, 1000) / 1000.00)}\\n    )\\nrun.finish()\\nthese are the different outputs for the above run object attributes\\nrun.config\\n{\"n_epochs\": 5}', 'source': 'guides/track/public-api-guide.md', 'score': 0.91102153, 'relevance': 1}, {'content': 'Known issues and limitations\\nIf the destination namespace does not exist, W&B creates one automatically.\\nIf a run or artifact has the same ID in the destination namespace, W&B treats it as an incremental import. The destination run/artifact is validated and retried if it failed in a previous import.\\nNo data is ever deleted from the source system.\\nSometimes when bulk importing (especially large artifacts), you can run into S3 rate limits. If you see `botocore.exceptions.ClientError: An error occurred (SlowDown) when calling the PutObject operation`, you can try spacing out imports by moving just a few namespaces at a time.\\nImported run tables appear to be blank in the workspace, but if you nav to the Artifacts tab and click the equivalent run table artifact you should see the table as expected.\\nSystem metrics and custom charts (not explicitly logged with `wandb.log`) are not imported\\nExport Data\\nUse the Public API to export or update data that you have saved to W&B. Before using this API, you\\'ll want to log data from your script — check the Quickstart for more details.\\nUse Cases for the Public API\\nExport Data: Pull down a dataframe for custom analysis in a Jupyter Notebook. Once you have explored the data, you can sync your findings by creating a new analysis run and logging results, for example: `wandb.init(job_type=\"analysis\")`\\nUpdate Existing Runs: You can update the data logged in association with a W&B run. For example, you might want to update the config of a set of runs to include additional information, like the architecture or a hyperparameter that wasn\\'t originally logged.\\nSee the Generated Reference Docs for details on available functions.\\nAuthentication\\nAuthenticate your machine with your API key in one of two ways:\\nRun `wandb login` on the command line and paste in your API key.\\nSet the `WANDB_API_KEY` environment variable to your API key.\\nFind the run path\\nTo use the Public API, you\\'ll often need the run path which is `<entity>/<project>/<run_id>`. In the app UI, open a run page and click the Overview tab to get the run path.', 'source': 'guides/track/public-api-guide.md', 'score': 0.75635755, 'relevance': 1}, {'content': \"Common Questions\\nWhere do I find my API key?\\nOnce you've signed in to www.wandb.ai, the API key will be on the Authorize page.\\nHow do I use W&B in an automated environment?\\nIf you are training models in an automated environment where it's inconvenient to run shell commands, such as Google's CloudML, you should look at our guide to configuration with Environment Variables.\\nDo you offer local, on-prem installs?\\nYes, you can privately host W&B locally on your own machines or in a private cloud, try this quick tutorial notebook to see how.\\nHow do I turn off wandb logging temporarily?\\nIf are testing code and want to disable wandb syncing, set the environment variable WANDB_MODE=offline.\", 'source': 'quickstart.md', 'score': 0.49597558, 'relevance': 1}, {'content': '📏 Best Practices\\nProjects: Log multiple runs to a project to compare them. `wandb.init(project=\"project-name\")`\\nGroups: For multiple processes or cross validation folds, log each process as a runs and group them together. `wandb.init(group=\\'experiment-1\\')`\\nTags: Add tags to track your current baseline or production model.\\nNotes: Type notes in the table to track the changes between runs.\\nReports: Take quick notes on progress to share with colleagues and make dashboards and snapshots of your ML projects.\\n🤓 Advanced Setup\\nEnvironment variables: Set API keys in environment variables so you can run training on a managed cluster.\\nOffline mode: Use `dryrun` mode to train offline and sync results later.\\nOn-prem: Install W&B in a private cloud or air-gapped servers in your own infrastructure. We have local installations for everyone from academics to enterprise teams.\\nArtifacts: Track and version models and datasets in a streamlined way that automatically picks up your pipeline steps as you train models.', 'source': 'tutorials/tensorflow.md', 'score': 0.21883628, 'relevance': 1}, {'content': '5. Download and use the artifact\\nThe following code example demonstrates the steps you can take to use an artifact you have logged and saved to the W&B servers.\\nFirst, initialize a new run object with `wandb.init()`.\\nSecond, use the run objects use_artifact() method to tell W&B what artifact to use. This returns an artifact object.\\nThird, use the artifacts download() method to download the contents of the artifact.\\n# Create a W&B Run. Here we specify \\'training\\' for \\'type\\'\\n# because we will use this run to track training.\\nrun = wandb.init(project=\"artifacts-example\", job_type=\"training\")\\n# Query W&B for an artifact and mark it as input to this run\\nartifact = run.use_artifact(\"bicycle-dataset:latest\")\\n# Download the artifact\\'s contents\\nartifact_dir = artifact.download()\\nAlternatively, you can use the Public API (`wandb.Api`) to export (or update data) data already saved in a W&B outside of a Run. See Track external files for more information.', 'source': 'guides/artifacts/artifacts-walkthrough.md', 'score': 0.82375413, 'relevance': 0}, {'content': \"How is W&B different from TensorBoard?\\nWe love the TensorBoard folks, and we have a TensorBoard integration! We were inspired to improve experiment tracking tools for everyone. When the co-founders started working on W&B, they were inspired to build a tool for the frustrated TensorBoard users at OpenAI. Here are a few things we focused on improving:\\nReproduce models: W&B is good for experimentation, exploration, and reproducing models later. We capture not just the metrics, but also the hyperparameters and version of the code, and we can save your model checkpoints for you so your project is reproducible.\\nAutomatic organization: If you hand off a project to a collaborator or take a vacation, W&B makes it easy to see all the models you've tried so you're not wasting hours re-running old experiments.\\nFast, flexible integration: Add W&B to your project in 5 minutes. Install our free open-source Python package and add a couple of lines to your code, and every time you run your model you'll have nice logged metrics and records.\\nPersistent, centralized dashboard: Anywhere you train your models, whether on your local machine, your lab cluster, or spot instances in the cloud, we give you the same centralized dashboard. You don't need to spend your time copying and organizing TensorBoard files from different machines.\\nPowerful table: Search, filter, sort, and group results from different models. It's easy to look over thousands of model versions and find the best-performing models for different tasks. TensorBoard isn't built to work well on large projects.\\nTools for collaboration: Use W&B to organize complex machine learning projects. It's easy to share a link to W&B, and you can use private teams to have everyone send results to a shared project. We also support collaboration via reports— add interactive visualizations and describe your work in markdown. This is a great way to keep a work log, share findings with your supervisor, or present findings to your lab.\\nGet started with a free account\\nHow does wandb stream logs and writes to disk?\\nW&B queues in memory but also write the events to disk asynchronously to handle failures and for the `WANDB_MODE=offline` case where you can sync the data after it's been logged.\\nIn your terminal, you can see a path to the local run directory. This directory will contain a `.wandb` file that is the datastore above. If you're also logging images, we write them to `media/images` in that directory before uploading them to cloud storage.\", 'source': 'guides/technical-faq/general.md', 'score': 0.7994422999999999, 'relevance': 0}, {'content': 'Check out interactive examples!\\nYou can follow along in our video tutorial with our tutorial colab here\\nFrequently Asked Questions\\nHow does W&B integrate with Lightning?\\nThe core integration is based on the Lightning loggers API, which lets you write much of your logging code in a framework-agnostic way. `Logger`s are passed to the Lightning Trainer and are triggered based on that API\\'s rich hook-and-callback system. This keeps your research code well-separated from engineering and logging code.\\nWhat does the integration log without any additional code?\\nWe\\'ll save your model checkpoints to W&B, where you can view them or download them for use in future runs. We\\'ll also capture system metrics, like GPU usage and network I/O, environment information, like hardware and OS information, code state (including git commit and diff patch, notebook contents and session history), and anything printed to the standard out.\\nWhat if I really need to use wandb.run in my training setup?\\nYou will have to essentially expand the scope of the variable you need to access yourself. In other words, making sure that the initial conditions are the same on all processes.\\nif os.environ.get(\"LOCAL_RANK\", None) is None:\\n    os.environ[\"WANDB_DIR\"] = wandb.run.dir\\nThen, you can use `os.environ[\"WANDB_DIR\"]` to set up the model checkpoints directory. This way, `wandb.run.dir` can be used by any non-zero rank processes as well.', 'source': 'guides/integrations/lightning.md', 'score': 0.49688914, 'relevance': 0}])"
        ],
        [
         "9",
         "i am not getting loss/average data from training only loss. What are reasons for this?",
         "If you're only seeing loss data and not the average loss during your training sessions with Weights & Biases, there could be several reasons for this issue. Here are some steps and checks you can perform to troubleshoot and resolve this:\n\n1. **Check Your Logging Code**: Ensure that you are logging the average loss explicitly. Weights & Biases does not automatically calculate averages unless specified. You need to manually calculate the average and log it using `wandb.log()`. For example:\n\n   ```python\n   average_loss = total_loss / number_of_batches\n   wandb.log({\"average_loss\": average_loss})\n   ```\n\n2. **Ensure Consistent Logging**: Make sure that you are logging the average loss at the same frequency as the loss. Inconsistencies in logging can lead to missing data in the dashboard.\n\n3. **Check for Errors or Warnings**: Look at the console or the logs for any errors or warnings that might indicate what the issue is. Sometimes, misconfigurations or exceptions during logging can cause some metrics not to be recorded.\n\n4. **Version Compatibility**: Verify that the version of Weights & Biases you are using is compatible with your training framework. Incompatibilities can sometimes lead to unexpected behavior.\n\n5. **Review W&B Initialization**: Ensure that your `wandb.init()` call is correctly configured and that your project is set up correctly in the Weights & Biases dashboard.\n\n6. **Manual Calculation and Logging**: If you're not already, ensure that you are calculating the average loss manually and logging it explicitly. Weights & Biases logs exactly what you tell it to log, so you need to provide the average calculation yourself.\n\nHere is an example snippet that demonstrates how to log both loss and average loss:\n\n```python\nimport wandb\n\n# Initialize a new run\nwandb.init(project=\"my_project\")\n\nfor epoch in range(epochs):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Training code here\n        loss = train(data, target)\n        total_loss += loss.item()\n        # Log loss\n        wandb.log({\"loss\": loss.item()})\n\n    # Calculate and log average loss\n    average_loss = total_loss / len(train_loader)\n    wandb.log({\"average_loss\": average_loss})\n\nwandb.finish()\n```\n\nThis code ensures that both the loss for each batch and the average loss per epoch are logged to your Weights & Biases dashboard. Make sure that your actual training loop and loss calculations are correctly set up to reflect this structure.",
         "WeaveList([{'content': 'Log metrics inside your training loop\\nLog metrics during each `for` loop (epoch), the accuracy and loss values are computed and logged to W&B with wandb.log(). By default, when you call wandb.log it appends a new step to the history object and updates the summary object.\\nThe following code example shows how to log metrics with `wandb.log`.\\n:::note\\nDetails of how to set up your mode and retrieve data are omitted. \\n:::\\n# Set up model and data\\nmodel, dataloader = get_model(), get_data()\\nfor epoch in range(wandb.config.epochs):\\n    for batch in dataloader:\\n        loss, accuracy = model.training_step()\\n        # \\u20033. Log metrics inside your training loop to visualize\\n        # model performance\\n        wandb.log({\"accuracy\": accuracy, \"loss\": loss})\\nFor more information on different data types you can log with W&B, see Log Data During Experiments.\\nLog an artifact to W&B\\nOptionally log a W&B Artifact. Artifacts make it easy to version datasets and models. \\nwandb.log_artifact(model)\\nFor more information about Artifacts, see the Artifacts Chapter. For more information about versioning models, see Model Management.\\nPutting it all together\\nThe full script with the preceding code snippets is found below:\\n# Import the W&B Python Library\\nimport wandb\\n# 1. Start a W&B Run\\nrun = wandb.init(project=\"cat-classification\", notes=\"\", tags=[\"baseline\", \"paper1\"])\\n# \\u20032. Capture a dictionary of hyperparameters\\nwandb.config = {\"epochs\": 100, \"learning_rate\": 0.001, \"batch_size\": 128}\\n# Set up model and data\\nmodel, dataloader = get_model(), get_data()\\nfor epoch in range(wandb.config.epochs):\\n    for batch in dataloader:\\n        loss, accuracy = model.training_step()\\n        # \\u20033. Log metrics inside your training loop to visualize\\n        # model performance\\n        wandb.log({\"accuracy\": accuracy, \"loss\": loss})\\n# 4. Log an artifact to W&B\\nwandb.log_artifact(model)\\n# Optional: save model at the end\\nmodel.to_onnx()\\nwandb.save(\"model.onnx\")', 'source': 'guides/track/launch.md', 'score': 0.003976228, 'relevance': 2}, {'content': 'Log a dictionary of data to the current run\\'s history.\\nUse `wandb.log` to log data from runs, such as scalars, images, video,\\nhistograms, plots, and tables.\\nSee our guides to logging for\\nlive examples, code snippets, best practices, and more.\\nThe most basic usage is `wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.\\nThis will save the loss and accuracy to the run\\'s history and update\\nthe summary values for these metrics.\\nVisualize logged data in the workspace at wandb.ai,\\nor locally on a self-hosted instance\\nof the W&B app, or export data to visualize and explore locally, e.g. in\\nJupyter notebooks, with our API.\\nIn the UI, summary values show up in the run table to compare single values across runs.\\nSummary values can also be set directly with `wandb.run.summary[\"key\"] = value`.\\nLogged values don\\'t have to be scalars. Logging any wandb object is supported.\\nFor example `wandb.log({\"example\": wandb.Image(\"myimage.jpg\")})` will log an\\nexample image which will be displayed nicely in the W&B UI.\\nSee the reference documentation\\nfor all of the different supported types or check out our\\nguides to logging for examples,\\nfrom 3D molecular structures and segmentation masks to PR curves and histograms.\\n`wandb.Table`s can be used to logged structured data. See our\\nguide to logging tables\\nfor details.\\nLogging nested metrics is encouraged and is supported in the W&B UI.\\nIf you log with a nested dictionary like `wandb.log({\"train\": {\"acc\": 0.9}, \"val\": {\"acc\": 0.8}})`, the metrics will be organized into\\n`train` and `val` sections in the W&B UI.\\nwandb keeps track of a global step, which by default increments with each\\ncall to `wandb.log`, so logging related metrics together is encouraged.\\nIf it\\'s inconvenient to log related metrics together\\ncalling `wandb.log({\"train-loss\": 0.5}, commit=False)` and then\\n`wandb.log({\"accuracy\": 0.9})` is equivalent to calling\\n`wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.', 'source': 'ref/python/run.md', 'score': 0.0020667946, 'relevance': 2}, {'content': 'Use `wandb.log` to log data from runs, such as scalars, images, video,\\nhistograms, plots, and tables.\\nSee our guides to logging for\\nlive examples, code snippets, best practices, and more.\\nThe most basic usage is `wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.\\nThis will save the loss and accuracy to the run\\'s history and update\\nthe summary values for these metrics.\\nVisualize logged data in the workspace at wandb.ai,\\nor locally on a self-hosted instance\\nof the W&B app, or export data to visualize and explore locally, e.g. in\\nJupyter notebooks, with our API.\\nIn the UI, summary values show up in the run table to compare single values across runs.\\nSummary values can also be set directly with `wandb.run.summary[\"key\"] = value`.\\nLogged values don\\'t have to be scalars. Logging any wandb object is supported.\\nFor example `wandb.log({\"example\": wandb.Image(\"myimage.jpg\")})` will log an\\nexample image which will be displayed nicely in the W&B UI.\\nSee the reference documentation\\nfor all of the different supported types or check out our\\nguides to logging for examples,\\nfrom 3D molecular structures and segmentation masks to PR curves and histograms.\\n`wandb.Table`s can be used to logged structured data. See our\\nguide to logging tables\\nfor details.\\nLogging nested metrics is encouraged and is supported in the W&B UI.\\nIf you log with a nested dictionary like `wandb.log({\"train\": {\"acc\": 0.9}, \"val\": {\"acc\": 0.8}})`, the metrics will be organized into\\n`train` and `val` sections in the W&B UI.\\nwandb keeps track of a global step, which by default increments with each\\ncall to `wandb.log`, so logging related metrics together is encouraged.\\nIf it\\'s inconvenient to log related metrics together\\ncalling `wandb.log({\"train-loss\": 0.5}, commit=False)` and then\\n`wandb.log({\"accuracy\": 0.9})` is equivalent to calling\\n`wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.', 'source': 'ref/python/log.md', 'score': 0.0017546144, 'relevance': 2}, {'content': '@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\\n    def log_training_loss(engine):\\n        pbar.desc = desc.format(engine.state.output)\\n        pbar.update(log_interval)\\n    @trainer.on(Events.EPOCH_COMPLETED)\\n    def log_training_results(engine):\\n        pbar.refresh()\\n        evaluator.run(train_loader)\\n        metrics = evaluator.state.metrics\\n        avg_accuracy = metrics[\\'accuracy\\']\\n        avg_nll = metrics[\\'nll\\']\\n        tqdm.write(\\n            \"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\\n            .format(engine.state.epoch, avg_accuracy, avg_nll)\\n        )\\n    @trainer.on(Events.EPOCH_COMPLETED)\\n    def log_validation_results(engine):\\n        evaluator.run(val_loader)\\n        metrics = evaluator.state.metrics\\n        avg_accuracy = metrics[\\'accuracy\\']\\n        avg_nll = metrics[\\'nll\\']\\n        tqdm.write(\\n            \"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\\n            .format(engine.state.epoch, avg_accuracy, avg_nll))\\n        pbar.n = pbar.last_print_n = 0\\n    trainer.run(train_loader, max_epochs=epochs)\\n    pbar.close()\\nif __name__ == \"__main__\":\\n    parser = ArgumentParser()\\n    parser.add_argument(\\'--batch_size\\', type=int, default=64,\\n                        help=\\'input batch size for training (default: 64)\\')\\n    parser.add_argument(\\'--val_batch_size\\', type=int, default=1000,\\n                        help=\\'input batch size for validation (default: 1000)\\')\\n    parser.add_argument(\\'--epochs\\', type=int, default=10,\\n                        help=\\'number of epochs to train (default: 10)\\')\\n    parser.add_argument(\\'--lr\\', type=float, default=0.01,\\n                        help=\\'learning rate (default: 0.01)\\')\\n    parser.add_argument(\\'--momentum\\', type=float, default=0.5,\\n                        help=\\'SGD momentum (default: 0.5)\\')\\n    parser.add_argument(\\'--log_interval\\', type=int, default=10,\\n                        help=\\'how many batches to wait before logging training status\\')\\n    args = parser.parse_args()\\n    run(args.batch_size, args.val_batch_size, args.epochs, args.lr, args.momentum, args.log_interval)\\nWe get these visualizations on running the above code:\\nRefer Ignite Docs for more detailed documentation', 'source': 'guides/integrations/other/ignite.md', 'score': 0.021125192, 'relevance': 1}, {'content': '# ✨ W&B: Log loss over training steps, visualized in the UI live\\n        wandb.log({\"loss\" : loss})\\n        if (i+1) % 100 == 0:\\n            print (\\'Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}\\'\\n                .format(epoch+1, EPOCHS, i+1, total_step, loss.item()))', 'source': 'tutorials/tables.md', 'score': 0.00685048, 'relevance': 1}, {'content': 'Logging FAQs\\nFrequently Asked Questions About Logging Data from Experiments\\nHow can I organize my logged charts and media in the W&B UI?\\nWe treat `/` as a separator for organizing logged panels in the W&B UI. By default, the component of the logged item\\'s name before a `/` is used to define a group of panel called a \"Panel Section\".\\nwandb.log({\"val/loss\": 1.1, \"val/acc\": 0.3})\\nwandb.log({\"train/loss\": 0.1, \"train/acc\": 0.94})\\nIn the Workspace settings, you can change whether panels are grouped by just the first component or by all components separated by `/`.\\nHow can I compare images or media across epochs or steps?\\nEach time you log images from a step, we save them to show in the UI. Expand the image panel, and use the step slider to look at images from different steps. This makes it easy to compare how a model\\'s output changes during training.\\nWhat if I want to log some metrics on batches and some metrics only on epochs?\\nIf you\\'d like to log certain metrics in every batch and standardize plots, you can log x axis values that you want to plot with your metrics. Then in the custom plots, click edit and select a custom x-axis.\\nwandb.log({\"batch\": batch_idx, \"loss\": 0.3})\\nwandb.log({\"epoch\": epoch, \"val_acc\": 0.94})\\nHow do I log a list of values?\\nwandb.log({f\"losses/loss-{ii}\": loss for ii, loss in enumerate(losses)})\\nwandb.log({\"losses\": wandb.Histogram(losses)})  # converts losses to a histogram', 'source': 'guides/track/log/logging-faqs.md', 'score': 0.0028228330000000003, 'relevance': 1}, {'content': 'Preventing x-axis Misalignments\\nSometimes you might need to perform multiple calls to `wandb.log` for the same training step. The wandb SDK has its own internal step counter that is incremented every time a `wandb.log` call is made. This means that there is a possibility that the wandb log counter is not aligned with the training step in your training loop.\\nIn first pass of the example below, the internal `wandb` step for `train/loss` will be 0, while the internal `wandb` step for  `eval/loss`  will be 1. On the next pass, the `train/loss` will be 2, while the  `eval/loss` wandb step will be 3, etc\\nfor input, ground_truth in data:\\n    ...\\n    wandb.log(“train/loss”: 0.1)  \\n    wandb.log(“eval/loss”: 0.2)\\nTo avoid this, we recommend that you specifically define your x-axis step. You can define the x-axis with `wandb.define_metric` and you only need to do this once, after `wandb.init` is called:\\nwandb.init(...)\\nwandb.define_metric(\"*\", step_metric=\"global_step\")\\nThe glob pattern, \"*\", means that every metric will use “global_step” as the x-axis in your charts. If you only want certain metrics to be logged against \"global_step\", you can specify them instead:\\nwandb.define_metric(\"train/loss\", step_metric=\"global_step\")\\nNow that you\\'ve called `wandb.define_metric` , you just need to log your metrics as well as your `step_metric`, \"global_step\", every time you call `wandb.log`:\\nfor step, (input, ground_truth) in enumerate(data):\\n    ...\\n    wandb.log({“global_step”: step, “train/loss”: 0.1})\\n    wandb.log({“global_step”: step, “eval/loss”: 0.2})\\nIf you do not have access to the independent step variable, for example “global_step” is not available during your validation loop, the previously logged value for \"global_step\" is automatically used by wandb. In this case, ensure you log an initial value for the metric so it has been defined when it’s needed.', 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.0027255826, 'relevance': 1}, {'content': 'print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\\n    # If you had a test set, this is how you could log it as a Summary metric\\n    wandb.summary[\\'test_accuracy\\'] = 0.8\\n    # 🐝 Close your wandb run \\n    wandb.finish()', 'source': 'tutorials/experiments.md', 'score': 0.0043483814, 'relevance': 0}, {'content': \"What happens if internet connection is lost while I'm training a model?\\nIf our library is unable to connect to the internet it will enter a retry loop and keep attempting to stream metrics until the network is restored. During this time your program is able to continue running.\\nIf you need to run on a machine without internet, you can set `WANDB_MODE=offline` to only have metrics stored locally on your hard drive. Later you can call `wandb sync DIRECTORY` to have the data streamed to our server.\", 'source': 'guides/technical-faq/troubleshooting.md', 'score': 0.0013406205, 'relevance': 0}, {'content': 'Logged Metrics\\nEach recipe has their own training loop, so check each individual recipe to see what metrics are logged. The default metrics logged are:\\nMetric\\nDescription\\n`loss`\\nThe loss of the model\\n`lr`\\nThe learning rate\\n`tokens_per_second`\\nThe tokens per second of the model\\n`grad_norm`\\nThe gradient norm of the model\\n`global_step`\\nCorresponds to the current step in the training loop. Takes into account gradient accumulation, basically every time an optimizer step is taken, the model is updated, the gradients are accumulated and the model is updated once every `gradient_accumulation_steps`\\n:::info\\n`global_step` is not the same as the number of training steps. It corresponds to the current step in the training loop. Takes into account gradient accumulation, basically every time an optimizer step is taken the `global_step` is incremented by 1. For example, if the dataloader has 10 batches, gradient accumulation steps is 2 and run for 3 epochs, the optimizer will step 15 times, in this case `global_step` will range from 1 to 15.\\n:::\\nThe streamlined design of torchtune allows to easily add custom metrics or modify the existing ones. It suffices to modify the corresponding recipe file, for example, computing one could log `current_epoch` as a percentage of the total number of epochs as following:\\n# inside `train.py` function in the recipe file\\nself._metric_logger.log_dict(\\n    {\"current_epoch\": self.epochs * self.global_step / self._steps_per_epoch},\\n    step=self.global_step,\\n)\\n:::info\\nThis is a fast evolving library, the current metrics are subject to change. If you want to add a custom metric, you should modify the recipe and call the corresponding `self._metric_logger.*` function.\\n:::', 'source': 'guides/integrations/torchtune.md', 'score': 0.0006411777000000001, 'relevance': 0}])"
        ],
        [
         "10",
         "set entity of weave project to not personal",
         "To change the entity of a Weave project from personal to another type in Weights & Biases, you can specify the entity during the initialization of your project using the `wandb.init()` function. Here are the steps to set a Weave project to a non-personal entity:\n\n1. **Specify the Entity in `wandb.init()`:** When you start a new run in your script, you can specify the `entity` parameter to the name of the team or organization you want to use instead of your personal user account.\n\n   ```python\n   import wandb\n\n   # Initialize a W&B run\n   wandb.init(project=\"your_project_name\", entity=\"team_or_organization_name\")\n   ```\n\n2. **Using Weave SDK:** If you are using Weave SDK, you can specify the entity when initializing monitoring or logging configurations.\n\n   ```python\n   from weave.monitoring import init_monitor\n\n   # Initialize monitoring to a specific entity and project\n   monitor = init_monitor(\"team_or_organization_name/project_name/table_name\")\n   ```\n\n3. **Change Default Entity Settings:** You can also change your default entity settings in your user settings on the W&B web interface. Navigate to `/settings` on the W&B website to set the default entity to a team or organization. This setting will apply to all new projects created unless specified otherwise in the code.\n\nBy setting the `entity` parameter, you direct all logs and data from your project to the specified team or organization, allowing for collaborative work and centralized data management within that entity.",
         "WeaveList([{'content': 'Project Defaults\\nYou can change your project default settings manually in your User Settings at `/settings`.\\nDefault location to create new projects: This is set to your own personal entity by default. By clicking on the dropdown, you can switch between your personal entity and the teams you\\'re part of.\\nDefault project privacy in your personal account: This is set to \\'Private\\' by default. In other words, your projects will be private and can only be accessed by you.\\nEnable code saving in your personal account: This is turned off by default. You can turn this on to save the main script or notebook to W&B.\\n:::note\\nThese settings can also be specified by passing arguments to \\nwandb.init.\\n:::\\nFrequently Asked Questions\\nHow can I delete projects?\\nYou can delete your project by clicking the three dots on the right of the overview tab.\\nIf the project is empty (i.e. it has no runs), you can delete it by clicking the dropdown menu in the top-right and selecting \"Delete project\".\\nWhere are the privacy settings for projects? How can I make a project public or private?\\nClick the lock in the navigation bar at the top of the page to change project privacy settings. You can edit who can view or submit runs to your project. These settings include all runs and reports in the project. If you\\'d like to share your results with just a few people, you can create a private team.\\nHow do I reset my workspace?\\nIf you see an error like the one below on your project page, here\\'s how to reset your workspace.`\"objconv: \"100000000000\" overflows the maximum values of a signed 64 bits integer\"`\\nAdd `?workspace=clear` to the end of the URL and press enter. This should take you to a cleared version of your project page workspace.', 'source': 'guides/app/pages/project-page.md', 'score': 0.5212427, 'relevance': 2}, {'content': 'Manage Runs\\nMove runs to a team\\nOn the project page:\\nClick the table tab to expand the runs table\\nClick the checkbox to select all runs\\nClick Move: the destination project can be in your personal account or any team that you\\'re a member of.\\nSend new runs to a team\\nIn your script, set the entity to your team. \"Entity\" just means your username or team name. Create an entity (personal account or team account) in the web app before sending runs there.\\nwandb.init(entity=\"example-team\")\\nYour default entity is updated when you join a team. This means that on your settings page, you\\'ll see that the default location to create a new project is now the team you\\'ve just joined. Here\\'s an example of what that settings page section looks like:', 'source': 'guides/runs/manage-runs.md', 'score': 0.08137364, 'relevance': 2}, {'content': 'At the end of your script, we will automatically call `wandb.finish` to\\nfinalize and cleanup the run. However, if you call `wandb.init` from a\\nchild process, you must explicitly call `wandb.finish` at the end of the\\nchild process.\\nFor more on using `wandb.init()`, including detailed examples, check out our\\nguide and FAQs.\\nArguments\\n`project`\\n(str, optional) The name of the project where you\\'re sending the new run. If the project is not specified, the run is put in an \"Uncategorized\" project.\\n`entity`\\n(str, optional) An entity is a username or team name where you\\'re sending runs. This entity must exist before you can send runs there, so make sure to create your account or team in the UI before starting to log runs. If you don\\'t specify an entity, the run will be sent to your default entity. Change your default entity in your settings under \"default location to create new projects\".\\n`config`\\n(dict, argparse, absl.flags, str, optional) This sets `wandb.config`, a dictionary-like object for saving inputs to your job, like hyperparameters for a model or settings for a data preprocessing job. The config will show up in a table in the UI that you can use to group, filter, and sort runs. Keys should not contain `.` in their names, and values should be under 10 MB. If dict, argparse or absl.flags: will load the key value pairs into the `wandb.config` object. If str: will look for a yaml file by that name, and load config from that file into the `wandb.config` object.\\n`save_code`\\n(bool, optional) Turn this on to save the main script or notebook to W&B. This is valuable for improving experiment reproducibility and to diff code across experiments in the UI. By default this is off, but you can flip the default behavior to on in your settings page.\\n`group`\\n(str, optional) Specify a group to organize individual runs into a larger experiment. For example, you might be doing cross validation, or you might have multiple jobs that train and evaluate a model against different test sets. Group gives you a way to organize runs together into a larger whole, and you can toggle this on and off in the UI. For more details, see our guide to grouping runs.', 'source': 'ref/python/init.md', 'score': 0.037892427, 'relevance': 2}, {'content': 'Admin\\nWhat is the difference between team and organization?\\nA team is a collaborative workspace for a group of users working on the same projects, while an organization is a higher-level entity that may consist of multiple teams and is often related to billing and account management.\\nWhat is the difference between team and entity? As a user - what does entity mean for me?\\nA team is a collaborative workspace for a group of users working on the same projects, while an entity refers to either a username or a team name. When you log runs in W&B, you can set the entity to your personal account or a team account `wandb.init(entity=\"example-team\")`.\\nWhat is a team and where can I find more information about it?\\nIf you want to know more about teams, visit the teams section.\\nWhen should I log to my personal entity against my team entity?\\nPersonal Entities are no longer available for accounts created after May 21st, 2024. W&B encourages all users, regardless of sign up date, to log new projects to a Team so you have the option to share your results with others.\\nWho can create a team? Who can add or delete people from a team? Who can delete projects?\\nYou can check the different roles and permissions here.\\nWhat type of roles are available and what are the differences between them?\\nGo to this page to see the different roles and permissions available.\\nWhat are service accounts, and how do we add one to our team?\\nCheck this page from our docs to know more about service accounts.\\nHow can I see the bytes stored, bytes tracked and tracked hours of my organization?\\nYou can check the bytes stored of your organization at `https://<host-url>/usage/<team-name>`.\\nYou can check the bytes tracked of your organization at `https://<host-url>/usage/<team-name>/tracked`.\\nYou can check the tracked hours of your organization at `https://<host-url>/usage/<team-name>/computehour`.\\nWhat really good functionalities are hidden and where can I find those?\\nWe have some functionalities hidden under a feature flag in the “Beta Features” section. These can be enabled under the user settings page.', 'source': 'guides/technical-faq/admin.md', 'score': 0.7354516999999999, 'relevance': 1}, {'content': 'User settings\\nNavigate to your user profile page and select your user icon on the top right corner. From the dropdown, choose Settings.\\nProfile\\nWithin the Profile section you can manage and modify your account name and institution. You can optionally add a biography, location, link to a personal or your institution’s website, and upload a profile image.\\nProject defaults\\nChange the default behavior for your account within the Project Defaults section. You can manage the proceeding:\\nDefault location to create new projects - Select the dropdown menu and choose the entity to set as the new default. Specify either your account or a team you are a member of.\\nDefault projects privacy in your personal account - Set a project to public (anyone can view), private (only you can view and contribute) or open (anyone can submit runs or write the reports) automatically when you create a project. You can optionally create a team to collaborate on private projects.\\nEnable code savings in your personal account - Permit Weights and Biases to save the latest git commit hash by default. To enable code saving, toggle the Enable code savings in your personal account option. For more information about saving and comparing code, see Code Saving.\\nTeams\\nCreate a new team in the Team section. To create a new team, select the New team button and provide the following:\\nTeam name - the name of your team. The team mane must be unique. Team names can not be changed.\\nTeam type - Select either the Work or Academic button.\\nCompany/Organization - Provide the name of the team’s company or organization. Choose the dropdown menu to select a company or organization. You can optionally provide a new organization.\\n:::info\\nOnly administrative accounts can create a team.\\n:::\\nBeta features\\nWithin the Beta Features section you can optionally enable fun add-ons and sneak previews of new products in development. Select the toggle switch next to the beta feature you want to enable.\\nAlerts\\nGet notified when your runs crash, finish, or set custom alerts with wandb.alert(). Receive notifications either through Email or Slack. Toggle the switch next to the event type you want to receive alerts from.\\nRuns finished: whether a Weights and Biases run successfully finished.\\nRun crashed: notification if a run has failed to finish.\\nFor more information about how to set up and manage alerts, see Send alerts with wandb.alert.', 'source': 'guides/app/settings-page/user-settings.md', 'score': 0.006463138, 'relevance': 1}, {'content': 'User Login\\nThere are a few ways for your users to log in to W&B:\\nLog into W&B with a bash command in a terminal\\nwandb login $MY_WANDB_KEY\\nIf they\\'re in a Jupyter or Colab notebook, log into W&B like so\\nimport wandb\\nwandb.login\\nSet a W&B environment variable for the API key\\nexport WANDB_API_KEY=$YOUR_API_KEY\\nor\\nos.environ[\\'WANDB_API_KEY\\'] = \"abc123...\"\\nIf a user is using wandb for the first time without following any of the steps mentioned above, they will automatically be prompted to login when your script calls `wandb.init`\\nStarting A wandb Run\\nA W&B Run is a unit of computation logged by W&B. Typically you associate a single W&B Run per training experiment.\\nInitialize W&B and start a Run within your code with:\\nwandb.init()\\nOptionally you can provide a name for their project, or let the user set it themselves with parameter such as `wandb_project` in your code along with the username or team name, such as `wandb_entity` , for the entity parameter:\\nwandb.init(project=wandb_project, entity=wandb_entity)\\nWhere To Place wandb.init?\\nYour library should create W&B Run as early as possible because any output in your console, including error messages, are logged as part of the W&B Run. This makes debugging easier.', 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.004331502, 'relevance': 1}, {'content': 'Serving a Weave Model\\nGiven a weave reference any WeaveModel object, you can spin up a fastapi server and serve it.\\nYou can find the weave reference of any WeaveModel by navigating to the model and copying it from the UI.\\nYou can serve your model by using the following command in the terminal:\\nweave serve weave:///your_entity/project-name/YourModel:<hash>', 'source': 'weave/guides/integrations/dspy.md', 'score': 0.031026179, 'relevance': 0}, {'content': 'Dagster\\nUse Dagster and W&B (W&B) to orchestrate your MLOps pipelines and maintain ML assets. The integration with W&B makes it easy within Dagster to:\\nUse and create W&B Artifacts.\\nUse and create Registered Models in W&B Model Registry.\\nRun training jobs on dedicated compute using W&B Launch.\\nUse the wandb client in ops and assets.\\nThe W&B Dagster integration provides a W&B-specific Dagster resource and IO Manager:\\n`wandb_resource`: a Dagster resource used to authenticate and communicate to the W&B API. \\n`wandb_artifacts_io_manager`: a Dagster IO Manager used to consume W&B Artifacts.\\nThe following guide demonstrates how to satisfy prerequisites to use W&B in Dagster, how to create and use W&B Artifacts in ops and assets, how to use W&B Launch and recommended best practices.\\nBefore you get started\\nYou will need the following resources to use Dagster within Weights and Biases:\\n1. W&B API Key.\\n2. W&B entity (user or team): An entity is a username or team name where you send W&B Runs and Artifacts. Make sure to create your account or team entity in the W&B App UI before you log runs. If you do not specify ain entity, the run will be sent to your default entity, which is usually your username. Change your default entity in your settings under Project Defaults.\\n3. W&B project: The name of the project where W&B Runs are stored.\\nFind your W&B entity by checking the profile page for that user or team in the W&B App. You can use a pre-existing W&B project or create a new one. New projects can be created on the W&B App homepage or on user/team profile page. If a project does not exist it will be automatically created when you first use it. The proceeding instructions demonstrate how to get an API key:', 'source': 'guides/integrations/dagster.md', 'score': 0.015483987500000001, 'relevance': 0}, {'content': 'Serve\\nGiven a Weave ref to any Weave Model you can run:\\nweave serve <ref>\\nto run a FastAPI server for that model. Visit http://0.0.0.0:9996/docs to query the model interactively.\\nInstall FastAPI\\npip install fastapi uvicorn\\nServe Model\\nIn a terminal, call:\\nweave serve <your model ref>\\nGet your model ref by navigating to the model and copying it from the UI. It should look like:\\n`weave:///your_entity/project-name/YourModel:<hash>`\\nTo use it, navigate to the Swagger UI link, click the predict endpoint and then click \"Try it out!\".', 'source': 'weave/guides/tools/serve.md', 'score': 0.01384861, 'relevance': 0}, {'content': 'Objects\\nWeave\\'s serialization layer saves and versions Python objects.\\nPublishing an object\\nimport weave\\n# Initialize tracking to the project \\'intro-example\\'\\nweave.init(\\'intro-example\\')\\n# Save a list, giving it the name \\'cat-names\\'\\nweave.publish([\\'felix\\', \\'jimbo\\', \\'billie\\'], \\'cat-names\\')\\nSaving an object with a name will create the first version of that object if it doesn\\'t exist.\\nGetting an object back\\n`weave.publish` returns a Ref. You can call `.get()` on any Ref to get the object back.\\nYou can construct a ref and then fetch the object back.\\nweave.init(\\'intro-example\\')\\ncat_names = weave.ref(\\'cat-names\\').get()\\nRef styles\\nA fully qualified weave object ref uri looks like this:\\nweave:///<entity>/<project>/object/<object_name>:<object_version>\\nentity: wandb entity (username or team)\\nproject: wandb project\\nobject_name: object name\\nobject_version: either a version hash, a string like v0, v1..., or an alias like \":latest\". All objects have the \":latest\" alias.\\nRefs can be constructed with a few different styles\\n`weave.ref(<name>)`: requires `weave.init(<project>)` to have been called. Refers to the \":latest\" version\\n`weave.ref(<name>:<version>)`: requires `weave.init(<project>)` to have been called.\\n`weave.ref(<fully_qualified_ref_uri>)`: can be constructed without calling weave.init', 'source': 'weave/guides/tracking/objects.md', 'score': 0.009090384, 'relevance': 0}])"
        ],
        [
         "11",
         "what are the differences between grid, random and bayesian sweep and when would you chose one over the other",
         "In Weights & Biases, you can choose from three main types of hyperparameter search strategies for your sweeps: grid, random, and Bayesian. Each has its own advantages and is suitable for different scenarios.\n\n### Grid Search\n- **Description**: Grid search iterates over every possible combination of hyperparameter values. It's very thorough as it exhaustively explores the search space.\n- **Advantages**: This method is deterministic and can be very effective in finding the optimal combination of parameters.\n- **Disadvantages**: It can be computationally expensive and impractical when the number of hyperparameters or their possible values is large.\n- **When to Use**: Choose grid search when you have a manageable number of hyperparameters and discrete values, and computational resources are not a constraint.\n\n### Random Search\n- **Description**: Random search selects hyperparameter values randomly according to specified distributions. This method does not consider past results but relies on random sampling.\n- **Advantages**: It's surprisingly effective and often finds a good solution faster than grid search, especially in high-dimensional spaces.\n- **Disadvantages**: It might miss important areas of the search space and relies heavily on randomness, which can lead to variability in the results.\n- **When to Use**: Use random search when you have a high-dimensional space or when you want a quicker search that still covers a broad range of the search space.\n\n### Bayesian Search\n- **Description**: Bayesian search builds a probabilistic model that maps hyperparameters to the probability of a metric score and uses this model to choose new hyperparameters to evaluate.\n- **Advantages**: It's more efficient than grid or random search as it uses previous results to inform future choices, potentially reducing the number of steps to find an optimal solution.\n- **Disadvantages**: It can be complex to set up and may scale poorly with the number of hyperparameters.\n- **When to Use**: Opt for Bayesian search when you have a smaller number of continuous hyperparameters and sufficient computational resources to support the building and querying of the probabilistic model.\n\n### Choosing the Right Method\n- **Grid Search** is best when you need thoroughness over efficiency and have the computational resources to support it.\n- **Random Search** is suitable for a quick, broad search, especially when the parameter space is large.\n- **Bayesian Search** is ideal when you can leverage prior knowledge from previous experiments, focusing on areas of the parameter space that are more likely to yield improvements.\n\nEach method has its place depending on the specific requirements of your project, including the size of the hyperparameter space, the nature of the parameters (discrete vs. continuous), and computational constraints.",
         "WeaveList([{'content': 'Grid search\\nIterate over every combination of hyperparameter values. Grid search makes uninformed decisions on the set of hyperparameter values to use on each iteration. Grid search can be computationally costly.     \\nGrid search executes forever if it is searching within in a continuous search space.\\nRandom search\\nChoose a random, uninformed, set of hyperparameter values on each iteration based on a distribution. Random search runs forever unless you stop the process from the command line, within your python script, or the W&B App UI.\\nSpecify the distribution space with the metric key if you choose random (`method: random`) search.\\nBayesian search\\nIn contrast to random and grid search, Bayesian models make informed decisions. Bayesian optimization uses a probabilistic model to decide which values to use through an iterative process of testing values on a surrogate function before evaluating the objective function. Bayesian search works well for small numbers of continuous parameters but scales poorly. For more information about Bayesian search, see the Bayesian Optimization Primer paper.\\nBayesian search runs forever unless you stop the process from the command line, within your python script, or the W&B App UI.', 'source': 'guides/sweeps/sweep-config-keys.md', 'score': 0.94994867, 'relevance': 2}, {'content': \"👈 Pick a method\\nThe first thing we need to define is the `method`\\nfor choosing new parameter values.\\nWe provide the following search `methods`:\\n*   `grid` Search – Iterate over every combination of hyperparameter values.\\nVery effective, but can be computationally costly.\\n*   `random` Search – Select each new combination at random according to provided `distribution`s. Surprisingly effective!\\n*   `bayes`ian Search – Create a probabilistic model of metric score as a function of the hyperparameters, and choose parameters with high probability of improving the metric. Works well for small numbers of continuous parameters but scales poorly.\\nWe'll stick with `random`.\\nsweep_config = {\\n    'method': 'random'\\n    }\\nFor `bayes`ian Sweeps,\\nyou also need to tell us a bit about your `metric`.\\nWe need to know its `name`, so we can find it in the model outputs\\nand we need to know whether your `goal` is to `minimize` it\\n(e.g. if it's the squared error)\\nor to `maximize` it\\n(e.g. if it's the accuracy).\\nmetric = {\\n    'name': 'loss',\\n    'goal': 'minimize'   \\n    }\\nsweep_config['metric'] = metric\\nIf you're not running a `bayes`ian Sweep, you don't have to,\\nbut it's not a bad idea to include this in your `sweep_config` anyway,\\nin case you change your mind later.\\nIt's also good reproducibility practice to keep note of things like this,\\nin case you, or someone else,\\ncome back to your Sweep in 6 months or 6 years\\nand don't know whether `val_G_batch` is supposed to be high or low.\", 'source': 'tutorials/sweeps.md', 'score': 0.8441521999999999, 'relevance': 2}, {'content': '1. Define the Sweep\\nWeights & Biases sweeps give you powerful levers to configure your sweeps exactly how you want them, with just a few lines of code. The sweeps config can be defined as\\na dictionary or a YAML file.\\nLet\\'s walk through some of them together:\\n*   Metric – This is the metric the sweeps are attempting to optimize. Metrics can take a `name` (this metric should be logged by your training script) and a `goal` (`maximize` or `minimize`). \\n*   Search Strategy – Specified using the `\"method\"` key. We support several different search strategies with sweeps. \\n  *   Grid Search – Iterates over every combination of hyperparameter values.\\n  *   Random Search – Iterates over randomly chosen combinations of hyperparameter values.\\n  *   Bayesian Search – Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values.\\n*   Parameters – A dictionary containing the hyperparameter names, and discrete values, a range, or distributions from which to pull their values on each iteration.\\nYou can find a list of all configuration options here.\\nsweep_config = {\\n    \"method\": \"random\", # try grid or random\\n    \"metric\": {\\n      \"name\": \"accuracy\",\\n      \"goal\": \"maximize\"   \\n    },\\n    \"parameters\": {\\n        \"booster\": {\\n            \"values\": [\"gbtree\",\"gblinear\"]\\n        },\\n        \"max_depth\": {\\n            \"values\": [3, 6, 9, 12]\\n        },\\n        \"learning_rate\": {\\n            \"values\": [0.1, 0.05, 0.2]\\n        },\\n        \"subsample\": {\\n            \"values\": [1, 0.5, 0.3]\\n        }\\n    }\\n}\\n2. Initialize the Sweep\\nCalling `wandb.sweep` starts a Sweep Controller --\\na centralized process that provides settings of the `parameters` to any who query it\\nand expects them to return performance on `metrics` via `wandb` logging.\\nsweep_id = wandb.sweep(sweep_config, project=\"XGBoost-sweeps\")', 'source': 'tutorials/xgboost_sweeps.md', 'score': 0.30404168, 'relevance': 2}, {'content': 'parameters\\nIn your YAML file or Python script, specify `parameters` as a top level key. Within the `parameters` key, provide the name of a hyperparameter you want to optimize. Common hyperparameters include: learning rate, batch size, epochs, optimizers, and more. For each hyperparameter you define in your sweep configuration, specify one or more search constraints. \\nThe proceeding table shows supported hyperparameter search constraints. Based on your hyperparameter and use case, use one of the search constraints below to tell your sweep agent where (in the case of a distribution) or what (`value`, `values`, and so forth) to search or use.\\nSearch constraint\\nDescription\\n`values`\\nSpecifies all valid values for this hyperparameter. Compatible with `grid`.\\n`value`\\nSpecifies the single valid value for this hyperparameter. Compatible with `grid`.\\n`distribution`\\nSpecify a probability distribution. See the note following this table for information on default values.\\n`probabilities`\\nSpecify the probability of selecting each element of `values` when using `random`.\\n`min`, `max`\\n(`int`or `float`) Maximum and minimum values. If `int`, for `int_uniform` -distributed hyperparameters. If `float`, for `uniform` -distributed hyperparameters.\\n`mu`\\n(`float`) Mean parameter for `normal` - or `lognormal` -distributed hyperparameters.\\n`sigma`\\n(`float`) Standard deviation parameter for `normal` - or `lognormal` -distributed hyperparameters.\\n`q`\\n(`float`) Quantization step size for quantized hyperparameters.\\n`parameters`\\nNest other parameters inside a root level parameter.\\n:::info\\nW&B sets the following distributions based on the following conditions if a distribution is not specified:\\n* `categorical` if you specify `values`\\n* `int_uniform` if you specify `max` and `min` as integers\\n* `uniform` if you specify `max` and `min` as floats\\n* `constant` if you provide a set to `value`\\n:::\\nmethod\\nSpecify the hyperparameter search strategy with the `method` key. There are three hyperparameter search strategies to choose from: grid, random, and Bayesian search.', 'source': 'guides/sweeps/sweep-config-keys.md', 'score': 0.17652959999999998, 'relevance': 1}, {'content': \"📃 Name the hyperparameters\\nOnce you've picked a `method` to try out new values of the hyperparameters,\\nyou need to define what those `parameters` are.\\nMost of the time, this step is straightforward:\\nyou just give the `parameter` a name\\nand specify a list of legal `values`\\nof the parameter.\\nFor example, when we choose the `optimizer` for our network,\\nthere's only a finite number of options.\\nHere we stick with the two most popular choices, `adam` and `sgd`.\\nEven for hyperparameters that have potentially infinite options,\\nit usually only makes sense to try out\\na few select `values`,\\nas we do here with the hidden `layer_size` and `dropout`.\\nparameters_dict = {\\n    'optimizer': {\\n        'values': ['adam', 'sgd']\\n        },\\n    'fc_layer_size': {\\n        'values': [128, 256, 512]\\n        },\\n    'dropout': {\\n          'values': [0.3, 0.4, 0.5]\\n        },\\n    }\\nsweep_config['parameters'] = parameters_dict\\nIt's often the case that there are hyperparameters\\nthat we don't want to vary in this Sweep,\\nbut which we still want to set in our `sweep_config`.\\nIn that case, we just set the `value` directly:\\nparameters_dict.update({\\n    'epochs': {\\n        'value': 1}\\n    })\\nFor a `grid` search, that's all you ever need.\\nFor a `random` search,\\nall the `values` of a parameter are equally likely to be chosen on a given run.\\nIf that just won't do,\\nyou can instead specify a named `distribution`,\\nplus its parameters, like the mean `mu`\\nand standard deviation `sigma` of a `normal` distribution.\\nSee more on how to set the distributions of your random variables here.\", 'source': 'tutorials/sweeps.md', 'score': 0.002396731, 'relevance': 1}, {'content': 'Tune Hyperparameters\\nTune Hyperparameters with Sweeps\\nUse W&B Sweeps to automate hyperparameter search and visualize rich, interactive experiment tracking. Pick from popular search methods such as Bayesian, grid search, and random to search the hyperparameter space. Scale and parallelize sweep across one or more machines.\\nHow it works\\nCreate a sweep with two W&B CLI commands:\\nInitialize a sweep\\nwandb sweep --project <propject-name> <path-to-config file>\\nStart the sweep agent\\nwandb agent <sweep-ID>\\n:::tip\\nThe preceding code snippet, and the colab linked on this page, show how to initialize and create a sweep with wht W&B CLI. See the Sweeps Walkthrough for a step-by-step outline of the W&B Python SDK commands to use to define a sweep configuration, initialize a sweep, and start a sweep.\\n:::\\nHow to get started\\nDepending on your use case, explore the following resources to get started with W&B Sweeps:\\nIf this is your first time using W&B Sweeps, we recommend you go through the Sweeps Colab notebook.\\nRead through the sweeps walkthrough for a step-by-step outline of the W&B Python SDK commands to use to define a sweep configuration, initialize a sweep, and start a sweep.\\nExplore this chapter to learn how to:\\nAdd W&B to your code\\nDefine sweep configuration\\nInitialize sweeps\\nStart sweep agents\\nVisualize sweep results\\nExplore a curated list of Sweep experiments that explore hyperparameter optimization with W&B Sweeps. Results are stored in W&B Reports.\\nFor a step-by-step video, see: Tune Hyperparameters Easily with W&B Sweeps.', 'source': 'guides/sweeps/intro.md', 'score': 0.0003487043, 'relevance': 1}, {'content': \"Check out more on Sweep Configs $rightarrow$\\nsweep_config = {\\n  'method': 'random', \\n  'metric': {\\n      'name': 'val_loss',\\n      'goal': 'minimize'\\n  },\\n  'early_terminate':{\\n      'type': 'hyperband',\\n      'min_iter': 5\\n  },\\n  'parameters': {\\n      'batch_size': {\\n          'values': [32, 64, 128, 256]\\n      },\\n      'learning_rate':{\\n          'values': [0.01, 0.005, 0.001, 0.0005, 0.0001]\\n      }\\n  }\\n}\", 'source': 'tutorials/tensorflow_sweeps.md', 'score': 0.0002305067, 'relevance': 0}, {'content': \"Sweeps on Launch\\nCreate a hyperparameter tuning job (sweeps) with W&B Launch. With sweeps on launch, a sweep scheduler is pushed to a Launch Queue with the specified hyperparameters to sweep over. The sweep scheduler starts as it is picked up by the agent, launching sweep runs onto the same queue with chosen hyperparameters. This continues until the sweep finishes or is stopped. \\nYou can use the default W&B Sweep scheduling engine or implement your own custom scheduler:\\nStandard sweep scheduler: Use the default W&B Sweep scheduling engine that controls W&B Sweeps. The familiar `bayes`, `grid`, and `random` methods are available.\\nCustom sweep scheduler: Configure the sweep scheduler to run as a job. This option enables full customization. An example of how to extend the standard sweep scheduler to include more logging can be found in the section below.\\n:::note\\nThis guide assumes that W&B Launch has been previously configured. If W&B Launch has is not configured, see the how to get started section of the launch documentation. \\n:::\\n:::tip\\nWe recommend you create a sweep on launch using the 'basic' method if you are a first time users of sweeps on launch. Use a custom sweeps on launch scheduler when the standard W&B scheduling engine does not meet your needs.\\n:::\", 'source': 'guides/launch/sweeps-on-launch.md', 'score': 0.00013342, 'relevance': 0}, {'content': \"3. Run the Sweep with an agent\\nNow, we call `wandb.agent` to start up our sweep.\\nYou can call `wandb.agent` on any machine where you're logged into W&B that has\\n- the `sweep_id`,\\n- the dataset and `train` function\\nand that machine will join the sweep!\\nNote: a `random` sweep will by defauly run forever,\\ntrying new parameter combinations until the cows come home --\\nor until you turn the sweep off from the app UI.\\nYou can prevent this by providing the total `count` of runs you'd like the `agent` to complete.\\nwandb.agent(sweep_id, train, count=25)\\nVisualize your results\\nNow that your sweep is finished, it's time to look at the results.\\nWeights & Biases will generate a number of useful plots for you automatically.\\nParallel coordinates plot\\nThis plot maps hyperparameter values to model metrics. It’s useful for honing in on combinations of hyperparameters that led to the best model performance.\\nThis plot seems to indicate that using a tree as our learner slightly,\\nbut not mind-blowingly,\\noutperforms using a simple linear model as our learner.\\nHyperparameter importance plot\\nThe hyperparameter importance plot shows which hyperparameter values had the biggest impact\\non your metrics.\\nWe report both the correlation (treating it as a linear predictor)\\nand the feature importance (after training a random forest on your results)\\nso you can see which parameters had the biggest effect\\nand whether that effect was positive or negative.\\nReading this chart, we see quantitative confirmation \\nof the trend we noticed in the parallel coordinates chart above:\\nthe largest impact on validation accuracy came from the choice of\\nlearner, and the `gblinear` learners were generally worse than `gbtree` learners.\\nThese visualizations can help you save both time and resources running expensive hyperparameter optimizations by honing in on the parameters (and value ranges) that are the most important, and thereby worthy of further exploration.\", 'source': 'tutorials/xgboost_sweeps.md', 'score': 0.00011323440000000001, 'relevance': 0}, {'content': \"👀 Visualize Sweep Results\\n🔀 Parallel Coordinates Plot\\nThis plot maps hyperparameter values to model metrics. It’s useful for honing in on combinations of hyperparameters that led to the best model performance.\\n📊 Hyperparameter Importance Plot\\nThe hyperparameter importance plot surfaces which hyperparameters were the best predictors of your metrics.\\nWe report feature importance (from a random forest model) and correlation (implicitly a linear model).\\nThese visualizations can help you save both time and resources running expensive hyperparameter optimizations by honing in on the parameters (and value ranges) that are the most important, and thereby worthy of further exploration.\\n🧤 Get your hands dirty with sweeps\\nWe created a simple training script and a few flavors of sweep configs for you to play with. We highly encourage you to give these a try.\\nThat repo also has examples to help you try more advanced sweep features like Bayesian Hyperband, and Hyperopt.\\nWhat's next?\\nIn the next tutorial, you will learn how to manage model weights & dataset versions using W&B Artifacts:\\n👉 Manage Model Weights & Dataset Versions\", 'source': 'tutorials/sweeps.md', 'score': 5.6058300000000005e-05, 'relevance': 0}])"
        ],
        [
         "12",
         "is there a graphical interface to set parameters to run a job?",
         "Yes, Weights & Biases (W&B) provides a graphical interface that allows you to set parameters for running a job. This can be done through the W&B UI, specifically using the Launch feature.\n\n### How to Use the Graphical Interface to Set Parameters for a Job:\n\n1. **Navigate to Your Project**: Open your W&B project where you want to run the job.\n\n2. **Access the Jobs Section**: Click on the **Jobs** icon on the left panel of your project page. This section lists all the W&B launch jobs created from previously executed W&B runs.\n\n3. **Configure a Job**: Select the **Launch** button next to the job you want to configure. A modal will appear on the right side of the page.\n\n4. **Set Job Parameters**:\n   - **Job Version**: From the dropdown, select the version of the launch job you want to use. Launch jobs are versioned like any other W&B Artifact.\n   - **Overrides**: Here, you can provide new values for any inputs configured for your launch job. Common overrides include changing the entrypoint command, arguments, or values in the `wandb.config` of your new run.\n\n5. **Select Queue**: Choose the queue where the job should be launched. If you haven't created a queue yet, you will have the option to create a new one, like a **Starter Queue**.\n\n6. **Launch the Job**: After setting all the parameters, click the **Launch now** button to start the job on the selected queue.\n\nThis graphical interface makes it easy to modify job parameters without directly editing code or configuration files, providing a user-friendly way to manage and execute jobs.\n\nFor more detailed information and step-by-step guides on using the Launch feature, you can visit the [W&B Launch documentation](https://docs.wandb.ai/guides/launch).",
         "WeaveList([{'content': 'Manage job inputs\\nThe core experience of Launch is easily experimenting with different job inputs like hyperparameters and datasets, and routing these jobs to appropriate hardware. Once a job is created, users beyond the original author can adjust these inputs via the W&B GUI or CLI. For information on how job inputs can be set when launching from the CLI or UI, see the Enqueue jobs guide.\\nThis section describes how to programmatically control the inputs that can be tweaked for a job.\\nBy default, W&B jobs capture the entire `Run.config` as the inputs to a job, but the Launch SDK provides a function to control select keys in the run config or to specify JSON or YAML files as inputs.\\n:::info\\nLaunch SDK functions require `wandb-core`. See the wandb-core README for more information.\\n:::', 'source': 'guides/launch/job-inputs.md', 'score': 0.93891287, 'relevance': 2}, {'content': 'Add jobs to your queue\\nAdd jobs to your queue interactively with the W&B App or programmatically with the W&B CLI.\\nAdd a job to your queue programmatically with the W&B App.\\nNavigate to your W&B Project Page.\\nSelect the Jobs icon on the left panel:\\n  \\nThe Jobs page displays a list of W&B launch jobs that were created from previously executed W&B runs. \\n  \\nSelect the Launch button next to the name of the Job name. A modal will appear on the right side of the page.\\nFrom the Job version dropdown, select the version of hte launch job you want to use. Launch jobs are versioned like any other W&B Artifact. Different versions of the same launch job will be created if you make modifications to the software dependencies or source code used to run the job.\\nWithin the Overrides section, provide new values for any inputs that are configured for your launch job. Common overrides include a new entrypoint command, arguments, or values in the `wandb.config` of your new W&B run.\\n  You can copy and paste values from other W&B runs that used your launch job by clicking on the Paste from... button.\\nFrom the Queue dropdown, select the name of the launch queue you want to add your launch job to. \\nUse the Job Priority dropdown to specify the priority of your launch job.  A launch job\\'s priority is set to \"Medium\" if the launch queue does not support prioritization.\\n(Optional) Follow this step only if a queue config template was created by your team admin\\nWithin the Queue Configurations field, provide values for configuration options that were created by the admin of your team.\\nFor example, in the following example, the team admin configured AWS instance types that can be used by the team. In this case, team members can pick either the `ml.m4.xlarge` or `ml.p3.xlarge` compute instance type to train their model.\\nSelect the Destination project, where the resulting run will appear.  This project needs to belong to the same entity as the queue.\\nSelect the Launch now button. \\nUse the `wandb launch` command to add jobs to a queue. Create a JSON configuration with hyperparameter overrides. For example, using the script from the Quickstart guide, we create a JSON file with the following overrides:\\nconfig.json', 'source': 'guides/launch/add-job-to-queue.md', 'score': 0.05791808700000001, 'relevance': 2}, {'content': '{\\n    \"trainer\": {\\n        \"learning_rate\": \"float\",\\n        \"batch_size\": \"int\",\\n        \"model\": \"str\",\\n        \"dataset\": \"str\",\\n    },\\n}\\nWhen launching the job from the W&B CLI or UI, the user will be able to override only the four `trainer` parameters.', 'source': 'guides/launch/job-inputs.md', 'score': 0.19528697, 'relevance': 1}, {'content': \"Launch FAQs\\nFrequently Asked Questions About Launch\\nGetting Started\\nI do not want W&B to build a container for me, can I still use Launch?\\nYes. Run the following to launch a pre-built docker image. Replace the items in the `<>` with your information:\\nwandb launch -d <docker-image-uri> -q <queue-name> -E <entrypoint>\\nThis will build a job when you create a run.\\nOr you can make a job from an image:\\nwandb job create image <image-name> -p <project> -e <entity>\\nAre there best practices for using Launch effectively?\\nCreate your queue before you start your agent, so that you can set your agent to point to it easily.  If you don’t do this, your agent will give errors and not work until you add a queue.\\nCreate a W&B service account to start up the agent, so that it's not tied to an individual user account.\\nUse `wandb.config` to read and write your hyperparameters, so that they can be overwritten when re-running a job. Check out this guide if you use argsparse.\\nI do not like clicking- can I use Launch without going through the UI?\\nYes. The standard `wandb` CLI includes a `launch` subcommand that you can use to launch your jobs. For more info, try running\\nwandb launch --help\\nCan Launch automatically provision (and spin down) compute resources for me in the target environment?\\nThis depends on the environment, we are able to provision resources in SageMaker, and Vertex. In Kubernetes, autoscalers can be used to automatically spin up and spin down resources when required. The Solution Architects at W&B are happy to work with you to configure your underlying Kubernetes infrastructure to facilitate retries, autoscaling, and use of spot instance node pools. Reach out to\\xa0support@wandb.com\\xa0or your shared Slack channel.\", 'source': 'guides/launch/launch-faqs.md', 'score': 0.029703751, 'relevance': 1}, {'content': 'For more information about queue configurations, see the Set up Launch on Kubernetes and the Advanced queue setup guide.   \\nThe `${image_uri}` and `{{gpus}}` strings are examples of the two kinds of\\nvariable templates that you can use in your queue configuration. The `${image_uri}`\\ntemplate will be replaced with the image URI of the job you are launching by the\\nagent. The `{{gpus}}` template will be used to create a template variable that\\nyou can override from the launch UI, CLI, or SDK when submitting a job. These values\\nare placed in the job specification so that they will modify the correct fields\\nto control the image and GPU resources used by the job.\\nClick the Parse configuration button to begin customizing your `gpus` template\\nvariable. \\nSet the Type to `Integer` and the Default, Min, and Max to values of your choosing.\\nAttempts to submit a run to this queue which violate the constraints of the template variable will\\nbe rejected.\\nClick Create queue to create your queue. You will be redirected to the queue page for your new queue.\\nIn the next section, we will set up an agent that can pull and execute jobs from the queue you created.', 'source': 'tutorials/minikube_gpu.md', 'score': 0.0068771075, 'relevance': 1}, {'content': 'Advanced queue set up\\nThe following page describes how to configure additional launch queue options.\\nSet up queue config templates\\nAdminister and manage guardrails on compute consumption with Queue Config Templates. Set defaults, minimums, and maximum values for fields such as memory consumption, GPU, and runtime duration.\\nAfter you configure a queue with config templates, members of your team can alter fields you defined only within the specified range you defined.', 'source': 'guides/launch/setup-queue-advanced.md', 'score': 0.005533388, 'relevance': 1}, {'content': '{\\n  \"overrides\": {\\n      \"args\": [],\\n      \"run_config\": {\\n          \"learning_rate\": 0,\\n          \"epochs\": 0\\n      },   \\n      \"entry_point\": []\\n  }\\n}\\n:::note\\nW&B Launch will use the default parameters if you do not provide a JSON configuration file.\\n:::\\nIf you want to override the queue configuration, or if your launch queue does not have a configuration resource defined, you can specify the `resource_args` key in your config.json file. For example, following continuing the example above, your config.json file might look similar to the following:\\nconfig.json\\n{\\n  \"overrides\": {\\n      \"args\": [],\\n      \"run_config\": {\\n          \"learning_rate\": 0,\\n          \"epochs\": 0\\n      },\\n      \"entry_point\": []\\n  },\\n  \"resource_args\": {\\n        \"<resource-type>\" : {\\n            \"<key>\": \"<value>\"\\n        }\\n  }\\n}\\nReplace values within the `<>` with your own values.\\nProvide the name of the queue for the `queue`(`-q`) flag, the name of the job for the `job`(`-j`) flag, and the path to the configuration file for the `config`(`-c`) flag.\\nwandb launch -j <job> -q <queue-name> \\\\ \\n-e <entity-name> -c path/to/config.json\\nIf you work within a W&B Team, we suggest you specify the `entity` flag (`-e`) to indicate which entity the queue will use.', 'source': 'guides/launch/add-job-to-queue.md', 'score': 0.0050407033, 'relevance': 1}, {'content': '{\\n  \"kind\": \"Job\",\\n  \"spec\": {\\n    \"tasks\": [\\n      {\\n        \"name\": \"master\",\\n        \"policies\": [\\n          {\\n            \"event\": \"TaskCompleted\",\\n            \"action\": \"CompleteJob\"\\n          }\\n        ],\\n        \"replicas\": 1,\\n        \"template\": {\\n          \"spec\": {\\n            \"containers\": [\\n              {\\n                \"name\": \"master\",\\n                \"image\": \"${image_uri}\",\\n                \"imagePullPolicy\": \"IfNotPresent\"\\n              }\\n            ],\\n            \"restartPolicy\": \"OnFailure\"\\n          }\\n        }\\n      },\\n      {\\n        \"name\": \"worker\",\\n        \"replicas\": 1,\\n        \"template\": {\\n          \"spec\": {\\n            \"containers\": [\\n              {\\n                \"name\": \"worker\",\\n                \"image\": \"${image_uri}\",\\n                \"workingDir\": \"/home\",\\n                \"imagePullPolicy\": \"IfNotPresent\"\\n              }\\n            ],\\n            \"restartPolicy\": \"OnFailure\"\\n          }\\n        }\\n      }\\n    ],\\n    \"plugins\": {\\n      \"pytorch\": [\\n        \"--master=master\",\\n        \"--worker=worker\",\\n        \"--port=23456\"\\n      ]\\n    },\\n    \"minAvailable\": 1,\\n    \"schedulerName\": \"volcano\"\\n  },\\n  \"metadata\": {\\n    \"name\": \"wandb-job-${run_id}\",\\n    \"labels\": {\\n      \"wandb_entity\": \"${entity_name}\",\\n      \"wandb_project\": \"${project_name}\"\\n    },\\n    \"namespace\": \"wandb\"\\n  },\\n  \"apiVersion\": \"batch.volcano.sh/v1alpha1\"\\n}\\nClick the Create queue button at the bottom of the drawer to finish creating your queue.', 'source': 'tutorials/volcano.md', 'score': 0.0038844766, 'relevance': 0}, {'content': 'Launch a job\\nLet\\'s send a job to our agent. You can launch a simple \"hello world\" from a terminal logged into your W&B account with:\\nwandb launch -d wandb/job_hello_world:main -p <target-wandb-project> -q <your-queue-name> -e <your-queue-entity>\\nYou can test with any job or image you like, but make sure your cluster can pull your image.  See Minikube’s documentation for additional guidance. You can also test using one of our public jobs.', 'source': 'tutorials/minikube_gpu.md', 'score': 0.0031726828000000003, 'relevance': 0}, {'content': 'Quickstart\\nInstall W&B and start tracking your machine learning experiments in minutes.\\n1. Create an account and install W&B\\nBefore you get started, make sure you create an account and install W&B:\\nSign up for a free account at https://wandb.ai/site and then log in to your wandb account.  \\nInstall the wandb library on your machine in a Python 3 environment using pip.  \\nThe following code snippets demonstrate how to install and log into W&B using the W&B CLI and Python Library:\\nInstall the CLI and Python library for interacting with the Weights and Biases API:\\npip install wandb\\nInstall the CLI and Python library for interacting with the Weights and Biases API:\\n!pip install wandb\\n2. Log in to W&B\\nNext, log in to W&B:\\nwandb login\\nOr if you are using W&B Server (including Dedicated Cloud or Self-managed):\\nwandb login --relogin --host=http://your-shared-local-host.com\\nIf needed, ask your deployment admin for the hostname.\\nProvide your API key when prompted.\\nNext, import the W&B Python SDK and log in:\\nwandb.login()\\nProvide your API key when prompted.\\n  \\n3. Start a  run and track hyperparameters\\nInitialize a W&B Run object in your Python script or notebook with wandb.init() and pass a dictionary to the `config` parameter with key-value pairs of hyperparameter names and values:\\nrun = wandb.init(\\n    # Set the project where this run will be logged\\n    project=\"my-awesome-project\",\\n    # Track hyperparameters and run metadata\\n    config={\\n        \"learning_rate\": 0.01,\\n        \"epochs\": 10,\\n    },\\n)\\nA run is the basic building block of W&B. You will use them often to track metrics, create logs, create jobs, and more.', 'source': 'quickstart.md', 'score': 0.0030634168, 'relevance': 0}])"
        ],
        [
         "13",
         "can you help me setup LLM evaluation on my local machine?",
         "To set up LLM evaluation on your local machine using Weights & Biases, you'll need to follow several steps to ensure everything is configured correctly. Here’s a detailed guide to help you through the process:\n\n### System Requirements\n1. **Python**: Ensure Python is installed on your machine. Python 3.6 or later is generally recommended.\n2. **Weights & Biases**: Install the `wandb` library if it's not already installed. You can install it using pip:\n   ```bash\n   pip install wandb\n   ```\n3. **Docker**: For containerized environments, Docker is required. Install Docker from [Docker's official website](https://www.docker.com/get-started).\n4. **API Keys**: If your evaluation involves models like GPT-3, ensure you have the necessary API keys set as environment variables.\n\n### Weights & Biases Features Needed\n- **W&B Artifacts**: To manage and version your models and datasets.\n- **W&B Tables**: For logging and visualizing evaluation results.\n- **W&B Launch**: To automate and scale your evaluation jobs using Docker containers.\n\n### Configuration Steps\n1. **Initialize W&B**:\n   Start by initializing a W&B run in your evaluation script. This will track all your logs, metrics, and outputs.\n   ```python\n   import wandb\n   wandb.init(project='your_project_name', entity='your_wandb_username')\n   ```\n\n2. **Prepare Your Model**:\n   Load your LLM model. If you're using a model from Hugging Face's Transformers, you can load it like this:\n   ```python\n   from transformers import AutoModelForCausalLM, AutoTokenizer\n   tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n   model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n   ```\n\n3. **Set Up Docker**:\n   If using Docker, build your Docker image as per your `Dockerfile`:\n   ```bash\n   docker build -f Dockerfile.eval -t evaluate:v0 .\n   ```\n\n4. **Create a Launch Job**:\n   Use W&B CLI to create a launch job:\n   ```bash\n   wandb job create --project your_project_name --entity your_wandb_username --name eval_job --image evaluate:v0\n   ```\n\n5. **Run Evaluation**:\n   Implement your evaluation logic in the script. Log results to W&B using:\n   ```python\n   wandb.log({\"metric_name\": value})\n   ```\n\n### Troubleshooting Steps\n- **Check Docker Logs**: If using Docker, check the logs for any errors during the container execution.\n- **API Key Configuration**: Ensure that all required API keys are correctly configured in your environment variables.\n- **Dependency Issues**: Ensure all dependencies are correctly installed. Use a virtual environment to manage dependencies without conflicts.\n- **Network Issues**: Ensure your machine has proper internet access, especially if your LLM requires API calls to external services.\n\nBy following these steps, you should be able to set up LLM evaluation on your local machine using Weights & Biases effectively. If you encounter issues, check the logs and configurations as mentioned in the troubleshooting steps. For more detailed assistance or specific errors, you can reach out to the Weights & Biases [community forums](https://wandb.me/community) or [support](support@wandb.com).",
         "WeaveList([{'content': 'Iterate on LLMs\\nTry in a Colab Notebook here →\\nWeights & Biases Prompts is a suite of LLMOps tools built for the development of LLM-powered applications. \\nUse W&B Prompts to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.\\nInstallation\\n!pip install \"wandb==0.15.2\" -qqq\\n!pip install \"langchain==v0.0.158\" openai -qqq\\nSetup\\nThis demo requires that you have an OpenAI key\\nimport os\\nfrom getpass import getpass\\nif os.getenv(\"OPENAI_API_KEY\") is None:\\n  os.environ[\"OPENAI_API_KEY\"] = getpass(\"Paste your OpenAI key from: https://platform.openai.com/account/api-keys\\\\n\")\\nassert os.getenv(\"OPENAI_API_KEY\", \"\").startswith(\"sk-\"), \"This doesn\\'t look like a valid OpenAI API key\"\\nprint(\"OpenAI API key configured\")\\nW&B Prompts\\nW&B currently supports a tool called Trace. Trace consists of three main components:\\nTrace table: Overview of the inputs and outputs of a chain.\\nTrace timeline: Displays the execution flow of the chain and is color-coded according to component types.\\nModel architecture: View details about the structure of the chain and the parameters used to initialize each component of the chain.\\nAfter running this section, you will see a new panel automatically created in your workspace, showing each execution, the trace, and the model architecture\\nImport `WandbTracer`and optionally define a dictionary with arguments for `wandb.init()` that will later be passed to `WandbTracer`. This includes a project name, team name, entity, and more. For more information about wandb.init, see the API Reference Guide.\\nfrom wandb.integration.langchain import WandbTracer\\nwandb_config = {\"project\": \"wandb_prompts_quickstart\"}', 'source': 'tutorials/prompts.md', 'score': 0.6708239300000001, 'relevance': 2}, {'content': \"Traces\\nW&B’s LLM tool is called\\xa0Traces.\\xa0Traces\\xa0allow you to track and visualize the inputs and outputs, execution flow, model architecture, and any intermediate results of your LLM chains.\\nUse Traces for LLM chaining, plug-in or pipelining use cases. You can use your own LLM chaining implementation or use a W&B integration provided by LLM libraries such as LangChain.\\nTraces consists of three main components:\\nTrace table: Overview of the inputs and outputs of a chain.\\nTrace timeline: Displays the execution flow of the chain and is color-coded according to component types.\\nModel architecture: View details about the structure of the chain and the parameters used to initialize each component of the chain.\\nTrace Table\\nThe Trace Table provides an overview of the inputs and outputs of a chain. The trace table also provides information about the composition of a trace event in the chain, whether or not the chain ran successfully, and any error messages returned when running the chain.\\nClick on a row number on the left hand side of the Table to view the\\xa0Trace Timeline\\xa0for that instance of the chain.\\nTrace Timeline\\nThe Trace Timeline view displays the execution flow of the chain and is color-coded according to component types. Select a trace event to display the inputs, outputs, and metadata of that trace.\\nTrace events that raise an error are outlined in red. Click on a trace event colored in red to view the returned error message.\\nModel Architecture\\nThe Model Architecture view provides details about the structure of the chain and the parameters used to initialize each component of the chain. Click on a trace event to learn more details about that event.\\nEvaluation \\nTo iterate on an application, we need a way to evaluate if it's improving. To do so, a common practice is to test it against the same dataset when there is a change. See this tutorial to learn how to evaluate LLM applications using W&B.\\nTutorial: Evaluate LLM application performance\\nIntegrations\\nWeights and Biases also has lightweight integrations for:\\nLangChain\\nOpenAI API\\nOpenAI GPT-3.5 Fine-Tuning\\nHugging Face Transformers\\nGetting Started\\nWe recommend you go through the Prompts\\xa0Quickstart\\xa0guide, which will walk you through logging a custom LLM pipeline with Trace. A colab version of the guide is also available.\", 'source': 'guides/prompts/intro.md', 'score': 0.31195062, 'relevance': 2}, {'content': 'LLMs\\nPrompts\\nEvaluating the performance of Large Language Models (LLMs) can be difficult. Use W&B Prompts and LLM Monitoring to streamline the evaluation process, providing a visual way to analyze your generative models.\\nVisualize\\nW&B Prompts is a suite of LLMOps tools built for the development of LLM-powered applications. Use W&B Prompts to visualize and inspect the execution flow of your LLMs, analyze the inputs and outputs of your LLMs, view the intermediate results and securely store and manage your prompts and LLM chain configurations.\\nW&B Prompts provides several solutions for building and monitoring LLM-based apps. Software developers, prompt engineers, ML practitioners, data scientists, and other stakeholders working with LLMs need cutting-edge tools to:\\nExplore and debug LLM chains\\xa0and prompts with greater granularity.\\nMonitor and observe LLMs to better understand and evaluate performance, usage, and budgets.\\nIntegrations\\nW&B also has lightweight integrations for:\\nLangChain\\nOpenAI API\\nOpenAI GPT-3.5 Fine-Tuning\\nHugging Face Transformers\\nNext Steps\\nCheck out more detailed documentation on Prompts.', 'source': 'guides/prompts_platform.md', 'score': 0.693473, 'relevance': 1}, {'content': 'Next Steps\\nYou can use existing W&B features like Tables and Runs to track LLM application performance. See this tutorial to learn more:\\nTutorial: Evaluate LLM application performance', 'source': 'guides/prompts/quickstart.md', 'score': 0.587174, 'relevance': 1}, {'content': \"This evaluation builds on the example in the earlier section. Evaluating using `weave.Evaluation` requires an evaluation dataset, a scorer function and a `weave.Model`. Here are a few nuances about the three key components:\\nMake sure that the keys of the evaluation sample dicts matches the arguments of the scorer function and of the `weave.Model`'s `predict` method.\\nThe `weave.Model` should have a method with the name `predict` or `infer` or `forward`. Decorate this method with `weave.op()` for tracing.\\nThe scorer function should be decorated with `weave.op()` and should have `model_output` as named argument.\\nBy integrating Weave with LlamaIndex, you can ensure comprehensive logging and monitoring of your LLM applications, facilitating easier debugging and performance optimization using evaluation.\", 'source': 'weave/guides/integrations/llamaindex.md', 'score': 0.3760501, 'relevance': 1}, {'content': 'Next Steps\\nCheck out more detailed documentation on Trace, or our OpenAI Integration.\\nTry one of our demo colabs, which offer more detailed explanations of how to use Prompts for LLMOps.\\nYou can use existing W&B features like Tables and Runs to track LLM application performance. See this tutorial to learn more:\\nTutorial: Evaluate LLM application performance', 'source': 'guides/prompts/intro.md', 'score': 0.29767406, 'relevance': 1}, {'content': \"3. Evaluating with an LLM Judge\\nWhen there aren't simple ways to evaluate your application, one approach is to use an LLM to evaluate aspects of it. Here is an example of using an LLM judge to try to measure the context precision by prompting it to verify if the context was useful in arriving at the given answer. This prompt was augmented from the popular RAGAS framework.\", 'source': 'weave/tutorial-rag.md', 'score': 0.17314835, 'relevance': 0}, {'content': \"Tutorial: Model-Based Evaluation of RAG applications\\nRetrieval Augmented Generation (RAG) is a common way of building Generative AI applications that have access to custom knowledge bases. \\nIn this example, we'll show an example that has a retrieval step to get documents. By tracking this, you can debug your app and see what documents were pulled into the LLM context.\\nWe'll also show how to evaluate it using an LLM judge.\", 'source': 'weave/tutorial-rag.md', 'score': 0.15304215000000002, 'relevance': 0}, {'content': 'Doing Evaluation with weave.Evaluation\\nEvaluations help you measure the performance of your applications. By using the weave.Evaluation class, you can capture how well your model performs on specific tasks or datasets, making it easier to compare different models and iterations of your application. The following example demonstrates how to evaluate the model we created:\\nimport asyncio\\nfrom llama_index.core.evaluation import CorrectnessEvaluator\\neval_examples = [\\n    {\\n        \"id\": \"0\",\\n        \"query\": \"What programming language did Paul Graham learn to teach himself AI when he was in college?\",\\n        \"ground_truth\": \"Paul Graham learned Lisp to teach himself AI when he was in college.\",\\n    },\\n    {\\n        \"id\": \"1\",\\n        \"query\": \"What was the name of the startup Paul Graham co-founded that was eventually acquired by Yahoo?\",\\n        \"ground_truth\": \"The startup Paul Graham co-founded that was eventually acquired by Yahoo was called Viaweb.\",\\n    },\\n    {\\n        \"id\": \"2\",\\n        \"query\": \"What is the capital city of France?\",\\n        \"ground_truth\": \"I cannot answer this question because no information was provided in the text.\",\\n    },\\n]\\nllm_judge = OpenAI(model=\"gpt-4\", temperature=0.0)\\nevaluator = CorrectnessEvaluator(llm=llm_judge)\\n# highlight-next-line\\n@weave.op()\\ndef correctness_evaluator(query: str, ground_truth: str, model_output: dict):\\n    result = evaluator.evaluate(\\n        query=query, reference=ground_truth, response=model_output[\"response\"]\\n    )\\n    return {\"correctness\": float(result.score)}\\n# highlight-next-line\\nevaluation = weave.Evaluation(dataset=eval_examples, scorers=[correctness_evaluator])\\nrag_pipeline = SimpleRAGPipeline()\\n# highlight-next-line\\nasyncio.run(evaluation.evaluate(rag_pipeline))', 'source': 'weave/guides/integrations/llamaindex.md', 'score': 0.05834584, 'relevance': 0}, {'content': \"What's next?\\nFollow the Model-Based Evaluation of RAG applications to evaluate a RAG app using an LLM judge.\", 'source': 'weave/tutorial-eval.md', 'score': 0.0072890776, 'relevance': 0}])"
        ],
        [
         "14",
         "How can I set a custom metric to group runs?",
         "To set a custom metric and group runs based on it in Weights & Biases, you can follow these steps:\n\n### Step 1: Define Your Custom Metric\nFirst, you need to define the custom metric that you want to use for grouping. This can be done using the `wandb.define_metric()` function. This function allows you to specify properties of the metric that will later be logged.\n\n```python\nimport wandb\n\n# Start a new run\nwandb.init(project=\"your_project_name\")\n\n# Define your custom metric\nwandb.define_metric(\"custom_metric_name\")\n```\n\n### Step 2: Log Your Custom Metric\nNext, log values for your custom metric during your experiment. This is done using the `wandb.log()` function.\n\n```python\n# Log values to your custom metric\nwandb.log({\"custom_metric_name\": value})\n```\n\n### Step 3: Group Runs Based on the Custom Metric\nTo group runs based on your custom metric, you can use the grouping feature in the Weights & Biases UI. This allows you to visually organize and compare runs based on the custom metric.\n\n1. Go to your project page on the Weights & Biases website.\n2. Click on the \"Group\" option.\n3. Select your custom metric from the list of available fields to group by.\n\nThis setup will allow you to organize and analyze your runs based on the custom metric you've defined and logged.\n\nFor more detailed information on defining and using custom metrics, you can refer to the Weights & Biases documentation on [defining metrics](https://docs.wandb.ai/guides/track/log/plots) and [grouping runs](https://docs.wandb.ai/guides/runs/grouping).",
         "WeaveList([{'content': 'Group Runs\\nGroup W&B Runs\\nGroup individual jobs into experiments by passing a unique group name to wandb.init().\\nUse Cases\\nDistributed training: Use grouping if your experiments are split up into different pieces with separate training and evaluation scripts that should be viewed as parts of a larger whole.\\nMultiple processes: Group multiple smaller processes together into an experiment.\\nK-fold cross-validation: Group together runs with different random seeds to see a larger experiment. Here\\'s an example of k-fold cross-validation with sweeps and grouping.\\nThere are three ways to set grouping:\\n1. Set group in your script\\nPass an optional group and job_type to wandb.init(). This gives you a dedicated group page for each experiment, which contains the individual runs. For example:`wandb.init(group=\"experiment_1\", job_type=\"eval\")`\\n2. Set a group environment variable\\nUse `WANDB_RUN_GROUP` to specify a group for your runs as an environment variable. For more on this, check our docs for Environment Variables. Group should be unique within your project and shared by all runs in the group. You can use `wandb.util.generate_id()` to generate a unique 8 character string to use in all your processes— for example, `os.environ[\"WANDB_RUN_GROUP\"] = \"experiment-\" + wandb.util.generate_id()`\\n3. Toggle grouping in the UI\\nYou can dynamically group by any config column. For example, if you use `wandb.config` to log batch size or learning rate, you can then group by those hyperparameters dynamically in the web app.\\nDistributed training with grouping\\nSuppose you set grouping in `wandb.init()`, we will group runs by default in the UI. You can toggle this on and off by clicking the Group button at the top of the table. Here\\'s an example project generated from sample code where we set grouping. You can click on each \"Group\" row in the sidebar to get to a dedicated group page for that experiment.\\nFrom the project page above, you can click a Group in the left sidebar to get to a dedicated page like this one:\\nGrouping dynamically in the UI\\nYou can group runs by any column, for example by hyperparameter. Here\\'s an example of what that looks like:\\nSidebar: Runs are grouped by the number of epochs.\\nGraphs: Each line represents the group\\'s mean, and the shading indicates the variance. This behavior can be changed in the graph settings.', 'source': 'guides/runs/grouping.md', 'score': 0.9068183, 'relevance': 2}, {'content': 'Method 2: Many processes\\nIn this method, we track each process in the job, calling `wandb.init()` and `wandb.log()` from each process separately. We suggest you call `wandb.finish()` at the end of training, to mark that the run has completed so that all processes exit properly.\\nThis method makes more information accessible for logging. However, note that multiple W&B Runs are reported in the W&B App UI. It might be difficult to keep track of W&B Runs across multiple experiments. To mitigate this, provide a value to the group parameter when you initialize W&B to keep track of which W&B Run belongs to a given experiment. For more information about how to keep track of training and evaluation W&B Runs in experiments, see Group Runs.\\n:::info\\nUse this method if you want to track metrics from individual processes. Typical examples include the data and predictions on each node (for debugging data distribution) and metrics on individual batches outside of the main node. This method is not necessary to get system metrics from all nodes nor to get summary statistics available on the main node.\\n:::\\nThe following Python code snippet demonstrates how to set the group parameter when you initialize W&B:\\n`python\\nif __name__ == \"__main__\":\\n    # Get args\\n    args = parse_args()\\n    # Initialize run\\n    run = wandb.init(\\n        entity=args.entity,\\n        project=args.project,\\n        group=\"DDP\",  # all runs for the experiment in one group\\n    )\\n    # Train model with DDP\\n    train(args, run)`\\nExplore the W&B App UI to view an example dashboard of metrics tracked from multiple processes. Note that there are two W&B Runs grouped together in the left sidebar. Click on a group to view the dedicated group page for the experiment. The dedicated group page displays metrics from each process separately.\\nThe preceding image demonstrates the W&B App UI dashboard. On the sidebar we see two experiments. One labeled \\'null\\' and a second (bound by a yellow box) called \\'DPP\\'. If you expand the group (select the Group dropdown) you will see the W&B Runs that are associated to that experiment.', 'source': 'guides/track/log/distributed-training.md', 'score': 0.55275095, 'relevance': 2}, {'content': \"Turn off grouping\\nClick the grouping button and clear group fields at any time, which returns the table and graphs to their ungrouped state.\\nGrouping graph settings\\nClick the edit button in the upper right corner of a graph and select the Advanced tab to change the line and shading. You can select the mean, minimum, or maximum value for the line in each group. For the shading, you can turn off shading, and show the min and max, the standard deviation, and the standard error.\\nCommon Questions\\nCan we group runs by tags?\\nBecause a run can have multiple tags we don't support grouping by this field. Our recommendation would be to add a value to the config object of these runs and then group by this config value. You can do this with our API.\", 'source': 'guides/runs/grouping.md', 'score': 0.9272315999999999, 'relevance': 1}, {'content': \"How can I log additional metrics after a run completes?\\nThere are several ways to do this.\\nFor complicated workflows, we recommend using multiple runs and setting group parameters in wandb.init to a unique value in all the processes that are run as part of a single experiment. The runs table will automatically group the table by the group ID and the visualizations will behave as expected. This will allow you to run multiple experiments and training runs as separate processes log all the results into a single place.\\nFor simpler workflows, you can call `wandb.init` with `resume=True` and `id=UNIQUE_ID` and then later call `wandb.init` with the same `id=UNIQUE_ID`. Then you can log normally with wandb.log or `wandb.summary` and the runs values will update.\\nPerformance\\nWill wandb slow down my training?\\nW&B should have a negligible effect on your training performance if you use it normally. Normal use of wandb means logging less than once a second and logging less than a few megabytes of data at each step. W&B runs in a separate process and the function calls don't block, so if the network goes down briefly or there are intermittent read write issues on disk it should not affect your performance. It is possible to log a huge amount of data quickly, and if you do that you might create disk I/O issues. If you have any questions, please don't hesitate to contact us.\\nHow many runs to create per project?\\nWe recommend you have roughly 10k runs per project max for performance reasons.\\nBest practices to organize hyperparameter searches\\nIf 10k runs per project (approx.) is a reasonable limit then our recommendation would be to set tags in `wandb.init()` and have a unique tag for each search. This means that you'll easily be able to filter the project down to a given search by clicking that tag in the Project Page in the Runs Table. For example `wandb.init(tags='your_tag')`  docs for this can be found here.\", 'source': 'guides/technical-faq/metrics-and-performance.md', 'score': 0.60940796, 'relevance': 1}, {'content': 'Customize Log Axes\\nUse `define_metric` to set a custom x axis.Custom x-axes are useful in contexts where you need to log to different time steps in the past during training, asynchronously. For example, this can be useful in RL where you may track the per-episode reward and a per-step reward.\\nTry define_metric live in Google Colab →\\nCustomize axes\\nBy default, all metrics are logged against the same x-axis, which is the W&B internal `step`. Sometimes, you might want to log to a previous step, or use a different x-axis.\\nHere\\'s an example of setting a custom x-axis metric, instead of the default step.\\nimport wandb\\nwandb.init()\\n# define our custom x axis metric\\nwandb.define_metric(\"custom_step\")\\n# define which metrics will be plotted against it\\nwandb.define_metric(\"validation_loss\", step_metric=\"custom_step\")\\nfor i in range(10):\\n    log_dict = {\\n        \"train_loss\": 1 / (i + 1),\\n        \"custom_step\": i**2,\\n        \"validation_loss\": 1 / (i + 1),\\n    }\\n    wandb.log(log_dict)\\nThe x-axis can be set using globs as well. Currently, only globs that have string prefixes are available. The following example will plot all logged metrics with the prefix `\"train/\"` to the x-axis `\"train/step\"`:\\nimport wandb\\nwandb.init()\\n# define our custom x axis metric\\nwandb.define_metric(\"train/step\")\\n# set all other train/ metrics to use this step\\nwandb.define_metric(\"train/*\", step_metric=\"train/step\")\\nfor i in range(10):\\n    log_dict = {\\n        \"train/step\": 2**i,  # exponential growth w/ internal W&B step\\n        \"train/loss\": 1 / (i + 1),  # x-axis is train/step\\n        \"train/accuracy\": 1 - (1 / (1 + i)),  # x-axis is train/step\\n        \"val/loss\": 1 / (1 + i),  # x-axis is internal wandb step\\n    }\\n    wandb.log(log_dict)', 'source': 'guides/track/log/customize-logging-axes.md', 'score': 0.35499188, 'relevance': 1}, {'content': 'Enable Full fidelity mode\\nNavigate to your workspace settings or panel settings.\\nSelect the Runs tab.\\nUnder Point aggregation method, choose Full fidelity.\\n:::info Line Plot Grouping or Expressions\\nW&B downsamples points with buckets if you use Line Plot Grouping or Expressions on runs that have non-aligned x-axis values. The x-axis is divided into 200 segments, and points within each segment are averaged. These averages represent the metric values when grouping or combining metrics.\\n:::\\n:::caution Active feature development\\nApplying Grouping or Expressions will revert to Random sampling instead of Full fidelity. We are actively working on achieving full feature parity with the Run Plots settings for Full fidelity mode, including enabling Grouping and Custom Expressions, while also optimizing performance. For now, panels with grouping or expressions will use Random sampling. This feature is available early because it was highly requested and provided value to users, even though improvements are still ongoing. Please reachout to support@wandb.com if you have any issues. \\n:::', 'source': 'guides/app/features/panels/line-plot/sampling.md', 'score': 0.44598192000000003, 'relevance': 0}, {'content': \"Runs Table\\nOn the project page, we show runs in a sidebar. Expand the sidebar to view a table of hyperparameters and summary metrics across runs.\\nSearch run names\\nWe support full regex search on run names in the table. When you type a query in the search box, that will filter down the visible runs in the graphs on the workspace as well as filtering the rows of the table.\\nFilter and sort runs by minimum and maximum values\\nSort the runs table by the minimum or maximum value of a logged metric. This is particularly useful if you want to view the best (or worst) recorded value.\\nThe following steps describe how to sort the run table by a specific metric based on the minimum or maximum recorded value:\\nHover your mouse over the column with the metric you want to sort with.\\nSelect the kebob menu (three vertical lines).\\nFrom the dropdown, select either Show min or Show max.\\nFrom the same dropdown, select Sort by asc or Sort by desc to sort in ascending or descending order, respectively. \\nWhat to do in case regex fails?\\nIf regex doesn't provide you the desired results, you can make use of tags to filter out the runs in Runs Table. Tags can be added either on run creation or after they're finished. Once the tags are added to a run, you can add a tag filter as shown in the gif below.\\nSearch End Time for runs\\nWe provide a column named `End Time` that logs that last heartbeat from the client process. The field is hidden by default.\\nResize the sidebar\\nWould you like to make more space for the graphs on the project page? Click and drag the edge of the column header to resize the sidebar. You'll still be able to click on the eye icon to turn on and off runs on the graphs.\\nAdd sidebar columns\\nOn the project page, we show runs in a sidebar. To show more columns:\\nClick the button in the upper right corner of the sidebar to expand the table.\\nOn a column header, click the dropdown menu to pin a column.\\nPinned columns will be available in the sidebar when you collapse the table.\\nHere's a screen capture. I expand the table, pin two columns, collapse the table, then resize the sidebar.\\nBulk select runs\\nDelete multiple runs at once, or tag a group of runs— bulk selection makes it easier to keep the runs table organized.\", 'source': 'guides/app/features/runs-table.md', 'score': 0.2903768, 'relevance': 0}, {'content': \"Log Metrics\\nCreate a dictionary where the key value is the name of the metric. Pass this dictionary object to wandb.log:\\nfor epoch in range(NUM_EPOCHS):\\n    for input, ground_truth in data: \\n        prediction = model(input) \\n        loss = loss_fn(prediction, ground_truth) \\n        metrics = { “loss”: loss } \\n        wandb.log(metrics)\\nIf you have a lot of metrics, you can have them automatically grouped in the UI by using prefixes in the metric name, such as `train/...` and `val/...` This will create separate sections in your W&B Workspace for your training and validation metrics, or other metric types you'd like to separate.\\nmetrics = {\\n    “train/loss”: 0.4,\\n    “train/learning_rate”: 0.4,\\n    “val/loss”: 0.5, \\n    “val/accuracy”: 0.7\\n}\\nwandb.log(metrics)\\nFor more on `wandb.log`, see Log Data with wandb.log\", 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.22168749999999998, 'relevance': 0}, {'content': 'Metrics & Performance\\nMetrics\\nHow often are system metrics collected?\\nBy default, metrics are collected every 2 seconds and averaged over a 15-second period. If you need higher resolution metrics, email us a contact@wandb.com.\\nCan I just log metrics, no code or dataset examples?\\nDataset Examples\\nBy default, we don\\'t log any of your dataset examples. You can explicitly turn this feature on to see example predictions in our web interface.\\nCode Logging\\nThere are two ways to turn off code logging:\\nSet `WANDB_DISABLE_CODE` to `true` to turn off all code tracking. We won\\'t pick up the git SHA or the diff patch.\\nSet `WANDB_IGNORE_GLOBS` to `*.patch` to turn off syncing the diff patch to our servers. You\\'ll still have it locally and be able to apply it with the `wandb restore`.\\nCan I log metrics on two different time scales? (For example, I want to log training accuracy per batch and validation accuracy per epoch.)\\nYes, you can do this by logging your indices (e.g. `batch` and `epoch`) whenever you log your other metrics. So in one step you could call `wandb.log({\\'train_accuracy\\': 0.9, \\'batch\\': 200})` and in another step call `wandb.log({\\'val_accuracy\\': 0.8, \\'epoch\\': 4})`. Then, in the UI, you can set the appropriate value as the x-axis for each chart. If you want to set the default x-axis of a particular index you can do so using by using Run.define_metric(). In our above example we could do the following:\\nwandb.init()\\nwandb.define_metric(\"batch\")\\nwandb.define_metric(\"epoch\")\\nwandb.define_metric(\"train_accuracy\", step_metric=\"batch\")\\nwandb.define_metric(\"val_accuracy\", step_metric=\"epoch\")\\nHow can I log a metric that doesn\\'t change over time such as a final evaluation accuracy?\\nUsing `wandb.log({\\'final_accuracy\\': 0.9}` will work fine for this. By default `wandb.log({\\'final_accuracy\\'})` will update `wandb.settings[\\'final_accuracy\\']`, which is the value shown in the runs table.', 'source': 'guides/technical-faq/metrics-and-performance.md', 'score': 0.15507847, 'relevance': 0}, {'content': 'Why is nothing showing up in my graphs?\\nIf you\\'re seeing \"No visualization data logged yet\" that means that we haven\\'t gotten the first `wandb.log` call from your script yet. This could be because your run takes a long time to finish a step. If you\\'re logging at the end of each epoch, you could log a few times per epoch to see data stream in more quickly.\\nWhy is the same metric appearing more than once?\\nIf you\\'re logging different types of data under the same key, we have to split them out in our database. This means you\\'ll see multiple entries of the same metric name in a dropdown in the UI. The types we group by are `number`, `string`, `bool`, `other` (mostly arrays), and any `wandb` data type (`Histogram`, `Image`, etc). Send only one type to each key to avoid this behavior.\\nWe store metrics in a case-insensitive fashion, so make sure you don\\'t have two metrics with the same name like `\"My-Metric\"` and `\"my-metric\"`.\\nHow can I access the data logged to my runs directly and programmatically?\\nThe history object is used to track metrics logged by `wandb.log`. Using our API, you can access the history object via `run.history()`.\\napi = wandb.Api()\\nrun = api.run(\"username/project/run_id\")\\nprint(run.history())\\nWhat happens when I log millions of steps to W&B? How is that rendered in the browser?\\nThe more points you send us, the longer it will take to load your graphs in the UI. If you have more than 1000 points on a line, we sample down to 1000 points on the backend before we send your browser the data. This sampling is nondeterministic, so if you refresh the page you\\'ll see a different set of sampled points.\\nGuidelines\\nWe recommend that you try to log less than 10,000 points per metric. If you log more than 1 million points in a line, it will take us while to load the page. For more on strategies for reducing logging footprint without sacrificing accuracy, check out this Colab. If you have more than 500 columns of config and summary metrics, we\\'ll only show 500 in the table.', 'source': 'guides/track/log/logging-faqs.md', 'score': 0.053799644, 'relevance': 0}])"
        ],
        [
         "15",
         "how does Weights & Biases support GPU usage?",
         "Weights & Biases (W&B) provides robust support for GPU usage, particularly beneficial for training machine learning models more efficiently. Here’s how W&B supports and utilizes GPU:\n\n1. **Automatic GPU Metrics Logging**: When you train your models using W&B, it automatically logs GPU metrics such as GPU utilization, GPU memory allocated, and CPU utilization. This feature helps you monitor and optimize the computational resources used during training.\n\n2. **GPU Usage Visualization**: W&B provides visualization tools that allow you to see real-time metrics on GPU usage. This can help you understand how well your model is utilizing the GPU and make necessary adjustments to improve performance and resource usage.\n\n3. **Integration with GPU-Enabled Frameworks**: W&B seamlessly integrates with popular machine learning frameworks that support GPU acceleration, such as TensorFlow and PyTorch. This integration ensures that you can easily log and monitor GPU metrics without additional configuration.\n\n4. **Configuration for GPU Use**: While W&B itself does not require special configuration to use GPUs, it works with your machine learning code that is configured to use GPUs. For instance, in TensorFlow or PyTorch, you would typically set your device to a GPU with specific commands, and W&B will automatically track the usage once the model training begins on this device.\n\n5. **Enhanced Performance**: Using GPUs with W&B can significantly speed up the training process of complex models due to faster computation capabilities compared to CPUs. This is particularly advantageous when working with large datasets or deep learning models.\n\n6. **Documentation and Community Support**: W&B provides extensive documentation and community support that can help users troubleshoot issues related to GPU usage and optimize their GPU settings for better performance.\n\nFor specific steps on how to ensure your W&B setup is utilizing GPUs, you would typically need to ensure your machine learning framework (like TensorFlow or PyTorch) is set up to use GPUs. W&B will then automatically log relevant GPU metrics during training sessions.\n\n**Sources**:\n- [GPU Metrics Visualization](https://wandb.ai/wandb/common-ml-errors/reports/--VmlldzozMDYxMDQ=)\n- [Automatic GPU Metrics Logging](https://wandb.ai/wandb_fc/french/reports/--VmlldzoxNTEzMzYy)",
         "WeaveList([{'content': 'NVIDIA GPU\\nW&B uses an adapted version of the `pynvml` library to capture the NVIDIA GPU metrics.  Refer to this guide from NVIDIA for a detailed description of captured metrics.\\nIn addition to the metrics described below, if the process uses a particular GPU, W&B captures the corresponding metrics as `gpu.process.{gpu_index}...`\\nW&B uses the following code snippet to check if a process uses a particular GPU:\\ndef gpu_in_use_by_this_process(gpu_handle: \"GPUHandle\", pid: int) -> bool:\\n    if psutil is None:\\n        return False\\n    try:\\n        base_process = psutil.Process(pid=pid)\\n    except psutil.NoSuchProcess:\\n        # do not report any gpu metrics if the base process can\\'t be found\\n        return False\\n    our_processes = base_process.children(recursive=True)\\n    our_processes.append(base_process)\\n    our_pids = {process.pid for process in our_processes}\\n    compute_pids = {\\n        process.pid\\n        for process in pynvml.nvmlDeviceGetComputeRunningProcesses(gpu_handle)  # type: ignore\\n    }\\n    graphics_pids = {\\n        process.pid\\n        for process in pynvml.nvmlDeviceGetGraphicsRunningProcesses(gpu_handle)  # type: ignore\\n    }\\n    pids_using_device = compute_pids | graphics_pids\\n    return len(pids_using_device & our_pids) > 0\\nGPU Memory Utilization\\nRepresents the GPU memory utilization in percent for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\npynvml.nvmlDeviceGetUtilizationRates(handle).memory\\nW&B assigns a `gpu.{gpu_index}.memory` tag to this metric.\\nGPU Memory Allocated\\nIndicates the GPU memory allocated as a percentage of the total available memory for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\nmemory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\\nmemory_info.used / memory_info.total * 100\\nThis computes the percentage of GPU memory allocated for each GPU.\\nW&B assigns a `gpu.{gpu_index}.memoryAllocated` tag to this metric.', 'source': 'guides/app/features/system-metrics.md', 'score': 0.0001465313, 'relevance': 2}, {'content': 'GPU Memory Allocated Bytes\\nSpecifies the GPU memory allocated in bytes for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\nmemory_info = pynvml.nvmlDeviceGetMemoryInfo(handle)\\nmemory_info.used\\nW&B assigns a `gpu.{gpu_index}.memoryAllocatedBytes` tag to this metric.\\nGPU Utilization\\nReflects the GPU utilization in percent for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\npynvml.nvmlDeviceGetUtilizationRates(handle).gpu\\nW&B assigns a `gpu.{gpu_index}.gpu` tag to this metric.\\nGPU Temperature\\nThe GPU temperature in Celsius for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\npynvml.nvmlDeviceGetTemperature(handle, pynvml.NVML_TEMPERATURE_GPU)\\nW&B assigns a `gpu.{gpu_index}.temp` tag to this metric.\\nGPU Power Usage Watts\\nIndicates the GPU power usage in Watts for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\npynvml.nvmlDeviceGetPowerUsage(handle) / 1000\\nW&B assigns a `gpu.{gpu_index}.powerWatts` tag to this metric.\\nGPU Power Usage Percent\\nReflects the GPU power usage as a percentage of its power capacity for each GPU.\\nhandle = pynvml.nvmlDeviceGetHandleByIndex(gpu_index)\\n(power_watts / power_capacity_watts) * 100\\nW&B assigns a `gpu.{gpu_index}.powerPercent` tag to this metric.\\nAMD GPU\\nThe metrics are extracted from the output (`stats`) of the `rocm-smi` tool supplied by AMD (`rocm-smi -a --json`).\\nAMD GPU Utilization\\nRepresents the GPU utilization in percent for each AMD GPU device.\\nstats.get(\"GPU use (%)\")\\nW&B assigns a `gpu.{gpu_index}.gpu` tag to this metric.', 'source': 'guides/app/features/system-metrics.md', 'score': 7.08633e-05, 'relevance': 2}, {'content': 'PyTorch\\nUse Weights & Biases for machine learning experiment tracking, dataset versioning, and project collaboration.\\nTry in a Colab Notebook here →\\nWhat this notebook covers:\\nWe show you how to integrate Weights & Biases with your PyTorch code to add experiment tracking to your pipeline.\\nThe resulting interactive W&B dashboard will look like:\\nIn pseudocode, what we\\'ll do is:\\n# import the library\\nimport wandb\\n# start a new experiment\\nwandb.init(project=\"new-sota-model\")\\n#\\u2003capture a dictionary of hyperparameters with config\\nwandb.config = {\"learning_rate\": 0.001, \"epochs\": 100, \"batch_size\": 128}\\n# set up model and data\\nmodel, dataloader = get_model(), get_data()\\n# optional: track gradients\\nwandb.watch(model)\\nfor batch in dataloader:\\n  metrics = model.training_step()\\n  #\\u2003log metrics inside your training loop to visualize model performance\\n  wandb.log(metrics)\\n# optional: save model at the end\\nmodel.to_onnx()\\nwandb.save(\"model.onnx\")\\nFollow along with a video tutorial!\\nNote: Sections starting with Step are all you need to integrate W&B in an existing pipeline. The rest just loads data and defines a model.\\n🚀 Install, Import, and Log In\\nimport os\\nimport random\\nimport numpy as np\\nimport torch\\nimport torch.nn as nn\\nimport torchvision\\nimport torchvision.transforms as transforms\\nfrom tqdm.auto import tqdm\\n# Ensure deterministic behavior\\ntorch.backends.cudnn.deterministic = True\\nrandom.seed(hash(\"setting random seeds\") % 2**32 - 1)\\nnp.random.seed(hash(\"improves reproducibility\") % 2**32 - 1)\\ntorch.manual_seed(hash(\"by removing stochasticity\") % 2**32 - 1)\\ntorch.cuda.manual_seed_all(hash(\"so runs are repeatable\") % 2**32 - 1)\\n# Device configuration\\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\n# remove slow mirror from list of MNIST mirrors\\ntorchvision.datasets.MNIST.mirrors = [mirror for mirror in torchvision.datasets.MNIST.mirrors\\n                                      if not mirror.startswith(\"http://yann.lecun.com\")]', 'source': 'tutorials/pytorch.md', 'score': 0.9709644, 'relevance': 1}, {'content': 'PyTorch Ignite\\nSee the resulting visualizations in this example W&B report →\\nTry running the code yourself in this example hosted notebook →\\nIgnite supports Weights & Biases handler to log metrics, model/optimizer parameters, gradients during training and validation. It can also be used to log model checkpoints to the Weights & Biases cloud. This class is also a wrapper for the wandb module. This means that you can call any wandb function using this wrapper. See examples on how to save model parameters and gradients.', 'source': 'guides/integrations/other/ignite.md', 'score': 0.9174507, 'relevance': 1}, {'content': 'TensorBoard\\nHosted TensorBoard with 1 Line of Code\\nWith Weight & Biases you can easily upload your TensorBoard logs to the cloud, quickly share your results among colleagues and classmates and keep your analysis in one centralized location.\\nGet started now in with this Notebook: Try in a Colab Notebook here →\\nJust add 1 Line of Code\\nimport wandb\\n# Start a wandb run with `sync_tensorboard=True`\\nwandb.init(project=\"my-project\", sync_tensorboard=True)\\n# Your training code using TensorBoard\\n...\\n# [Optional]Finish the wandb run to upload the tensorboard logs to W&B (if running in Notebook)\\nwandb.finish()\\nSee here for an example of Tensorboard hosted in Weights & Biases\\nOnce your wandb run finishes, your TensorBoard event files will then be uploaded to Weights & Biases. These metrics will also be logged in native Weights & Biases charts along with a host of useful information such as your machines CPU or GPU utilization, the git state, the terminal command used, and much more.\\n:::info\\nWeights & Biases support TensorBoard with all versions of TensorFlow. W&B also supports TensorBoard > 1.14 with PyTorch as well as TensorBoardX.\\n:::\\nCommon questions\\nHow can I log metrics to W&B that aren\\'t logged to TensorBoard?\\nIf you need to log additional custom metrics that aren\\'t being logged to TensorBoard, you can call `wandb.log` in your code `wandb.log({\"custom\": 0.8})`\\nSetting the step argument in `wandb.log` is disabled when syncing Tensorboard. If you\\'d like to set a different step count, you can log the metrics with a step metric as:\\n`wandb.log({\"custom\": 0.8, \"global_step\": global_step})`', 'source': 'guides/integrations/tensorboard.md', 'score': 0.86882675, 'relevance': 1}, {'content': \"How to use multiple GPUs with Lightning and W&B?\\nPyTorch Lightning has Multi-GPU support through their DDP Interface. However, PyTorch Lightning's design requires us to be careful about how we instantiate our GPUs.\\nLightning assumes that each GPU (or Rank) in your training loop must be instantiated in exactly the same way - with the same initial conditions. However, only rank 0 process gets access to the `wandb.run` object, and for non-zero rank processes: `wandb.run = None`. This could cause your non-zero processes to fail. Such a situation can put you in a deadlock because rank 0 process will wait for the non-zero rank processes to join, which have already crashed.\\nFor this reason, we have to be careful about how we set up our training code. The recommended way to set it up would be to have your code be independent of the `wandb.run` object.\", 'source': 'guides/integrations/lightning.md', 'score': 0.003945407000000001, 'relevance': 1}, {'content': \"How is W&B different from TensorBoard?\\nWhen the cofounders started working on W&B, they were inspired to build a tool for the frustrated TensorBoard users at OpenAI. Here are a few things we've focused on improving:\\nReproduce models: Weights & Biases is good for experimentation, exploration, and reproducing models later. We capture not just the metrics, but also the hyperparameters and version of the code, and we can save your version-control status and model checkpoints for you so your project is reproducible. \\nAutomatic organization: Whether you're picking up a project from a collaborator, coming back from a vacation, or dusting off an old project, W&B makes it easy to see all the models that have been tried so no one wastes hours, GPU cycles, or carbon re-running experiments.\\nFast, flexible integration: Add W&B to your project in 5 minutes. Install our free open-source Python package and add a couple of lines to your code, and every time you run your model you'll have nice logged metrics and records.\\nPersistent, centralized dashboard: No matter where you train your models, whether on your local machine, in a shared lab cluster, or on spot instances in the cloud, your results are shared to the same centralized dashboard. You don't need to spend your time copying and organizing TensorBoard files from different machines.\\nPowerful tables: Search, filter, sort, and group results from different models. It's easy to look over thousands of model versions and find the best performing models for different tasks. TensorBoard isn't built to work well on large projects.\\nTools for collaboration: Use W&B to organize complex machine learning projects. It's easy to share a link to W&B, and you can use private teams to have everyone sending results to a shared project. We also support collaboration via reports— add interactive visualizations and describe your work in markdown. This is a great way to keep a work log, share findings with your supervisor, or present findings to your lab or team.\\nGet started with a free account\\nExamples\\nWe've created a few examples for you to see how the integration works:\\nExample on Github: MNIST example Using TensorFlow Estimators\\nExample on Github: Fashion MNIST example Using Raw TensorFlow\\nWandb Dashboard: View result on W&B\\nCustomizing Training Loops in TensorFlow 2 - Article | Colab Notebook | Dashboard\", 'source': 'guides/integrations/tensorflow.md', 'score': 0.8293537999999999, 'relevance': 0}, {'content': \"OpenAI and Weights & Biases\\nWeights & Biases has 2 OpenAI integrations to help you train better models faster and keep track of your prompts and generations:\\nOpenAI ChatGPT-3.5 and GPT-4 fine-tuning\\nWith Weights & Biases you can log your OpenAI model's fine-tuning metrics and configuration to Weights & Biases to analyse and understand the performance of your newly fine-tuned models and share the results with your colleagues.\\nSee the W&B and OpenAI Fine-Tuning docs here to learn more\\nOpenAI Python SDK API\\nLog requests, responses, token counts and model metadata with 1 line of code for all OpenAI models\\nSee the W&B and OpenAI API section docs to learn more\", 'source': 'guides/prompts/openai.md', 'score': 0.6068479999999999, 'relevance': 0}, {'content': 'More Resources\\nSupercharging Ultralytics with Weights & Biases\\nObject Detection using YOLOv8: An End-to-End Workflow', 'source': 'guides/integrations/ultralytics.md', 'score': 0.20833996, 'relevance': 0}, {'content': \"What is W&B?\\nWeights & Biases (W&B) is the AI developer platform, with tools for training models, fine-tuning models, and leveraging foundation models. \\nSet up W&B in 5 minutes, then quickly iterate on your machine learning pipeline with the confidence that your models and data are tracked and versioned in a reliable system of record.\\nThis diagram outlines the relationship between W&B products.\\nW&B Models is a set of lightweight, interoperable tools for machine learning practitioners training and fine-tuning models.\\n- Experiments: Machine learning experiment tracking\\n- Model Registry: Manage production models centrally\\n- Launch: Scale and automate workloads\\n- Sweeps: Hyperparameter tuning and model optimization\\nW&B Weave is a lightweight toolkit for tracking and evaluating LLM applications.\\nW&B Core is a core set of powerful building blocks for tracking and visualizing data and models, and communicating results.\\n- Artifacts: Version assets and track lineage\\n- Tables: Visualize and query tabular data\\n- Reports: Document and collaborate on your discoveries\\nAre you a first-time user of W&B?\\nStart exploring W&B with these resources:\\nIntro Notebook: Run quick sample code to track experiments in 5 minutes\\nQuickstart: Read a quick overview of how and where to add W&B to your code\\nExplore our Integrations guide and our W&B Easy Integration YouTube playlist for information on how to integrate W&B with your preferred machine learning framework.\\nView the API Reference guide for technical specifications about the W&B Python Library, CLI, and Query Language operations.\\nHow does W&B work?\\nWe recommend you read the following sections in this order if you are a first-time user of W&B:\\nLearn about Runs, W&B's basic unit of computation.\\nCreate and track machine learning experiments with Experiments.\\nDiscover W&B's flexible and lightweight building block for dataset and model versioning with Artifacts.\\nAutomate hyperparameter search and explore the space of possible models with Sweeps.\\nManage the model lifecycle from training to production with Model Management.\\nVisualize predictions across model versions with our Data Visualization guide.\\nOrganize W&B Runs, embed and automate visualizations, describe your findings, and share updates with collaborators with Reports.\", 'source': 'guides/intro.md', 'score': 0.080069266, 'relevance': 0}])"
        ],
        [
         "16",
         "We were wondering if there is a standard way to log non strictly increasing steps? We are offloading some of our evaluation to CPUs and we would like to continue training and log the eval metrics later (but to the original step). There is the trick of logging a `metric_step` metric for example and change the x-axis but this seems very inefficient for retrieving the time series.",
         "To log non-strictly increasing steps in Weights & Biases, you can utilize the `step` parameter in the `wandb.log` function. This allows you to manually specify the step number for each log entry, which is particularly useful when you want to log metrics out of the normal sequence, such as during asynchronous evaluations or when logging past events.\n\nHere's how you can log metrics at a specific step:\n\n```python\n# Assuming 'step' is the step number you want to log at\nwandb.log({\"accuracy\": 0.9}, step=step)\n```\n\nThis method is efficient and straightforward for logging metrics that correspond to non-sequential steps in your training or evaluation process.\n\nRegarding the best practices for logging evaluation metrics later in training:\n- **Consistency**: Ensure that the step numbers used for logging evaluation metrics correspond accurately to the training steps they represent.\n- **Documentation**: Clearly document the meaning of each step in your code and W&B logs to avoid confusion when analyzing the results.\n- **Verification**: Regularly verify that the logged data aligns with your expectations, especially when using manual step assignments.\n\nUsing a 'metric_step' metric to change the x-axis in Weights & Biases can be a viable approach, especially if you need to align multiple metrics that are logged at different times or conditions. However, it's important to ensure that this method does not complicate the retrieval and analysis of your data. If you find it cumbersome, using the `step` parameter directly, as shown above, is recommended for its simplicity and direct control over the logging process.\n\nFor more detailed examples and best practices, you can refer to the [W&B documentation on logging](https://docs.wandb.ai/guides/track/log).",
         "WeaveList([{'content': 'Customize Log Axes\\nUse `define_metric` to set a custom x axis.Custom x-axes are useful in contexts where you need to log to different time steps in the past during training, asynchronously. For example, this can be useful in RL where you may track the per-episode reward and a per-step reward.\\nTry define_metric live in Google Colab →\\nCustomize axes\\nBy default, all metrics are logged against the same x-axis, which is the W&B internal `step`. Sometimes, you might want to log to a previous step, or use a different x-axis.\\nHere\\'s an example of setting a custom x-axis metric, instead of the default step.\\nimport wandb\\nwandb.init()\\n# define our custom x axis metric\\nwandb.define_metric(\"custom_step\")\\n# define which metrics will be plotted against it\\nwandb.define_metric(\"validation_loss\", step_metric=\"custom_step\")\\nfor i in range(10):\\n    log_dict = {\\n        \"train_loss\": 1 / (i + 1),\\n        \"custom_step\": i**2,\\n        \"validation_loss\": 1 / (i + 1),\\n    }\\n    wandb.log(log_dict)\\nThe x-axis can be set using globs as well. Currently, only globs that have string prefixes are available. The following example will plot all logged metrics with the prefix `\"train/\"` to the x-axis `\"train/step\"`:\\nimport wandb\\nwandb.init()\\n# define our custom x axis metric\\nwandb.define_metric(\"train/step\")\\n# set all other train/ metrics to use this step\\nwandb.define_metric(\"train/*\", step_metric=\"train/step\")\\nfor i in range(10):\\n    log_dict = {\\n        \"train/step\": 2**i,  # exponential growth w/ internal W&B step\\n        \"train/loss\": 1 / (i + 1),  # x-axis is train/step\\n        \"train/accuracy\": 1 - (1 / (1 + i)),  # x-axis is train/step\\n        \"val/loss\": 1 / (1 + i),  # x-axis is internal wandb step\\n    }\\n    wandb.log(log_dict)', 'source': 'guides/track/log/customize-logging-axes.md', 'score': 0.99720997, 'relevance': 2}, {'content': 'Preventing x-axis Misalignments\\nSometimes you might need to perform multiple calls to `wandb.log` for the same training step. The wandb SDK has its own internal step counter that is incremented every time a `wandb.log` call is made. This means that there is a possibility that the wandb log counter is not aligned with the training step in your training loop.\\nIn first pass of the example below, the internal `wandb` step for `train/loss` will be 0, while the internal `wandb` step for  `eval/loss`  will be 1. On the next pass, the `train/loss` will be 2, while the  `eval/loss` wandb step will be 3, etc\\nfor input, ground_truth in data:\\n    ...\\n    wandb.log(“train/loss”: 0.1)  \\n    wandb.log(“eval/loss”: 0.2)\\nTo avoid this, we recommend that you specifically define your x-axis step. You can define the x-axis with `wandb.define_metric` and you only need to do this once, after `wandb.init` is called:\\nwandb.init(...)\\nwandb.define_metric(\"*\", step_metric=\"global_step\")\\nThe glob pattern, \"*\", means that every metric will use “global_step” as the x-axis in your charts. If you only want certain metrics to be logged against \"global_step\", you can specify them instead:\\nwandb.define_metric(\"train/loss\", step_metric=\"global_step\")\\nNow that you\\'ve called `wandb.define_metric` , you just need to log your metrics as well as your `step_metric`, \"global_step\", every time you call `wandb.log`:\\nfor step, (input, ground_truth) in enumerate(data):\\n    ...\\n    wandb.log({“global_step”: step, “train/loss”: 0.1})\\n    wandb.log({“global_step”: step, “eval/loss”: 0.2})\\nIf you do not have access to the independent step variable, for example “global_step” is not available during your validation loop, the previously logged value for \"global_step\" is automatically used by wandb. In this case, ensure you log an initial value for the metric so it has been defined when it’s needed.', 'source': 'guides/integrations/add-wandb-to-any-library.md', 'score': 0.9313376999999999, 'relevance': 2}, {'content': 'Metrics & Performance\\nMetrics\\nHow often are system metrics collected?\\nBy default, metrics are collected every 2 seconds and averaged over a 15-second period. If you need higher resolution metrics, email us a contact@wandb.com.\\nCan I just log metrics, no code or dataset examples?\\nDataset Examples\\nBy default, we don\\'t log any of your dataset examples. You can explicitly turn this feature on to see example predictions in our web interface.\\nCode Logging\\nThere are two ways to turn off code logging:\\nSet `WANDB_DISABLE_CODE` to `true` to turn off all code tracking. We won\\'t pick up the git SHA or the diff patch.\\nSet `WANDB_IGNORE_GLOBS` to `*.patch` to turn off syncing the diff patch to our servers. You\\'ll still have it locally and be able to apply it with the `wandb restore`.\\nCan I log metrics on two different time scales? (For example, I want to log training accuracy per batch and validation accuracy per epoch.)\\nYes, you can do this by logging your indices (e.g. `batch` and `epoch`) whenever you log your other metrics. So in one step you could call `wandb.log({\\'train_accuracy\\': 0.9, \\'batch\\': 200})` and in another step call `wandb.log({\\'val_accuracy\\': 0.8, \\'epoch\\': 4})`. Then, in the UI, you can set the appropriate value as the x-axis for each chart. If you want to set the default x-axis of a particular index you can do so using by using Run.define_metric(). In our above example we could do the following:\\nwandb.init()\\nwandb.define_metric(\"batch\")\\nwandb.define_metric(\"epoch\")\\nwandb.define_metric(\"train_accuracy\", step_metric=\"batch\")\\nwandb.define_metric(\"val_accuracy\", step_metric=\"epoch\")\\nHow can I log a metric that doesn\\'t change over time such as a final evaluation accuracy?\\nUsing `wandb.log({\\'final_accuracy\\': 0.9}` will work fine for this. By default `wandb.log({\\'final_accuracy\\'})` will update `wandb.settings[\\'final_accuracy\\']`, which is the value shown in the runs table.', 'source': 'guides/technical-faq/metrics-and-performance.md', 'score': 0.44724903, 'relevance': 2}, {'content': 'What if I want to integrate W&B into my project, but I don\\'t want to upload any images or media?\\nW&B can be used even for projects that only log scalars — you specify any files or data you\\'d like to upload explicitly. Here\\'s a quick example in PyTorch that does not log images.\\nWhat happens if I pass a class attribute into wandb.log()?\\nIt is generally not recommended to pass class attributes into `wandb.log()` as the attribute may change before the network call is made. If you are storing metrics as the attribute of a class, it is recommended to deep copy the attribute to ensure the metric logged matches the value of the attribute at the time that `wandb.log()` was called.\\nWhy am I seeing fewer data points than I logged?\\nIf you are visualizing your metrics against something other than `Step` on your X-Axis, you might see fewer data points than you expect. This is because we require the metrics to be plotted against one another to be logged at the same `Step` - that is how we keep your metrics synchronized, i.e., we only sample metrics that are logged at the same `Step` while interpolating in between samples.\\nGuidelines\\n*\\n*We recommend you bundle your metrics into the same `log()` call. If your code looks like this:\\nwandb.log({\"Precision\": precision})\\n...\\nwandb.log({\"Recall\": recall})\\nIt would be better to log it as:\\nwandb.log({\"Precision\": precision, \"Recall\": recall})\\nAlternatively, you can manually control the step parameter and synchronize your metrics in your own code:\\nwandb.log({\"Precision\": precision}, step=step)\\n...\\nwandb.log({\"Recall\": recall}, step=step)\\nIf the value of `step` is the same in both the calls to `log()`, your metrics will be logged under the same step and be sampled together. Please note that step must be monotonically increasing in each call, otherwise the `step` value is ignored during your call to `log()`.', 'source': 'guides/track/log/logging-faqs.md', 'score': 0.24113181, 'relevance': 2}, {'content': 'Logging FAQs\\nFrequently Asked Questions About Logging Data from Experiments\\nHow can I organize my logged charts and media in the W&B UI?\\nWe treat `/` as a separator for organizing logged panels in the W&B UI. By default, the component of the logged item\\'s name before a `/` is used to define a group of panel called a \"Panel Section\".\\nwandb.log({\"val/loss\": 1.1, \"val/acc\": 0.3})\\nwandb.log({\"train/loss\": 0.1, \"train/acc\": 0.94})\\nIn the Workspace settings, you can change whether panels are grouped by just the first component or by all components separated by `/`.\\nHow can I compare images or media across epochs or steps?\\nEach time you log images from a step, we save them to show in the UI. Expand the image panel, and use the step slider to look at images from different steps. This makes it easy to compare how a model\\'s output changes during training.\\nWhat if I want to log some metrics on batches and some metrics only on epochs?\\nIf you\\'d like to log certain metrics in every batch and standardize plots, you can log x axis values that you want to plot with your metrics. Then in the custom plots, click edit and select a custom x-axis.\\nwandb.log({\"batch\": batch_idx, \"loss\": 0.3})\\nwandb.log({\"epoch\": epoch, \"val_acc\": 0.94})\\nHow do I log a list of values?\\nwandb.log({f\"losses/loss-{ii}\": loss for ii, loss in enumerate(losses)})\\nwandb.log({\"losses\": wandb.Histogram(losses)})  # converts losses to a histogram', 'source': 'guides/track/log/logging-faqs.md', 'score': 0.09057448, 'relevance': 1}, {'content': '🚝 Training and Validation\\nBefore training, define the metric properties which will later be logged with `wandb.log()` for tracking the training and validation experiments.\\nwandb.define_metric(\"epoch/epoch_step\")\\nwandb.define_metric(\"epoch/*\", step_metric=\"epoch/epoch_step\")\\nwandb.define_metric(\"batch/batch_step\")\\nwandb.define_metric(\"batch/*\", step_metric=\"batch/batch_step\")\\nwandb.define_metric(\"validation/validation_step\")\\nwandb.define_metric(\"validation/*\", step_metric=\"validation/validation_step\")\\nbatch_step = 0\\nvalidation_step = 0\\nmetric_values = []\\nmetric_values_tumor_core = []\\nmetric_values_whole_tumor = []\\nmetric_values_enhanced_tumor = []', 'source': 'tutorials/monai_3d_segmentation.md', 'score': 0.90548956, 'relevance': 1}, {'content': \"Log metrics\\nYou can log your metrics to W&B when using the `WandbLogger` by calling `self.log('my_metric_name', metric_vale)` within your `LightningModule`, such as in your `training_step` or `validation_step methods.`\\nThe code snippet below shows how to define your `LightningModule` to log your metrics and your `LightningModule` hyperparameters. In this example we will use the torchmetrics library to calculate our metrics\", 'source': 'guides/integrations/lightning.md', 'score': 0.8262917, 'relevance': 0}, {'content': 'Log Media and Objects in Experiments\\nLog Media and Objects in Experiments\\nLog a dictionary of metrics, media, or custom objects to a step with the W&B Python SDK. W&B collects the key-value pairs during each step and stores them in one unified dictionary each time you log data with `wandb.log()`. Data logged from your script is saved locally to your machine in a directory called `wandb`, then synced to the W&B cloud or your private server. \\n:::info\\nKey-value pairs are stored in one unified dictionary only if you pass the same value for each step. W&B writes all of the collected keys and values to memory if you log a different value for `step`.\\n:::\\nEach call to `wandb.log` is a new `step` by default. W&B uses steps as the default x-axis when it creates charts and panels. You can optionally create and use a custom x-axis or capture a custom summary metric. For more information, see Customize log axes.\\n:::caution\\nUse `wandb.log()` to log consecutive values for each `step`: 0, 1, 2, and so on. It is not possible to write to a specific history step. W&B only writes to the \"current\" and \"next\" step.\\n:::\\nAutomatically logged data\\nW&B automatically logs the following information during a W&B Experiment:\\nSystem metrics: CPU and GPU utilization, network, etc. These are shown in the System tab on the run page. For the GPU, these are fetched with nvidia-smi.\\nCommand line: The stdout and stderr are picked up and show in the logs tab on the run page.\\nTurn on Code Saving in your account\\'s Settings page to log:\\nGit commit: Pick up the latest git commit and see it on the overview tab of the run page, as well as a `diff.patch` file if there are any uncommitted changes.\\nDependencies: The `requirements.txt` file will be uploaded and shown on the files tab of the run page, along with any files you save to the `wandb` directory for the run.', 'source': 'guides/track/log/intro.md', 'score': 0.23493281, 'relevance': 0}, {'content': 'Log metrics inside your training loop\\nLog metrics during each `for` loop (epoch), the accuracy and loss values are computed and logged to W&B with wandb.log(). By default, when you call wandb.log it appends a new step to the history object and updates the summary object.\\nThe following code example shows how to log metrics with `wandb.log`.\\n:::note\\nDetails of how to set up your mode and retrieve data are omitted. \\n:::\\n# Set up model and data\\nmodel, dataloader = get_model(), get_data()\\nfor epoch in range(wandb.config.epochs):\\n    for batch in dataloader:\\n        loss, accuracy = model.training_step()\\n        # \\u20033. Log metrics inside your training loop to visualize\\n        # model performance\\n        wandb.log({\"accuracy\": accuracy, \"loss\": loss})\\nFor more information on different data types you can log with W&B, see Log Data During Experiments.\\nLog an artifact to W&B\\nOptionally log a W&B Artifact. Artifacts make it easy to version datasets and models. \\nwandb.log_artifact(model)\\nFor more information about Artifacts, see the Artifacts Chapter. For more information about versioning models, see Model Management.\\nPutting it all together\\nThe full script with the preceding code snippets is found below:\\n# Import the W&B Python Library\\nimport wandb\\n# 1. Start a W&B Run\\nrun = wandb.init(project=\"cat-classification\", notes=\"\", tags=[\"baseline\", \"paper1\"])\\n# \\u20032. Capture a dictionary of hyperparameters\\nwandb.config = {\"epochs\": 100, \"learning_rate\": 0.001, \"batch_size\": 128}\\n# Set up model and data\\nmodel, dataloader = get_model(), get_data()\\nfor epoch in range(wandb.config.epochs):\\n    for batch in dataloader:\\n        loss, accuracy = model.training_step()\\n        # \\u20033. Log metrics inside your training loop to visualize\\n        # model performance\\n        wandb.log({\"accuracy\": accuracy, \"loss\": loss})\\n# 4. Log an artifact to W&B\\nwandb.log_artifact(model)\\n# Optional: save model at the end\\nmodel.to_onnx()\\nwandb.save(\"model.onnx\")', 'source': 'guides/track/launch.md', 'score': 0.19590157, 'relevance': 0}, {'content': 'Logged Metrics\\nEach recipe has their own training loop, so check each individual recipe to see what metrics are logged. The default metrics logged are:\\nMetric\\nDescription\\n`loss`\\nThe loss of the model\\n`lr`\\nThe learning rate\\n`tokens_per_second`\\nThe tokens per second of the model\\n`grad_norm`\\nThe gradient norm of the model\\n`global_step`\\nCorresponds to the current step in the training loop. Takes into account gradient accumulation, basically every time an optimizer step is taken, the model is updated, the gradients are accumulated and the model is updated once every `gradient_accumulation_steps`\\n:::info\\n`global_step` is not the same as the number of training steps. It corresponds to the current step in the training loop. Takes into account gradient accumulation, basically every time an optimizer step is taken the `global_step` is incremented by 1. For example, if the dataloader has 10 batches, gradient accumulation steps is 2 and run for 3 epochs, the optimizer will step 15 times, in this case `global_step` will range from 1 to 15.\\n:::\\nThe streamlined design of torchtune allows to easily add custom metrics or modify the existing ones. It suffices to modify the corresponding recipe file, for example, computing one could log `current_epoch` as a percentage of the total number of epochs as following:\\n# inside `train.py` function in the recipe file\\nself._metric_logger.log_dict(\\n    {\"current_epoch\": self.epochs * self.global_step / self._steps_per_epoch},\\n    step=self.global_step,\\n)\\n:::info\\nThis is a fast evolving library, the current metrics are subject to change. If you want to add a custom metric, you should modify the recipe and call the corresponding `self._metric_logger.*` function.\\n:::', 'source': 'guides/integrations/torchtune.md', 'score': 0.17781086000000002, 'relevance': 0}])"
        ],
        [
         "17",
         "I have a learning loop, where at each step `var1` and `var2` are generated. How can I achieve a single plot showing `var2` vs `var1` for each step in the learning loop?",
         "To create and update a plot showing `var2` vs `var1` for each step in a learning loop using Weights & Biases (W&B), you can follow these steps:\n\n1. **Initialize a W&B Run**: Start by initializing a W&B run using `wandb.init()`. This will track your experiment and store all the logs.\n\n2. **Log Data at Each Step**: Inside your learning loop, log `var1` and `var2` using `wandb.log()`. You need to log these variables as a dictionary where keys are the names of the variables and values are their respective values at each step.\n\n3. **Create a Line Plot**: W&B automatically plots these variables over steps if you log them consistently at each step. This will create a line plot with steps on the x-axis and the variable values on the y-axis.\n\nHere's a Python code snippet to illustrate this:\n\n```python\nimport wandb\n\n# Initialize a W&B run\nwandb.init(project='my_project', entity='my_username')\n\n# Example learning loop\nfor step in range(100):\n    # Simulate some data generation\n    var1 = step * 0.1  # Example function of var1\n    var2 = var1 ** 2   # Example function of var2\n    \n    # Log data to W&B\n    wandb.log({'Step': step, 'Var1': var1, 'Var2': var2})\n\n# Finish the W&B run\nwandb.finish()\n```\n\n**Best Practices for Visualizing Data in a Learning Loop Using W&B**:\n- **Consistent Logging**: Ensure that you log the data consistently at each step or at regular intervals to maintain a meaningful plot.\n- **Descriptive Names**: Use descriptive names for your logged variables and plots to make it easier to understand what each plot represents.\n- **Use Artifacts for Complex Data**: For more complex data or relationships, consider logging [W&B Artifacts](https://docs.wandb.ai/guides/artifacts) to track and version datasets, models, and other outputs of your steps.\n\nThis approach will help you visualize the relationship between `var1` and `var2` across the steps of your learning loop directly in the W&B dashboard, providing a dynamic and interactive way to monitor your experiment's progress.",
         "WeaveList([{'content': 'What data is logged with specific W&B API calls?\\nWith W&B, you can decide exactly what you want to log. The following lists some commonly logged objects:\\nDatasets: You have to specifically log images or other dataset samples for them to stream to W&B.\\nPlots: Use `wandb.plot` with `wandb.log` to track charts. See Log Plots for more information. \\nTables: Use `wandb.Table` to log data to visualize and query with W&B. See Log Tables for more information.\\nPyTorch gradients: Add `wandb.watch(model)` to see gradients of the weights as histograms in the UI.\\nConfiguration information: Log hyperparameters, a link to your dataset, or the name of the architecture you\\'re using as config parameters, passed in like this: `wandb.init(config=your_config_dictionary)`. See the PyTorch Integrations page for more information. \\nMetrics: Use `wandb.log` to see metrics from your model. If you log metrics like accuracy and loss from inside your training loop, you\\'ll get live updating graphs in the UI.\\nCommon workflows\\nCompare the best accuracy: To compare the best value of a metric across runs, set the summary value for that metric. By default, summary is set to the last value you logged for each key. This is useful in the table in the UI, where you can sort and filter runs based on their summary metrics — so you could compare runs in a table or bar chart based on their best accuracy, instead of final accuracy. For example, you could set summary like so: `wandb.run.summary[\"best_accuracy\"] = best_accuracy`\\nMultiple metrics on one chart: Log multiple metrics in the same call to `wandb.log`, like this: `wandb.log({\"acc\\'\": 0.9, \"loss\": 0.1})` and they will both be available to plot against in the UI\\nCustom x-axis: Add a custom x-axis to the same log call to visualize your metrics against a different axis in the W&B dashboard. For example: `wandb.log({\\'acc\\': 0.9, \\'epoch\\': 3, \\'batch\\': 117})`. To set the default x-axis for a given metric use Run.define_metric()\\nLog rich media and charts: `wandb.log` supports the logging of a wide variety of data types, from media like images and videos to tables and charts.', 'source': 'guides/track/log/intro.md', 'score': 0.21551579999999998, 'relevance': 2}, {'content': 'Log Plots\\nLog and Track Plots from  W&B Experiments.\\nUsing the methods in `wandb.plot`, you can track charts with `wandb.log`, including charts that change over time during training. To learn more about our custom charting framework, check out this guide.', 'source': 'guides/track/log/plots.md', 'score': 0.009020279, 'relevance': 2}, {'content': \"Scatter Plot\\nUse the scatter plot to compare multiple runs and visualize how your experiments are performing. We've added some customizable features:\\nPlot a line along the min, max, and average\\nCustom metadata tooltips\\nControl point colors \\nSet axes ranges\\nSwitch axes to log scale\\nHere’s an example of validation accuracy of different models over a couple of weeks of experimentation. The tooltip is customized to include the batch size and dropout as well as the values on the axes. There’s also a line plotting the running average of validation accuracy.\\nSee a live example →\\nCommon Questions\\nIs it possible to plot the max of a metric rather than plot step by step?\\nThe best way to do this is to create a Scatter Plot of the metric, go into the Edit menu, and select Annotations. From there you can plot the running max of the values\", 'source': 'guides/app/features/panels/scatter-plot.md', 'score': 0.016152835, 'relevance': 1}, {'content': \"3. Customize the chart\\nNow that looks pretty good, but I'd like to switch from a scatter plot to a line plot. Click Edit to change the Vega spec for this built in chart. Follow along in this workspace.\\nI updated the Vega spec to customize the visualization:\\nadd titles for the plot, legend, x-axis, and y-axis (set “title” for each field)\\nchange the value of “mark” from “point” to “line”\\nremove the unused “size” field\\nTo save this as a preset that you can use elsewhere in this project, click Save as at the top of the page. Here's what the result looks like, along with an ROC curve:\", 'source': 'guides/app/features/custom-charts/walkthrough.md', 'score': 0.0017409837, 'relevance': 1}, {'content': 'Or visualize all plots at once\\nW&B has functions such as `plot_classifier` that will plot several relevant plots:\\n# Visualize all classifier plots\\nwandb.sklearn.plot_classifier(\\n    clf,\\n    X_train,\\n    X_test,\\n    y_train,\\n    y_test,\\n    y_pred,\\n    y_probas,\\n    labels,\\n    model_name=\"SVC\",\\n    feature_names=None,\\n)\\n# All regression plots\\nwandb.sklearn.plot_regressor(reg, X_train, X_test, y_train, y_test, model_name=\"Ridge\")\\n# All clustering plots\\nwandb.sklearn.plot_clusterer(\\n    kmeans, X_train, cluster_labels, labels=None, model_name=\"KMeans\"\\n)\\nOr plot existing matplotlib plots:\\nPlots created on Matplotlib can also be logged on W&B dashboard. To do that, it is first required to install `plotly`.\\npip install plotly\\nFinally, the plots can be logged on W&B\\'s dashboard as follows:\\nimport matplotlib.pyplot as plt\\nimport wandb\\nwandb.init(project=\"visualize-sklearn\")\\n# do all the plt.plot(), plt.scatter(), etc. here.\\n# ...\\n# instead of doing plt.show() do:\\nwandb.log({\"plot\": plt})\\nSupported Plots\\nLearning Curve\\nTrains model on datasets of varying lengths and generates a plot of cross validated scores vs dataset size, for both training and test sets.\\n`wandb.sklearn.plot_learning_curve(model, X, y)`\\nmodel (clf or reg): Takes in a fitted regressor or classifier.\\nX (arr): Dataset features.\\ny (arr): Dataset labels.\\nROC\\nROC curves plot true positive rate (y-axis) vs false positive rate (x-axis). The ideal score is a TPR = 1 and FPR = 0, which is the point on the top left. Typically we calculate the area under the ROC curve (AUC-ROC), and the greater the AUC-ROC the better.\\n`wandb.sklearn.plot_roc(y_true, y_probas, labels)`\\ny_true (arr): Test set labels.\\ny_probas (arr): Test set predicted probabilities.\\nlabels (list): Named labels for target variable (y).', 'source': 'guides/integrations/scikit.md', 'score': 0.018334452, 'relevance': 0}, {'content': 'Turn on grouping to see settings for visualizing averaged values.\\nGroup key: Select a column, and all the runs with the same value in that column will be grouped together.\\nAgg: Aggregation— the value of the line on the graph. The options are mean, median, min, and max of the group.\\nRange: Switch the behavior for the shaded area behind the grouped curve. None means there is no shaded area. Min/Max shows a shaded region that covers the whole range of points in the group. Std Dev shows the standard deviation of values in the group. Std Err shows the standard error as the shaded area.\\nSampled runs: If you have hundreds of runs selected, we default to only sampling the first 100. You can select to have all your runs included in the grouping calculation, but it might slow things down in the UI.\\nLegend\\nTitle: Add a custom title for line plot, which shows up at the top of the chart\\nX-Axis title: Add a custom title for the x-axis of the line plot, which shows up in the lower right corner of the chart.\\nY-Axis title: Add a custom title for the y-axis of the line plot, which shows up in the upper left corner of the chart.\\nLegend: Select field that you want to see in the legend of the plot for each line. You could, for example, show the name of the run and the learning rate.\\nLegend template: Fully customizable, this powerful template allows you to specify exactly what text and variables you want to show up in the template at the top of the line plot as well as the legend that appears when you hover your mouse over the plot.\\nExpressions\\nY Axis Expressions: Add calculated metrics to your graph. You can use any of the logged metrics as well as configuration values like hyperparameters to calculate custom lines.\\nX Axis Expressions: Rescale the x-axis to use calculated values using custom expressions. Useful variables include**_step** for the default x-axis, and the syntax for referencing summary values is `${summary:value}`', 'source': 'guides/app/features/panels/line-plot/intro.md', 'score': 0.01384861, 'relevance': 0}, {'content': \"Bar Plot\\nA bar plot presents categorical data with rectangular bars which can be plotted vertically or horizontally. Bar plots show up by default with wandb.log() when all logged values are of length one.\\nCustomize with chart settings to limit max runs to show, group runs by any config and rename labels.\\nCustomize Bar Plots\\nYou can also create Box or Violin Plots to combine many summary statistics into one chart type.\\nGroup runs via runs table.\\nClick 'Add panel' in the workspace.\\nAdd a standard 'Bar Chart' and select the metric to plot.\\nUnder the 'Grouping' tab, pick 'box plot' or 'Violin', etc. to plot either of these styles.\", 'source': 'guides/app/features/panels/bar-plot.md', 'score': 0.009974921000000001, 'relevance': 0}, {'content': 'Merged view\\nInitially you will see both tables merged together. The first table selected has index 0 and a blue highlight, and the second table has index 1 and a yellow highlight. View a live example of merged tables here.\\nFrom the merged view, you can\\nchoose the join key: use the dropdown at the top left to set the column to use as the join key for the two tables. Typically this will be the unique identifier of each row, such as the file name of a specific example in your dataset or an incrementing index on your generated samples. Note that it\\'s currently possible to select any column, which may yield illegible tables and slow queries.\\nconcatenate instead of join: select \"concatenating all tables\" in this dropdown to union all the rows from both tables into one larger Table instead of joining across their columns\\nreference each Table explicitly: use 0, 1, and * in the filter expression to explicitly specify a column in one or both table instances\\nvisualize detailed numerical differences as histograms: compare the values in any cell at a glance\\nSide-by-side view\\nTo view the two tables side-by-side, change the first dropdown from \"Merge Tables: Table\" to \"List of: Table\" and then update the \"Page size\" respectively. Here the first Table selected is on the left and the second one is on the right. Also, you can compare these tables vertically as well by clicking on the \"Vertical\" checkbox.\\ncompare the tables at a glance: apply any operations (sort, filter, group) to both tables in tandem and spot any changes or differences quickly. For example, view the incorrect predictions grouped by guess, the hardest negatives overall, the confidence score distribution by true label, etc.\\nexplore two tables independently: scroll through and focus on the side/rows of interest\\nCompare artifacts\\nYou can also compare tables across time or model variants. \\nCompare tables across time\\nLog a table in an artifact for each meaningful step of training to analyze model performance over training time. For example, you could log a table at the end of every validation step, after every 50 epochs of training, or any frequency that makes sense for your pipeline. Use the side-by-side view to visualize changes in model predictions.\\nFor a more detailed walkthrough of visualizing predictions across training time, see this report and this interactive notebook example.', 'source': 'guides/tables/visualize-tables.md', 'score': 0.009559399000000001, 'relevance': 0}, {'content': 'Scikit-Learn\\nYou can use wandb to visualize and compare your scikit-learn models\\' performance with just a few lines of code. Try an example →\\n:fire: Getting Started\\nSign up and Log in to wandb\\na) Sign up for a free account\\nb) Pip install the `wandb` library\\nc) To log in in your training script, you\\'ll need to be signed in to you account at www.wandb.ai, then you will find your API key on the Authorize page.\\nIf you are using Weights and Biases for the first time you might want to check out our quickstart\\npip install wandb\\nwandb login\\n!pip install wandb\\nwandb.login()\\nLogging Metrics\\nimport wandb\\nwandb.init(project=\"visualize-sklearn\")\\ny_pred = clf.predict(X_test)\\naccuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\\n# If logging metrics over time, then use wandb.log\\nwandb.log({\"accuracy\": accuracy})\\n# OR to log a final metric at the end of training you can also use wandb.summary\\nwandb.summary[\"accuracy\"] = accuracy\\nMaking Plots\\nStep 1: Import wandb and initialize a new run.\\nimport wandb\\nwandb.init(project=\"visualize-sklearn\")\\nStep 2: Visualize individual plots\\nAfter training a model and making predictions you can then generate plots in wandb to analyze your predictions. See the Supported Plots section below for a full list of supported charts\\n# Visualize single plot\\nwandb.sklearn.plot_confusion_matrix(y_true, y_pred, labels)', 'source': 'guides/integrations/scikit.md', 'score': 0.0057302467, 'relevance': 0}, {'content': 'TensorBoard\\nHosted TensorBoard with 1 Line of Code\\nWith Weight & Biases you can easily upload your TensorBoard logs to the cloud, quickly share your results among colleagues and classmates and keep your analysis in one centralized location.\\nGet started now in with this Notebook: Try in a Colab Notebook here →\\nJust add 1 Line of Code\\nimport wandb\\n# Start a wandb run with `sync_tensorboard=True`\\nwandb.init(project=\"my-project\", sync_tensorboard=True)\\n# Your training code using TensorBoard\\n...\\n# [Optional]Finish the wandb run to upload the tensorboard logs to W&B (if running in Notebook)\\nwandb.finish()\\nSee here for an example of Tensorboard hosted in Weights & Biases\\nOnce your wandb run finishes, your TensorBoard event files will then be uploaded to Weights & Biases. These metrics will also be logged in native Weights & Biases charts along with a host of useful information such as your machines CPU or GPU utilization, the git state, the terminal command used, and much more.\\n:::info\\nWeights & Biases support TensorBoard with all versions of TensorFlow. W&B also supports TensorBoard > 1.14 with PyTorch as well as TensorBoardX.\\n:::\\nCommon questions\\nHow can I log metrics to W&B that aren\\'t logged to TensorBoard?\\nIf you need to log additional custom metrics that aren\\'t being logged to TensorBoard, you can call `wandb.log` in your code `wandb.log({\"custom\": 0.8})`\\nSetting the step argument in `wandb.log` is disabled when syncing Tensorboard. If you\\'d like to set a different step count, you can log the metrics with a step metric as:\\n`wandb.log({\"custom\": 0.8, \"global_step\": global_step})`', 'source': 'guides/integrations/tensorboard.md', 'score': 0.0027791907, 'relevance': 0}])"
        ],
        [
         "18",
         "My distributed runs are not in sync when using the Pytorch Lightning integration. How do I fix that ?",
         "Synchronization issues in distributed runs using Pytorch Lightning can often stem from how the GPUs (or ranks) are set up and how the `wandb.run` object is managed across different processes. Here are some steps and considerations to help ensure synchronization in your distributed training setup with Pytorch Lightning and Weights & Biases:\n\n### Common Causes of Synchronization Issues\n1. **Inconsistent Initialization**: Each GPU must be initialized in exactly the same way. If the initial conditions (like weights) differ across GPUs, it can lead to divergent behaviors and hinder synchronization.\n2. **Logging Discrepancies**: Only the rank 0 process should have access to the `wandb.run` object. For non-zero rank processes, `wandb.run` should be set to `None` to prevent them from attempting to log data to Weights & Biases, which can cause crashes or deadlocks.\n\n### Steps to Ensure Synchronization\n1. **Uniform Initialization**:\n   Ensure that all ranks are seeded the same way to start with identical weights. This can be done using `pl.seed_everything(seed)` before initializing your dataloaders and model.\n\n   ```python\n   import pytorch_lightning as pl\n\n   # Seed everything\n   pl.seed_everything(42)\n   ```\n\n2. **Proper Logging Setup**:\n   Configure the `WandbLogger` to ensure that only the rank 0 process logs information to Weights & Biases. This can be managed by setting up the logger conditionally based on the rank of the process.\n\n   ```python\n   import os\n   from pytorch_lightning.loggers import WandbLogger\n\n   # Determine the rank of the process\n   rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n\n   # Initialize WandbLogger only for rank 0\n   wandb_logger = WandbLogger() if rank == 0 else None\n   ```\n\n3. **Configure the Trainer**:\n   When setting up the `Trainer`, use the `strategy` parameter to specify the distributed backend (like 'ddp' for Distributed Data Parallel). Ensure that the logger is passed correctly.\n\n   ```python\n   from pytorch_lightning import Trainer\n\n   trainer = Trainer(\n       max_epochs=10,\n       gpus=2,\n       strategy='ddp',\n       logger=wandb_logger  # Pass the logger configured above\n   )\n   ```\n\n4. **Avoid Direct Access to `wandb.run`**:\n   In your training and validation steps, avoid direct calls to `wandb.run` and instead rely on the logging mechanisms provided by Pytorch Lightning and the `WandbLogger`.\n\nBy following these steps, you can help ensure that your distributed training runs are synchronized and that logging through Weights & Biases does not interfere with the training process.",
         "WeaveList([{'content': \"How to use multiple GPUs with Lightning and W&B?\\nPyTorch Lightning has Multi-GPU support through their DDP Interface. However, PyTorch Lightning's design requires us to be careful about how we instantiate our GPUs.\\nLightning assumes that each GPU (or Rank) in your training loop must be instantiated in exactly the same way - with the same initial conditions. However, only rank 0 process gets access to the `wandb.run` object, and for non-zero rank processes: `wandb.run = None`. This could cause your non-zero processes to fail. Such a situation can put you in a deadlock because rank 0 process will wait for the non-zero rank processes to join, which have already crashed.\\nFor this reason, we have to be careful about how we set up our training code. The recommended way to set it up would be to have your code be independent of the `wandb.run` object.\", 'source': 'guides/integrations/lightning.md', 'score': 0.9171543999999999, 'relevance': 2}, {'content': 'Check out interactive examples!\\nYou can follow along in our video tutorial with our tutorial colab here\\nFrequently Asked Questions\\nHow does W&B integrate with Lightning?\\nThe core integration is based on the Lightning loggers API, which lets you write much of your logging code in a framework-agnostic way. `Logger`s are passed to the Lightning Trainer and are triggered based on that API\\'s rich hook-and-callback system. This keeps your research code well-separated from engineering and logging code.\\nWhat does the integration log without any additional code?\\nWe\\'ll save your model checkpoints to W&B, where you can view them or download them for use in future runs. We\\'ll also capture system metrics, like GPU usage and network I/O, environment information, like hardware and OS information, code state (including git commit and diff patch, notebook contents and session history), and anything printed to the standard out.\\nWhat if I really need to use wandb.run in my training setup?\\nYou will have to essentially expand the scope of the variable you need to access yourself. In other words, making sure that the initial conditions are the same on all processes.\\nif os.environ.get(\"LOCAL_RANK\", None) is None:\\n    os.environ[\"WANDB_DIR\"] = wandb.run.dir\\nThen, you can use `os.environ[\"WANDB_DIR\"]` to set up the model checkpoints directory. This way, `wandb.run.dir` can be used by any non-zero rank processes as well.', 'source': 'guides/integrations/lightning.md', 'score': 0.07263671000000001, 'relevance': 2}, {'content': 'PyTorch Lightning\\nTry in a Colab Notebook here →\\nPyTorch Lightning provides a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training and 16-bit precision. W&B provides a lightweight wrapper for logging your ML experiments. But you don\\'t need to combine the two yourself: Weights & Biases is incorporated directly into the PyTorch Lightning library via the WandbLogger.\\n⚡ Get going lightning-fast with just a few lines.\\nfrom lightning.pytorch.loggers import WandbLogger\\nfrom lightning.pytorch import Trainer\\nwandb_logger = WandbLogger(log_model=\"all\")\\ntrainer = Trainer(logger=wandb_logger)\\n:::info\\nUsing wandb.log(): Please note that the `WandbLogger` logs to W&B using the Trainer\\'s `global_step`. If you are making additional calls to `wandb.log` directly in your code, do not use the `step` argument in `wandb.log()`. \\nInstead, log the Trainer\\'s `global_step` like your other metrics, like so:\\n`wandb.log({\"accuracy\":0.99, \"trainer/global_step\": step})`\\n:::\\nimport lightning as L\\nfrom wandb.integration.lightning.fabric import WandbLogger\\nwandb_logger = WandbLogger(log_model=\"all\")\\nfabric = L.Fabric(loggers=[wandb_logger])\\nfabric.launch()\\nfabric.log_dict({\"important_metric\": important_metric})\\nSign up and Log in to wandb\\na) Sign up for a free account\\nb) Pip install the `wandb` library\\nc) To log in in your training script, you\\'ll need to be signed in to you account at www.wandb.ai, then you will find your API key on the Authorize page.\\nIf you are using Weights and Biases for the first time you might want to check out our quickstart\\npip install wandb\\nwandb login\\n!pip install wandb\\nimport wandb\\nwandb.login()', 'source': 'guides/integrations/lightning.md', 'score': 0.6696368500000001, 'relevance': 1}, {'content': \"PyTorch Lightning\\nTry in a Colab Notebook here →\\nWe will build an image classification pipeline using PyTorch Lightning. We will follow this style guide to increase the readability and reproducibility of our code. A cool explanation of this available here.\\nSetting up PyTorch Lightning and W&B\\nFor this tutorial, we need PyTorch Lightning(ain't that obvious!) and Weights and Biases.\\n!pip install lightning -q\\n# install weights and biases\\n!pip install wandb -qU\\nYou're gonna need these imports.\\nimport lightning.pytorch as pl\\n# your favorite machine learning tracking tool\\nfrom lightning.pytorch.loggers import WandbLogger\\nimport torch\\nfrom torch import nn\\nfrom torch.nn import functional as F\\nfrom torch.utils.data import random_split, DataLoader\\nfrom torchmetrics import Accuracy\\nfrom torchvision import transforms\\nfrom torchvision.datasets import CIFAR10\\nimport wandb\\nNow you'll need to log in to you wandb account.\\nwandb.login()\", 'source': 'tutorials/lightning.md', 'score': 0.18922126, 'relevance': 1}, {'content': \"Log distributed training experiments\\nLog distributed training experiments\\nIn distributed training, models are trained using multiple GPUs in parallel. W&B supports two patterns to track distributed training experiments:\\nOne process: Initialize W&B (wandb.init) and log experiments (wandb.log) from a single process. This is a common solution for logging distributed training experiments with the PyTorch Distributed Data Parallel (DDP) Class. In some cases, users funnel data over from other processes using a multiprocessing queue (or another communication primitive) to the main logging process.\\nMany processes: Initialize W&B (wandb.init) and log experiments (wandb.log) in every process. Each process is effectively a separate experiment. Use the `group` parameter when you initialize W&B (`wandb.init(group='group-name')`) to define a shared experiment and group the logged values together in the W&B App UI.\\nThe proceeding examples demonstrate how to track metrics with W&B using PyTorch DDP on two GPUs on a single machine. PyTorch DDP (`DistributedDataParallel` in`torch.nn`) is a popular library for distributed training. The basic principles apply to any distributed training setup, but the details of implementation may differ.\\n:::info\\nExplore the code behind these examples in the W&B GitHub examples repository here. Specifically, see the log-dpp.py Python script for information on how to implement one process and many process methods.\\n:::\", 'source': 'guides/track/log/distributed-training.md', 'score': 0.007176882000000001, 'relevance': 1}, {'content': 'PyTorch and TensorBoard\\nIf you use PyTorch\\'s TensorBoard integration, you may need to manually upload the PyTorch Profiler JSON file:\\nwandb.save(glob.glob(f\"runs/*.pt.trace.json\")[0], base_path=f\"runs\")', 'source': 'guides/integrations/tensorboard.md', 'score': 0.42805406, 'relevance': 0}, {'content': 'Distributed runs\\nAllow a collection of runs to collaborate on a version before committing it. This is in contrast to single run mode described above where one run provides all the data for a new version.\\n:::info\\n1. Each run in the collection needs to be aware of the same unique ID (called `distributed_id`) in order to collaborate on the same version. By default, if present, W&B uses the run\\'s `group` as set by `wandb.init(group=GROUP)` as the `distributed_id`.\\n2. There must be a final run that \"commits\" the version, permanently locking its state.\\n3. Use `upsert_artifact` to add to the collaborative artifact and `finish_artifact` to finalize the commit.\\n:::\\nConsider the following example. Different runs (labelled below as Run 1, Run 2, and Run 3) add a different image file to the same artifact with `upsert_artifact`.\\nRun 1:\\nwith wandb.init() as run:\\n    artifact = wandb.Artifact(\"artifact_name\", \"artifact_type\")\\n    # Add Files and Assets to the artifact using\\n    # `.add`, `.add_file`, `.add_dir`, and `.add_reference`\\n    artifact.add_file(\"image1.png\")\\n    run.upsert_artifact(artifact, distributed_id=\"my_dist_artifact\")\\nRun 2:\\nwith wandb.init() as run:\\n    artifact = wandb.Artifact(\"artifact_name\", \"artifact_type\")\\n    # Add Files and Assets to the artifact using\\n    # `.add`, `.add_file`, `.add_dir`, and `.add_reference`\\n    artifact.add_file(\"image2.png\")\\n    run.upsert_artifact(artifact, distributed_id=\"my_dist_artifact\")\\nRun 3\\nMust run after Run 1 and Run 2 complete. The Run that calls `finish_artifact` can include files in the artifact, but does not need to.\\nwith wandb.init() as run:\\n    artifact = wandb.Artifact(\"artifact_name\", \"artifact_type\")\\n    # Add Files and Assets to the artifact\\n    # `.add`, `.add_file`, `.add_dir`, and `.add_reference`\\n    artifact.add_file(\"image3.png\")\\n    run.finish_artifact(artifact, distributed_id=\"my_dist_artifact\")', 'source': 'guides/artifacts/create-a-new-artifact-version.md', 'score': 0.1606607, 'relevance': 0}, {'content': 'Hugging Face Accelerate\\nAccelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code, making training and inference at scale made simple, efficient and adaptable.\\nAccelerate includes a Weights & Biases Tracker which we show how to use below. You can also read more about Accelerate Trackers in their docs here\\nStart logging with Accelerate\\nTo get started with Accelerate and Weights & Biases you can follow the pseudocode below:\\nfrom accelerate import Accelerator\\n# Tell the Accelerator object to log with wandb\\naccelerator = Accelerator(log_with=\"wandb\")\\n# Initialise your wandb run, passing wandb parameters and any config information\\naccelerator.init_trackers(\\n    project_name=\"my_project\", \\n    config={\"dropout\": 0.1, \"learning_rate\": 1e-2}\\n    init_kwargs={\"wandb\": {\"entity\": \"my-wandb-team\"}}\\n    )\\n...\\n# Log to wandb by calling `accelerator.log`, `step` is optional\\naccelerator.log({\"train_loss\": 1.12, \"valid_loss\": 0.8}, step=global_step)\\n# Make sure that the wandb tracker finishes correctly\\naccelerator.end_training()\\nExplaining more, you need to:\\n1. Pass `log_with=\"wandb\"` when initialising the Accelerator class\\n2. Call the init_trackers method and pass it:\\n- a project name via `project_name`\\n- any parameters you want to pass to wandb.init via a nested dict to `init_kwargs`\\n- any other experiment config information you want to log to your wandb run, via `config`\\n3. Use the `.log` method to log to Weigths & Biases; the `step` argument is optional\\n4. Call `.end_training` when finished training', 'source': 'guides/integrations/other/accelerate.md', 'score': 0.13927926000000002, 'relevance': 0}, {'content': 'import torch\\nfrom torch.nn import Linear, CrossEntropyLoss, functional as F\\nfrom torch.optim import Adam\\nfrom torchmetrics.functional import accuracy\\nfrom lightning.pytorch import LightningModule', 'source': 'guides/integrations/lightning.md', 'score': 0.047869008000000005, 'relevance': 0}, {'content': 'Distributed Training\\n`fastai` supports distributed training by using the context manager `distrib_ctx`. W&B supports this automatically and enables you to track your Multi-GPU experiments out of the box.\\nA minimal example is shown below:\\nimport wandb\\nfrom fastai.vision.all import *\\nfrom fastai.distributed import *\\nfrom fastai.callback.wandb import WandbCallback\\nwandb.require(experiment=\"service\")\\npath = rank0_first(lambda: untar_data(URLs.PETS) / \"images\")\\ndef train():\\n    dls = ImageDataLoaders.from_name_func(\\n        path,\\n        get_image_files(path),\\n        valid_pct=0.2,\\n        label_func=lambda x: x[0].isupper(),\\n        item_tfms=Resize(224),\\n    )\\n    wandb.init(\"fastai_ddp\", entity=\"capecape\")\\n    cb = WandbCallback()\\n    learn = vision_learner(dls, resnet34, metrics=error_rate, cbs=cb).to_fp16()\\n    with learn.distrib_ctx(sync_bn=False):\\n        learn.fit(1)\\nif __name__ == \"__main__\":\\n    train()\\nThen, in your terminal you will execute:\\n$ torchrun --nproc_per_node 2 train.py\\nin this case, the machine has 2 GPUs.\\nYou can now run distributed training directly inside a notebook!\\nimport wandb\\nfrom fastai.vision.all import *\\nfrom accelerate import notebook_launcher\\nfrom fastai.distributed import *\\nfrom fastai.callback.wandb import WandbCallback\\nwandb.require(experiment=\"service\")\\npath = untar_data(URLs.PETS) / \"images\"\\ndef train():\\n    dls = ImageDataLoaders.from_name_func(\\n        path,\\n        get_image_files(path),\\n        valid_pct=0.2,\\n        label_func=lambda x: x[0].isupper(),\\n        item_tfms=Resize(224),\\n    )\\n    wandb.init(\"fastai_ddp\", entity=\"capecape\")\\n    cb = WandbCallback()\\n    learn = vision_learner(dls, resnet34, metrics=error_rate, cbs=cb).to_fp16()\\n    with learn.distrib_ctx(in_notebook=True, sync_bn=False):\\n        learn.fit(1)\\nnotebook_launcher(train, num_processes=2)', 'source': 'guides/integrations/fastai/README.md', 'score': 0.01778039, 'relevance': 0}])"
        ],
        [
         "19",
         "give me the code to save outputs and parameters to a wandb table for this function:\nfor temp in [0, 0.5, 1., 2., 3.]:\n    gen = generate('Company:', temperature=temp)",
         "To save outputs and parameters to a `wandb.Table` for the function you've described, you can follow these steps. This example assumes that the `generate` function returns some output based on the 'Company:' prompt and a temperature parameter. Here's how you can integrate this with Weights & Biases to log the outputs and parameters:\n\n1. **Initialize a W&B run**: Before logging any data, you need to start a run using `wandb.init()`.\n\n2. **Create a W&B Table**: Define the columns of the table which will store the temperature values and the generated outputs.\n\n3. **Loop through the temperatures and generate outputs**: For each temperature, call the `generate` function, and add the temperature and output to the table.\n\n4. **Log the table to W&B**: After populating the table, log it to your W&B dashboard.\n\n5. **Finish the W&B run**: Once all data is logged, close the run with `wandb.finish()`.\n\nHere's the complete code snippet:\n\n```python\nimport wandb\n\n# Initialize a W&B run\nwandb.init(project='your_project_name', entity='your_wandb_username')\n\n# Create a W&B Table\ntable = wandb.Table(columns=[\"Temperature\", \"Generated Output\"])\n\n# Loop through temperatures and generate outputs\ntemperatures = [0, 0.5, 1., 2., 3.]\nfor temp in temperatures:\n    output = generate('Company:', temperature=temp)\n    table.add_data(temp, output)\n\n# Log the table to W&B\nwandb.log({\"Generated Outputs\": table})\n\n# Finish the W&B run\nwandb.finish()\n```\n\n**Explanation**:\n- `wandb.init()`: Starts a new W&B run. Replace `'your_project_name'` and `'your_wandb_username'` with your project's name and your W&B username.\n- `wandb.Table()`: Initializes a new table with specified columns.\n- `table.add_data()`: Adds a row to the table with the temperature and the corresponding output.\n- `wandb.log()`: Logs the table to your W&B dashboard under the key \"Generated Outputs\".\n- `wandb.finish()`: Marks the end of the W&B run.\n\nThis setup ensures that each temperature and its corresponding output are tracked and visualized effectively in your W&B project.",
         "WeaveList([{'content': 'Visualize your data\\nUse W&B Tables to visualize and query tabular data. For example:\\nCompare how different models perform on the same test set\\nIdentify patterns in your data\\nLook at sample model predictions visually\\nQuery to find commonly misclassified examples\\nThe above image shows a table with semantic segmentation and custom metrics. View this table here in this sample project from the W&B ML Course.\\nHow it works\\nA Table is a two-dimensional grid of data where each column has a single type of data. Tables support primitive and numeric types, as well as nested lists, dictionaries, and rich media types. \\nLog a Table\\nLog a table with a few lines of code:\\nwandb.init(): Create a run to track results.\\nwandb.Table(): Create a new table object.\\n`columns`: Set the column names.\\n`data`: Set the contents of the table.\\nrun.log(): Log the table to save it to W&B.\\npython showLineNumbers\\nimport wandb\\nrun = wandb.init(project=\"table-test\")\\nmy_table = wandb.Table(columns=[\"a\", \"b\"], data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]])\\nrun.log({\"Table Name\": my_table})\\nHow to get started\\nQuickstart: Learn to log data tables, visualize data, and query data.\\nTables Gallery: See example use cases for Tables.', 'source': 'guides/tables/intro.md', 'score': 0.7685595, 'relevance': 2}, {'content': 'Log a table to a run\\nUse `wandb.log()` to save your table to the run, like so:\\nrun = wandb.init()\\nmy_table = wandb.Table(columns=[\"a\", \"b\"], data=[[\"1a\", \"1b\"], [\"2a\", \"2b\"]])\\nrun.log({\"table_key\": my_table})\\nEach time a table is logged to the same key, a new version of the table is created and stored in the backend. This means you can log the same table across multiple training steps to see how model predictions improve over time, or compare tables across different runs, as long as they\\'re logged to the same key. You can log up to 200,000 rows.\\n:::info\\nTo log more than 200,000 rows, you can override the limit with:\\n`wandb.Table.MAX_ARTIFACTS_ROWS = X`\\nHowever, this would likely cause performance issues, such as slower queries, in the UI.\\n:::\\nAccess tables programmatically\\nIn the backend, Tables are persisted as Artifacts. If you are interested in accessing a specific version, you can do so with the artifact API:\\nwith wandb.init() as run:\\n    my_table = run.use_artifact(\"run-<run-id>-<table-name>:<tag>\").get(\"<table-name>\")\\nFor more information on Artifacts, see the Artifacts Chapter in the Developer Guide. \\nVisualize tables\\nAny table logged this way will show up in your Workspace on both the Run Page and the Project Page. For more information, see Visualize and Analyze Tables.\\nAdvanced: Artifact Tables\\nUse `artifact.add()` to log tables to the Artifacts section of your run instead of the workspace. This could be useful if you have a dataset that you want to log once and then reference for future runs. \\nrun = wandb.init(project=\"my_project\")\\n# create a wandb Artifact for each meaningful step\\ntest_predictions = wandb.Artifact(\"mnist_test_preds\", type=\"predictions\")\\n# [build up your predictions data as above]\\ntest_table = wandb.Table(data=data, columns=columns)\\ntest_predictions.add(test_table, \"my_test_key\")\\nrun.log_artifact(test_predictions)\\nRefer to this Colab for a detailed example of artifact.add() with image data and this Report for an example of how to use Artifacts and Tables to version control and deduplicate tabular data.', 'source': 'guides/track/log/log-tables.md', 'score': 0.6895146999999999, 'relevance': 2}, {'content': 'Log Tables\\nUse `wandb.Table` to log data to visualize and query with W&B. In this guide, learn how to:\\nCreate Tables\\nAdd Data\\nRetrieve Data\\nSave Tables\\nCreate Tables\\nTo define a Table, specify the columns you want to see for each row of data. Each row might be a single item in your training dataset, a particular step or epoch during training, a prediction made by your model on a test item, an object generated by your model, etc. Each column has a fixed type: numeric, text, boolean, image, video, audio, etc. You do not need to specify the type in advance. Give each column a name, and make sure to only pass data of that type into that column index. For a more detailed example, see this report.\\nUse the `wandb.Table` constructor in one of two ways:\\nList of Rows: Log named columns and rows of data. For example the proceeding code snippet generates a table with two rows and three columns:\\nwandb.Table(columns=[\"a\", \"b\", \"c\"], data=[[\"1a\", \"1b\", \"1c\"], [\"2a\", \"2b\", \"2c\"]])\\nPandas DataFrame: Log a DataFrame using `wandb.Table(dataframe=my_df)`. Column names will be extracted from the DataFrame.\\nFrom an existing array or dataframe\\n# assume a model has returned predictions on four images\\n# with the following fields available:\\n# - the image id\\n# - the image pixels, wrapped in a wandb.Image()\\n# - the model\\'s predicted label\\n# - the ground truth label\\nmy_data = [\\n    [0, wandb.Image(\"img_0.jpg\"), 0, 0],\\n    [1, wandb.Image(\"img_1.jpg\"), 8, 0],\\n    [2, wandb.Image(\"img_2.jpg\"), 7, 1],\\n    [3, wandb.Image(\"img_3.jpg\"), 1, 1],\\n]\\n# create a wandb.Table() with corresponding columns\\ncolumns = [\"id\", \"image\", \"prediction\", \"truth\"]\\ntest_table = wandb.Table(data=my_data, columns=columns)', 'source': 'guides/track/log/log-tables.md', 'score': 0.569314, 'relevance': 2}, {'content': 'Walkthrough\\nThe following Quickstart demonstrates how to log data tables, visualize data, and query data.\\nSelect the button below to try a PyTorch Quickstart example project on MNIST data. \\n1. Log a table\\nLog a table with W&B. You can either construct a new table or pass a Pandas DataFrame.\\nTo construct and log a new Table, you will use:\\n- wandb.init(): Create a run to track results.\\n- wandb.Table(): Create a new table object.\\n  - `columns`: Set the column names.\\n  - `data`: Set the contents of each row.\\n- run.log(): Log the table to save it to W&B.\\nHere\\'s an example:\\nimport wandb\\nrun = wandb.init(project=\"table-test\")\\n# Create and log a new table.\\nmy_table = wandb.Table(columns=[\"a\", \"b\"], data=[[\"a1\", \"b1\"], [\"a2\", \"b2\"]])\\nrun.log({\"Table Name\": my_table})\\nPass a Pandas DataFrame to `wandb.Table()` to create a new table.\\nimport wandb\\nimport pandas as pd\\ndf = pd.read_csv(\"my_data.csv\")\\nrun = wandb.init(project=\"df-table\")\\nmy_table = wandb.Table(dataframe=df)\\nwandb.log({\"Table Name\": my_table})\\nFor more information on supported data types, see the wandb.Table in the W&B API Reference Guide.\\n2. Visualize tables in your project workspace\\nView the resulting table in your workspace. \\nNavigate to your project in the W&B App.\\nSelect the name of your run in your project workspace. A new panel is added for each unique table key. \\nIn this example, `my_table`, is logged under the key `\"Table Name\"`.\\n3. Compare across model versions\\nLog sample tables from multiple W&B Runs and compare results in the project workspace. In this example workspace, we show how to combine rows from multiple different versions in the same table.\\nUse the table filter, sort, and grouping features to explore and evaluate model results.', 'source': 'guides/tables/tables-walkthrough.md', 'score': 0.48160622000000003, 'relevance': 2}, {'content': '# 🐝 Create a wandb Table to log images, labels and predictions to\\n    table = wandb.Table(columns=[\"image\", \"pred\", \"target\"]+[f\"score_{i}\" for i in range(10)])\\n    for img, pred, targ, prob in zip(images.to(\"cpu\"), predicted.to(\"cpu\"), labels.to(\"cpu\"), probs.to(\"cpu\")):\\n        table.add_data(wandb.Image(img[0].numpy()*255), pred, targ, *prob.numpy())\\n    wandb.log({\"predictions_table\":table}, commit=False)', 'source': 'tutorials/experiments.md', 'score': 0.5131958, 'relevance': 1}, {'content': 'Log a dictionary of data to the current run\\'s history.\\nUse `wandb.log` to log data from runs, such as scalars, images, video,\\nhistograms, plots, and tables.\\nSee our guides to logging for\\nlive examples, code snippets, best practices, and more.\\nThe most basic usage is `wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.\\nThis will save the loss and accuracy to the run\\'s history and update\\nthe summary values for these metrics.\\nVisualize logged data in the workspace at wandb.ai,\\nor locally on a self-hosted instance\\nof the W&B app, or export data to visualize and explore locally, e.g. in\\nJupyter notebooks, with our API.\\nIn the UI, summary values show up in the run table to compare single values across runs.\\nSummary values can also be set directly with `wandb.run.summary[\"key\"] = value`.\\nLogged values don\\'t have to be scalars. Logging any wandb object is supported.\\nFor example `wandb.log({\"example\": wandb.Image(\"myimage.jpg\")})` will log an\\nexample image which will be displayed nicely in the W&B UI.\\nSee the reference documentation\\nfor all of the different supported types or check out our\\nguides to logging for examples,\\nfrom 3D molecular structures and segmentation masks to PR curves and histograms.\\n`wandb.Table`s can be used to logged structured data. See our\\nguide to logging tables\\nfor details.\\nLogging nested metrics is encouraged and is supported in the W&B UI.\\nIf you log with a nested dictionary like `wandb.log({\"train\": {\"acc\": 0.9}, \"val\": {\"acc\": 0.8}})`, the metrics will be organized into\\n`train` and `val` sections in the W&B UI.\\nwandb keeps track of a global step, which by default increments with each\\ncall to `wandb.log`, so logging related metrics together is encouraged.\\nIf it\\'s inconvenient to log related metrics together\\ncalling `wandb.log({\"train-loss\": 0.5}, commit=False)` and then\\n`wandb.log({\"accuracy\": 0.9})` is equivalent to calling\\n`wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.', 'source': 'ref/python/run.md', 'score': 0.35947657, 'relevance': 1}, {'content': 'Use `wandb.log` to log data from runs, such as scalars, images, video,\\nhistograms, plots, and tables.\\nSee our guides to logging for\\nlive examples, code snippets, best practices, and more.\\nThe most basic usage is `wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.\\nThis will save the loss and accuracy to the run\\'s history and update\\nthe summary values for these metrics.\\nVisualize logged data in the workspace at wandb.ai,\\nor locally on a self-hosted instance\\nof the W&B app, or export data to visualize and explore locally, e.g. in\\nJupyter notebooks, with our API.\\nIn the UI, summary values show up in the run table to compare single values across runs.\\nSummary values can also be set directly with `wandb.run.summary[\"key\"] = value`.\\nLogged values don\\'t have to be scalars. Logging any wandb object is supported.\\nFor example `wandb.log({\"example\": wandb.Image(\"myimage.jpg\")})` will log an\\nexample image which will be displayed nicely in the W&B UI.\\nSee the reference documentation\\nfor all of the different supported types or check out our\\nguides to logging for examples,\\nfrom 3D molecular structures and segmentation masks to PR curves and histograms.\\n`wandb.Table`s can be used to logged structured data. See our\\nguide to logging tables\\nfor details.\\nLogging nested metrics is encouraged and is supported in the W&B UI.\\nIf you log with a nested dictionary like `wandb.log({\"train\": {\"acc\": 0.9}, \"val\": {\"acc\": 0.8}})`, the metrics will be organized into\\n`train` and `val` sections in the W&B UI.\\nwandb keeps track of a global step, which by default increments with each\\ncall to `wandb.log`, so logging related metrics together is encouraged.\\nIf it\\'s inconvenient to log related metrics together\\ncalling `wandb.log({\"train-loss\": 0.5}, commit=False)` and then\\n`wandb.log({\"accuracy\": 0.9})` is equivalent to calling\\n`wandb.log({\"train-loss\": 0.5, \"accuracy\": 0.9})`.', 'source': 'ref/python/log.md', 'score': 0.16844618, 'relevance': 1}, {'content': 'import wandb\\nreport = wr.Report(\\n    project=\"report-editing\",\\n    title=\"An amazing title\",\\n    description=\"A descriptive description.\",\\n)\\npanel_grids = wr.PanelGrid(\\n    panels=[\\n        wr.LinePlot(\\n            title=\"line title\",\\n            x=\"x\",\\n            y=[\"y\"],\\n            range_x=[0, 100],\\n            range_y=[0, 100],\\n            log_x=True,\\n            log_y=True,\\n            title_x=\"x axis title\",\\n            title_y=\"y axis title\",\\n            ignore_outliers=True,\\n            groupby=\"hyperparam1\",\\n            groupby_aggfunc=\"mean\",\\n            groupby_rangefunc=\"minmax\",\\n            smoothing_factor=0.5,\\n            smoothing_type=\"gaussian\",\\n            smoothing_show_original=True,\\n            max_runs_to_show=10,\\n            plot_type=\"stacked-area\",\\n            font_size=\"large\",\\n            legend_position=\"west\",\\n        ),\\n        wr.ScatterPlot(\\n            title=\"scatter title\",\\n            x=\"y\",\\n            y=\"y\",\\n            # z=\\'x\\',\\n            range_x=[0, 0.0005],\\n            range_y=[0, 0.0005],\\n            # range_z=[0,1],\\n            log_x=False,\\n            log_y=False,\\n            # log_z=True,\\n            running_ymin=True,\\n            running_ymean=True,\\n            running_ymax=True,\\n            font_size=\"small\",\\n            regression=True,\\n        ),\\n    ],\\n    runsets=[wr.RunSet(project=\"<project-name>\", entity=\"<entity-name>\")],\\n)\\nreport.blocks = [panel_grids]\\nreport.save()', 'source': 'guides/reports/edit-a-report.md', 'score': 0.5048483, 'relevance': 0}, {'content': '`generator`\\n(generator) a generator that returns validation data for wandb to visualize. This generator should return tuples `(X,y)`. Either `validate_data` or generator should be set for wandb to visualize specific data examples. In case you are working with image data, please also set `input_type` and `output_type` in order to log correctly.\\n`validation_steps`\\n(int) if `validation_data` is a generator, how many steps to run the generator for the full validation set.\\n`labels`\\n(list) If you are visualizing your data with wandb this list of labels will convert numeric output to understandable string if you are building a multiclass classifier. If you are making a binary classifier you can pass in a list of two labels [\"label for false\", \"label for true\"]. If `validate_data` and generator are both false, this won\\'t do anything.\\n`predictions`\\n(int) the number of predictions to make for visualization each epoch, max is 100.\\n`input_type`\\n(string) type of the model input to help visualization. can be one of: (`image`, `images`, `segmentation_mask`, `auto`).\\n`output_type`\\n(string) type of the model output to help visualization. can be one of: (`image`, `images`, `segmentation_mask`, `label`).\\n`log_evaluation`\\n(boolean) if True, save a Table containing validation data and the model\\'s predictions at each epoch. See `validation_indexes`, `validation_row_processor`, and `output_row_processor` for additional details.\\n`class_colors`\\n([float, float, float]) if the input or output is a segmentation mask, an array containing an rgb tuple (range 0-1) for each class.\\n`log_batch_frequency`\\n(integer) if None, callback will log every epoch. If set to integer, callback will log training metrics every `log_batch_frequency` batches.\\n`log_best_prefix`\\n(string) if None, no extra summary metrics will be saved. If set to a string, the monitored metric and epoch will be prepended with this value and stored as summary metrics.', 'source': 'ref/python/integrations/keras/wandbcallback.md', 'score': 0.26020142, 'relevance': 0}, {'content': 'Quickstart\\nInstall W&B and start tracking your machine learning experiments in minutes.\\n1. Create an account and install W&B\\nBefore you get started, make sure you create an account and install W&B:\\nSign up for a free account at https://wandb.ai/site and then log in to your wandb account.  \\nInstall the wandb library on your machine in a Python 3 environment using pip.  \\nThe following code snippets demonstrate how to install and log into W&B using the W&B CLI and Python Library:\\nInstall the CLI and Python library for interacting with the Weights and Biases API:\\npip install wandb\\nInstall the CLI and Python library for interacting with the Weights and Biases API:\\n!pip install wandb\\n2. Log in to W&B\\nNext, log in to W&B:\\nwandb login\\nOr if you are using W&B Server (including Dedicated Cloud or Self-managed):\\nwandb login --relogin --host=http://your-shared-local-host.com\\nIf needed, ask your deployment admin for the hostname.\\nProvide your API key when prompted.\\nNext, import the W&B Python SDK and log in:\\nwandb.login()\\nProvide your API key when prompted.\\n  \\n3. Start a  run and track hyperparameters\\nInitialize a W&B Run object in your Python script or notebook with wandb.init() and pass a dictionary to the `config` parameter with key-value pairs of hyperparameter names and values:\\nrun = wandb.init(\\n    # Set the project where this run will be logged\\n    project=\"my-awesome-project\",\\n    # Track hyperparameters and run metadata\\n    config={\\n        \"learning_rate\": 0.01,\\n        \"epochs\": 10,\\n    },\\n)\\nA run is the basic building block of W&B. You will use them often to track metrics, create logs, create jobs, and more.', 'source': 'quickstart.md', 'score': 0.12624557, 'relevance': 0}])"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 20
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How can I access the run object from the Light...</td>\n",
       "      <td>In PyTorch Lightning, the `WandbLogger` is use...</td>\n",
       "      <td>[{'content': 'Using PyTorch Lightning's WandbL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>is there a method to auto delete log files fro...</td>\n",
       "      <td>Currently, Weights &amp; Biases does not provide a...</td>\n",
       "      <td>[{'content': 'wandb artifact cache cleanup\n",
       "Usa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is it possible to group by tags within the UI?</td>\n",
       "      <td>In Weights &amp; Biases, while tags are very usefu...</td>\n",
       "      <td>[{'content': 'Turn off grouping\n",
       "Click the grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>why did wandb stop logging model parameters wi...</td>\n",
       "      <td>It sounds like you're experiencing issues with...</td>\n",
       "      <td>[{'content': 'Additional W&amp;B settings\n",
       "Further ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>can we stopped to automatically add new config...</td>\n",
       "      <td>In Weights &amp; Biases, when you initialize a new...</td>\n",
       "      <td>[{'content': 'Set the configuration throughout...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I feel wandb cannot connect the server, how ca...</td>\n",
       "      <td>To test the connection to the `wandb` server f...</td>\n",
       "      <td>[{'content': 'Verify your installation\n",
       "Very yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>give me a script that goes through every proje...</td>\n",
       "      <td>To create a script that goes through every pro...</td>\n",
       "      <td>[{'content': 'Delete multiple artifact version...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I'm trying to get data from a run.history() an...</td>\n",
       "      <td>When you use `run.history()` in Weights &amp; Bias...</td>\n",
       "      <td>[{'content': 'run.history()\n",
       "   _step  val   lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>if I execute a run in offline mode, is there a...</td>\n",
       "      <td>When you execute a run in offline mode using W...</td>\n",
       "      <td>[{'content': 'Is it possible to save metrics o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>i am not getting loss/average data from traini...</td>\n",
       "      <td>If you're only seeing loss data and not the av...</td>\n",
       "      <td>[{'content': 'Log metrics inside your training...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>set entity of weave project to not personal</td>\n",
       "      <td>To change the entity of a Weave project from p...</td>\n",
       "      <td>[{'content': 'Project Defaults\n",
       "You can change ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>what are the differences between grid, random ...</td>\n",
       "      <td>In Weights &amp; Biases, you can choose from three...</td>\n",
       "      <td>[{'content': 'Grid search\n",
       "Iterate over every c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>is there a graphical interface to set paramete...</td>\n",
       "      <td>Yes, Weights &amp; Biases (W&amp;B) provides a graphic...</td>\n",
       "      <td>[{'content': 'Manage job inputs\n",
       "The core exper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>can you help me setup LLM evaluation on my loc...</td>\n",
       "      <td>To set up LLM evaluation on your local machine...</td>\n",
       "      <td>[{'content': 'Iterate on LLMs\n",
       "Try in a Colab N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>How can I set a custom metric to group runs?</td>\n",
       "      <td>To set a custom metric and group runs based on...</td>\n",
       "      <td>[{'content': 'Group Runs\n",
       "Group W&amp;B Runs\n",
       "Group ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>how does Weights &amp; Biases support GPU usage?</td>\n",
       "      <td>Weights &amp; Biases (W&amp;B) provides robust support...</td>\n",
       "      <td>[{'content': 'NVIDIA GPU\n",
       "W&amp;B uses an adapted v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>We were wondering if there is a standard way t...</td>\n",
       "      <td>To log non-strictly increasing steps in Weight...</td>\n",
       "      <td>[{'content': 'Customize Log Axes\n",
       "Use `define_m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I have a learning loop, where at each step `va...</td>\n",
       "      <td>To create and update a plot showing `var2` vs ...</td>\n",
       "      <td>[{'content': 'What data is logged with specifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>My distributed runs are not in sync when using...</td>\n",
       "      <td>Synchronization issues in distributed runs usi...</td>\n",
       "      <td>[{'content': 'How to use multiple GPUs with Li...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>give me the code to save outputs and parameter...</td>\n",
       "      <td>To save outputs and parameters to a `wandb.Tab...</td>\n",
       "      <td>[{'content': 'Visualize your data\n",
       "Use W&amp;B Tabl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0   How can I access the run object from the Light...   \n",
       "1   is there a method to auto delete log files fro...   \n",
       "2      Is it possible to group by tags within the UI?   \n",
       "3   why did wandb stop logging model parameters wi...   \n",
       "4   can we stopped to automatically add new config...   \n",
       "5   I feel wandb cannot connect the server, how ca...   \n",
       "6   give me a script that goes through every proje...   \n",
       "7   I'm trying to get data from a run.history() an...   \n",
       "8   if I execute a run in offline mode, is there a...   \n",
       "9   i am not getting loss/average data from traini...   \n",
       "10        set entity of weave project to not personal   \n",
       "11  what are the differences between grid, random ...   \n",
       "12  is there a graphical interface to set paramete...   \n",
       "13  can you help me setup LLM evaluation on my loc...   \n",
       "14       How can I set a custom metric to group runs?   \n",
       "15       how does Weights & Biases support GPU usage?   \n",
       "16  We were wondering if there is a standard way t...   \n",
       "17  I have a learning loop, where at each step `va...   \n",
       "18  My distributed runs are not in sync when using...   \n",
       "19  give me the code to save outputs and parameter...   \n",
       "\n",
       "                                               answer  \\\n",
       "0   In PyTorch Lightning, the `WandbLogger` is use...   \n",
       "1   Currently, Weights & Biases does not provide a...   \n",
       "2   In Weights & Biases, while tags are very usefu...   \n",
       "3   It sounds like you're experiencing issues with...   \n",
       "4   In Weights & Biases, when you initialize a new...   \n",
       "5   To test the connection to the `wandb` server f...   \n",
       "6   To create a script that goes through every pro...   \n",
       "7   When you use `run.history()` in Weights & Bias...   \n",
       "8   When you execute a run in offline mode using W...   \n",
       "9   If you're only seeing loss data and not the av...   \n",
       "10  To change the entity of a Weave project from p...   \n",
       "11  In Weights & Biases, you can choose from three...   \n",
       "12  Yes, Weights & Biases (W&B) provides a graphic...   \n",
       "13  To set up LLM evaluation on your local machine...   \n",
       "14  To set a custom metric and group runs based on...   \n",
       "15  Weights & Biases (W&B) provides robust support...   \n",
       "16  To log non-strictly increasing steps in Weight...   \n",
       "17  To create and update a plot showing `var2` vs ...   \n",
       "18  Synchronization issues in distributed runs usi...   \n",
       "19  To save outputs and parameters to a `wandb.Tab...   \n",
       "\n",
       "                                             contexts  \n",
       "0   [{'content': 'Using PyTorch Lightning's WandbL...  \n",
       "1   [{'content': 'wandb artifact cache cleanup\n",
       "Usa...  \n",
       "2   [{'content': 'Turn off grouping\n",
       "Click the grou...  \n",
       "3   [{'content': 'Additional W&B settings\n",
       "Further ...  \n",
       "4   [{'content': 'Set the configuration throughout...  \n",
       "5   [{'content': 'Verify your installation\n",
       "Very yo...  \n",
       "6   [{'content': 'Delete multiple artifact version...  \n",
       "7   [{'content': 'run.history()\n",
       "   _step  val   lo...  \n",
       "8   [{'content': 'Is it possible to save metrics o...  \n",
       "9   [{'content': 'Log metrics inside your training...  \n",
       "10  [{'content': 'Project Defaults\n",
       "You can change ...  \n",
       "11  [{'content': 'Grid search\n",
       "Iterate over every c...  \n",
       "12  [{'content': 'Manage job inputs\n",
       "The core exper...  \n",
       "13  [{'content': 'Iterate on LLMs\n",
       "Try in a Colab N...  \n",
       "14  [{'content': 'Group Runs\n",
       "Group W&B Runs\n",
       "Group ...  \n",
       "15  [{'content': 'NVIDIA GPU\n",
       "W&B uses an adapted v...  \n",
       "16  [{'content': 'Customize Log Axes\n",
       "Use `define_m...  \n",
       "17  [{'content': 'What data is logged with specifi...  \n",
       "18  [{'content': 'How to use multiple GPUs with Li...  \n",
       "19  [{'content': 'Visualize your data\n",
       "Use W&B Tabl...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(eval_dataset.rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragnar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
